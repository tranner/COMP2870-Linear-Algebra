[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "",
    "text": "1 Welcome to linear algebra",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to linear algebra</span>"
    ]
  },
  {
    "objectID": "index.html#contents-of-this-submodule",
    "href": "index.html#contents-of-this-submodule",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.1 Contents of this submodule",
    "text": "1.1 Contents of this submodule\nThis part of the module will deal with numerical algorithms that involve matrices. The study of this type of problem is called linear algebra. We will approach these problems using a mix of theoretical ideas thinking through the lens of practical solutions. This means that you will do some programming (using Python) and some pen and paper theoretical work too.\n\n1.1.1 Topics\nWe will have 7 double-lectures, 3/4 tutorials and 3/4 labs. We break up the topics as follows:\nLectures\n\nIntroduction and motivation, key problem statements\nWhen can we solve systems of linear equations?\nDirect methods for systems of linear equations\nIterative solution of linear equations\nComplex numbers\nEigenvalues and eigenvectors\nPractical solutions for eigenvalues and eigenvectors / Summary\n\nLabs\n\nFloating point numbers\nWhen can we solve systems of linear equations?\nSystems of linear equations\nEigenvalues and eigenvectors\n\nTutorials\n\nDirect methods for systems of linear equations\nIndirect methods for systems of linear equations\nComplex numbers\nEigenvalues and eigenvectors",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to linear algebra</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.2 Learning outcomes",
    "text": "1.2 Learning outcomes\nCandidates should be able to:\n\nexplain practical challenges working with floating point numbers;\ndefine and identify what is means for a set of vectors to be a basis, spanning set or linear independent;\napply direct and iterative solvers to solve systems of linear equations; implement methods using floating point numbers and investigate computational cost using computer experiments;\napply algorithms to compute eigenvectors and eigenvalues of large matrices.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to linear algebra</span>"
    ]
  },
  {
    "objectID": "src/lec01.html",
    "href": "src/lec01.html",
    "title": "2  Linear Algebra introduction",
    "section": "",
    "text": "2.1 Contents of this submodule\nThis part of the module will deal with numerical algorithms that involve matrices. The study of this type of problem is called linear algebra. We will approach these problems using a mix of theoretical ideas thinking through the lens of practical solutions. This means that you will do some programming (using Python) and some pen and paper theoretical work too.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra introduction</span>"
    ]
  },
  {
    "objectID": "src/lec01.html#contents-of-this-submodule",
    "href": "src/lec01.html#contents-of-this-submodule",
    "title": "2  Linear Algebra introduction",
    "section": "",
    "text": "2.1.1 Topics\nWe will have 7 double-lectures, 3/4 tutorials and 3/4 labs. We break up the topics as follows:\nLectures\n\nIntroduction and motivation, key problem statements\nWhen can we solve systems of linear equations?\nDirect methods for systems of linear equations\nIterative solution of linear equations\nComplex numbers\nEigenvalues and eigenvectors\nPractical solutions for eigenvalues and eigenvectors / Summary\n\nLabs\n\nFloating point numbers\nWhen can we solve systems of linear equations?\nSystems of linear equations\nEigenvalues and eigenvectors\n\nTutorials\n\nDirect methods for systems of linear equations\nIndirect methods for systems of linear equations\nComplex numbers\nEigenvalues and eigenvectors\n\n\n\n2.1.2 Learning outcomes\nCandidates should be able to:\n\nexplain practical challenges working with floating point numbers;\ndefine and identify what is means for a set of vectors to be a basis, spanning set or linear independent;\napply direct and iterative solvers to solve systems of linear equations; implement methods using floating point numbers and investigate computational cost using computer experiments;\napply algorithms to compute eigenvectors and eigenvalues of large matrices.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra introduction</span>"
    ]
  },
  {
    "objectID": "src/lec01.html#matrices-and-vectors",
    "href": "src/lec01.html#matrices-and-vectors",
    "title": "2  Linear Algebra introduction",
    "section": "2.2 Matrices and vectors",
    "text": "2.2 Matrices and vectors\nThere are two important objects we wil work with that were defined in your first year Theoretical Foundations module.\n\nDefinition 2.1 A matrix is a rectangular array of numbers called entries or elements of the matrix. A matrix with \\(m\\) rows and \\(n\\) columns is called an \\(m \\times n\\) matrix or \\(m\\)-by-\\(n\\) matrix. We may additionally say that the matrix is of order \\(m \\times n\\). If \\(m = n\\), then we say that the matrix is square.\n\n\nExample 2.1 \\(A\\) is a \\(4 \\times 4\\) matrix and \\(B\\) is a \\(3 \\times 4\\) matrix: \\[\\begin{align*}\nA = \\begin{pmatrix} 10 & 1 & 0 & 9 \\\\ 12.4 & 6 & 1 & 0 \\\\ 1 &\n3.14 & 1 & 0 \\end{pmatrix}\n\\quad\nB = \\begin{pmatrix} 0 & 6 & 3 & 1 \\\\ 1 & 4 & 1\n& 0 \\\\ 7 & 0 & 10 & 20 \\end{pmatrix} \\quad C = \\begin{pmatrix} 4 & 1 & 8 & -1 \\\\\n1.5 & 1 & 3 & 4 \\\\ 6 & -4 & 2 & 8 \\end{pmatrix}\n\\end{align*}\\]\n\nExercise 2.1  \n\nCompute, if defined, \\(A + B\\), \\(B + C\\).\nCompute, if defined, \\(A B\\), \\(B A\\), \\(B C\\) (here, by writing matrices next to each other we mean the matrix product).\n\n\n\nWhen considering systems of linear equations the entries of the matrix will always be real numbers (later we will explore using complex numbers (Chapter 7) too)\n\nDefinition 2.2 A column vector, often just called a vector, is a matrix with a single column. A matrix with a single row is a row vector. The entries of a vector are called components. A vector with \\(n\\)-rows is called an \\(n\\)-vector.\n\n\nExample 2.2 \\(\\vec{a}\\) is a row vector, \\(\\vec{b}\\) and \\(\\vec{c}\\) are (column) vectors. \\[\\begin{align*}\n\\vec{a} =\n\\begin{pmatrix} 0 & 1 & 7 \\end{pmatrix}\n\\quad\n\\vec{b} =\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 3.1 \\\\ 7 \\end{pmatrix}\n\\quad\n\\vec{c} = \\begin{pmatrix} 4 \\\\ 6 \\\\ -4 \\\\ 0 \\end{pmatrix}.\n\\end{align*}\\]\n\nExercise 2.2  \n\nCompute, if defined, \\(\\vec{b} + \\vec{c}\\), \\(0.25 \\vec{c}\\).\nWhat is the meaning of \\(\\vec{b}^T \\vec{c}\\)? (here, we are interpreting the vectors as matrices).\nCompute, if defined, \\(B \\vec{b}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra introduction</span>"
    ]
  },
  {
    "objectID": "src/lec01.html#systems-of-linear-equations",
    "href": "src/lec01.html#systems-of-linear-equations",
    "title": "2  Linear Algebra introduction",
    "section": "2.3 Systems of linear equations",
    "text": "2.3 Systems of linear equations\nGiven an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies: \\[\\begin{equation}\n\\label{eq:sle}\nA \\vec{x} = \\vec{b}.\n\\end{equation}\\]\nWe can also write \\(\\eqref{eq:sle}\\) as a system of linear equations: \\[\\begin{align*}\n\\text{Equation 1:} &&\na_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} &&\na_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} &&\na_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} &&\na_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nNotes:\n\nThe values \\(a_{ij}\\) are known as coefficients.\nThe right hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.\n\nMany computational algorithms require the solution of linear equations, e.g. in fields such as\n\nScientific computation;\nNetwork design and optimisation;\nGraphics and visualisation;\nMachine learning.\n\nTODO precise examples\nTypically these systems are very large (\\(n \\approx 10^9\\)).\nIt is therefore important that this problem can be solved\n\naccurately: we are allowed to make small errors but not big errors;\nefficiently: we need to find the answer quickly;\nreliably: we need to know that our algorithm will give us an answer that we are happy with.\n\n\nExample 2.3 (Temperature in a sealed room) Suppose we wish to estimate the temperature distribution inside an object:\n\n\n\n\n\nImage showing temperature sample points and relations in a room.\n\n\n\n\nWe can place a network of points inside the object and use the following model: the temperature at each interior point is the average of its neighbours.\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & -1/6 & -1/6 & 0 & -1/6 & 0 & 0 \\\\\n-1/6 & 1 & -1/6 & -1/6 & 0 & -1/6 & 0 \\\\\n-1/4 & -1/4 & 1 & 0 & -1/4 & -1/4 & 0 \\\\\n0 & -1/5 & 0 & 1 & 0 & -1/5 & 0 \\\\\n-1/6 & 0 & -1/6 & 0 & 1 & -1/6 & -1/6 \\\\\n0 & -1/8 & -1/8 & -1/8 & -1/8 & 1 & -1/8 \\\\\n0 & 0 & 0 & 0 & -1/5 & -1/5 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7\n\\end{pmatrix}\n= \\begin{pmatrix}\n400/6 \\\\ 100/6 \\\\ 0 \\\\ 0 \\\\ 100/6 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n\\]\n\n\nExample 2.4 (Traffic network) Suppose we wish to monitor the flow of traffic in a city centre:\n\n\n\n\n\nExample network showing traffic flow in a city\n\n\n\n\nAs the above example shows, it is not necessary to monitor at every single road. If we know all of the \\(y\\) values we can calculate the \\(x\\) values!\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_5 \\\\ y_{12} - y_6 \\\\ y_8 - y_7 \\\\ y_{11} - y_4 \\\\\ny_{11} + y_{12} - y_{10} \\\\ y_9 \\\\ y_2 - y_3 \\\\ y_1\n\\end{pmatrix}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra introduction</span>"
    ]
  },
  {
    "objectID": "src/lec01.html#eigenvalues-and-eigenvectors",
    "href": "src/lec01.html#eigenvalues-and-eigenvectors",
    "title": "2  Linear Algebra introduction",
    "section": "2.4 Eigenvalues and eigenvectors",
    "text": "2.4 Eigenvalues and eigenvectors\nFor this problem, we will think of a matrix \\(A\\) acting on functions \\(\\vec{x}\\): \\[\\begin{equation*}\n\\vec{x} \\mapsto A \\vec{x}.\n\\end{equation*}\\] We are interested in when is the output vector \\(A \\vec{x}\\) is parallel to \\(\\vec{x}\\).\n\nDefinition 2.3 We say that any vector \\(\\vec{x}\\) where \\(A \\vec{x}\\) is parallel is \\(\\vec{x}\\) is called an eigenvector of \\(A\\). Here by parallel, we mean that there exists a number \\(\\lambda\\) (can be positive, negative or zero) such that \\[\\begin{equation}\n\\label{eq:evalues}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We call the associated number \\(\\lambda\\) an eigenvalue of \\(A\\).\nWe will later see that an \\(n \\times n\\) square matrix always has \\(n\\) eigenvalues (which may not always be distinct).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra introduction</span>"
    ]
  },
  {
    "objectID": "src/lec02.html",
    "href": "src/lec02.html",
    "title": "3  Floating point number systems",
    "section": "",
    "text": "3.1 Finite precision number systems\nThis topic will not be presented in lectures explicitly instead you will study the material yourself in Lab session 1.\nComputers store numbers with finite precision, i.e. using a finite set of bits (binary digits), typically 32 or 64 of them. You met how to store numbers as floating point numbers last year in the module COMP18XX??.\nYou will recall that many numbers cannot be stored exactly.\nThe inaccuracies inherent in finite precision arithmetic must be modelled in order to understand:\nThe examples shown here will be in decimal by the issues apply to any base, e.g. binary.\nThis is important when trying to solve problems with floating point numbers so that we learn how to avoid the key pitfalls.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#finite-precision-number-systems",
    "href": "src/lec02.html#finite-precision-number-systems",
    "title": "3  Floating point number systems",
    "section": "",
    "text": "Some numbers cannot be represented precisely using any finite set of digits: e.g. \\(\\sqrt{2} = 1.141 42\\ldots\\), \\(\\pi = 3.141 59\\ldots\\), etc.\nSome cannot be represented precisely in a given number base:\ne.g. \\(\\frac{1}{9} = 0.111\\ldots\\) (decimal), \\(\\frac{1}{5} = 0.0011 0011\n  \\ldots\\) (binary).\nOthers can be represented by a finite number of digits but only using more than are available: e.g. \\(1.526 374 856 437\\) cannot be stored exactly using 10 decimal digits.\n\n\n\nhow the numbers are represented (and the nature of associated limitations);\nthe errors in their representation;\nthe errors which occur when arithmetic operations are applied to them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#normalised-systems",
    "href": "src/lec02.html#normalised-systems",
    "title": "3  Floating point number systems",
    "section": "3.2 Normalised systems",
    "text": "3.2 Normalised systems\nTo understand how this works in practice, we introduce an abstract way to think about the practicalities of floating point numbers, but our examples will have smaller numbers of digits.\nAny finite precision number can be written using the floating point representation\n\\[\nx = \\pm 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e.\n\\]\n\nThe digits \\(b_i\\) are integers satisfying \\(0 \\le b_i \\le \\beta - 1\\).\nThe mantissa, \\(b_1 b_2 b_3 \\ldots b_{t-1} b_t\\), contains \\(t\\) digits.\n\\(\\beta\\) is the base (always a positive integer).\n\\(e\\) is the integer exponent and is bounded (\\(L \\le e \\le U\\)).\n\n\\((\\beta, t, L, U)\\) fully defines a finite precision number system.\nNormalised finite precision systems will be considered here for which\n\\[ b_1 \\neq 0 \\quad (0 &lt; b_1 \\le \\beta -1). \\]\n\nExample 3.1  \n\nIn the case \\((\\beta, t, L, U) = (10, 4, -49, 50)\\) (base 10), \\[ 10 000 =\n.1000 \\times 10^5, \\quad 22.64 = .2264 \\times 10^2, \\quad 0.000 056 7 =\n.5670 \\times 10^{-4} \\]\nIn the case \\((\\beta, t, L, U) = (2, 6, -7, 8)\\) (binary), \\[ 1 0000 = .1000 00\n\\times 2^5, \\quad 1011.11 = .1011 11 \\times 2^4,\\] \\[ 0.0000 11 = .1100 00\n\\times 2^{-4}. \\]\nZero is always taken to be a special case e.g., \\(0 = \\pm .00\\ldots 0\n\\times \\beta^0\\).\n\nOur familiar floating point numbers can be representing using this format too:\n\nThe IEEE single precision standard is \\((\\beta, t, L, U) = (2, 23, -127, 128)\\). This is available via numpy.single.\nThe IEEE double precision standard is \\((\\beta, t, L, U) = (2, 52, -1023, 1024)\\). This is available via numpy.double.\n\n\nimport numpy as np\n\na = np.double(1.1)\nprint(type(a))\nb = np.single(1.2)\nprint(type(b))\nc = np.half(1.3)\nprint(type(c))\n\n&lt;class 'numpy.float64'&gt;\n&lt;class 'numpy.float32'&gt;\n&lt;class 'numpy.float16'&gt;\n\n\n\n\nExample 3.2 Consider the number system given by \\((\\beta, t, L, U) =\n(10, 2, -1, 2)\\) which gives\n\\[ x = \\pm .b_1 b_2 \\times 10^e \\text{ where } -1 \\le e \\le 2. \\]\n\n\nHow many numbers can be represented by this normalised system?\n\n\n\nthe sign can be positive or negative\n\\(b_1\\) can take on the values \\(1\\) to \\(9\\) (9 options)\n\\(b_2\\) can take on the values \\(0\\) to \\(9\\) (10 options)\n\\(e\\) can take on the values \\(-1, 0, 1, 2\\) (4 options)\n\nOverall this gives us: \\[ 2 \\times 9 \\times 10 \\times 4 \\text{ options } = 720\n\\text{ options}. \\]\n\n\nWhat are the two largest positive numbers in this system?\n\n\nThe largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 9\\) and \\(e = 2\\) which gives \\[+ 0.99 \\times 10^{2} = 99. \\]\nThe second largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 8\\) and \\(e = 2\\) which gives \\[+ 0.98 \\times 10^{2} = 98. \\]\n\n\nWhat are the two smallest positive numbers?\n\n\nThe smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 =0\\) and \\(e=-1\\) which gives \\[+ 0.10 \\times 10^{-1} = 0.01. \\]\nThe second smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 = 1\\) and \\(e =\n-1\\) which gives \\[+ 0.11 \\times 10^{-1} = 0.011. \\]\n\n\nWhat is the smallest possible difference between two numbers in this system?\n\n\nThe smallest different will be between numbers of the form \\(+0.10 \\ times\n10^{-1}\\) and \\(+0.11 \\times 10^{-1}\\) which gives \\[ 0.11 \\times 10^{-1} - 0.10\n\\times 10^{-1} = 0.011 - 0.010 = 0.001. \\]\nAlternatively, we can brute force search for this:\n\n\nThe minimum difference min_diff=0.0010\nat x=+0.57 x 10^{-1} y=+0.58 x 10^{-1}.\n\n\n\n\nExercise 3.1 Consider the number system given by \\((\\beta, t, L, U) =\n(10, 3, -3, 3)\\) which gives\n\\[ x = \\pm .b_1 b_2 b_3 \\times 10^e \\text{ where } -3 \\le e \\le 3. \\]\n\nHow many numbers can be represented by this normalised system?\nWhat are the two largest positive numbers in this system?\nWhat are the two smallest positive numbers?\nWhat is the smallest possible difference between two numbers in this system?\nWhat is the smallest possible difference in this system, \\(x\\) and \\(y\\), for which \\(x &lt; 100 &lt; y\\)?\n\n\n\nExample 3.3 (What about in python) We find that even with double-precision floating point numbers, we see sum funniness when working with decimals:\n\na = np.double(0.0)\n\n# add 0.1 ten times to get 1?\nfor _ in range(10):\n    a = a + np.double(0.1)\n    print(a)\n\nprint(\"Is a = 1?\", a == 1.0)\n\n0.1\n0.2\n0.30000000000000004\n0.4\n0.5\n0.6\n0.7\n0.7999999999999999\n0.8999999999999999\n0.9999999999999999\nIs a = 1? False\n\n\n\nWhy is this output not a surprise?\n\nWe also see that even adding up numbers can have different results depending on what order we add them:\n\nx = np.double(1e30)\ny = np.double(-1e30)\nz = np.double(1.0)\n\nprint(f\"{(x + y) + z=:.16f}\")\n\n(x + y) + z=1.0000000000000000\n\n\n\nprint(f\"{x + (y + z)=:.16f}\")\n\nx + (y + z)=0.0000000000000000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#errors-and-machine-precision",
    "href": "src/lec02.html#errors-and-machine-precision",
    "title": "3  Floating point number systems",
    "section": "3.3 Errors and machine precision",
    "text": "3.3 Errors and machine precision\nFrom now on \\(fl(x)\\) will be used to represent the (approximate) stored value of \\(x\\). The error in this representation can be expressed in two ways.\n\\[ \\begin{aligned} \\text{Absolute error} &= | fl(x) - x | \\\\ \\text{Relative\nerror} &= \\frac{| fl(x) - x |}{|x|}. \\end{aligned} \\]\nThe number \\(fl(x)\\) is said to approximate \\(x\\) to \\(t\\) significant digits (or figures) if \\(t\\) is the largest non-negative integer for which\n\\[ \\text{Relative error} &lt; 0.5 \\times \\beta^{1-t}. \\]\nIt can be proved that if the relative error is equal to \\(\\beta^{-d}\\) then \\(fl(x)\\) has \\(d\\) correct significant digits.\nIn the number system given by \\((\\beta, t, L, U)\\), the nearest (larger) representable number to \\(x = 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e\\) is\n\\[ \\tilde{x} = x + .\\underbrace{000\\ldots01}_{t \\text{ digits}} \\times \\beta^e =\nx + \\beta^{e-t} \\]\nAny number \\(y \\in (x, \\tilde{x})\\) is stored as either \\(x\\) or \\(\\tilde{x}\\) by rounding to the nearest representable number, so\n\nthe largest possible error is \\(\\frac{1}{2} \\beta^{e-t}\\),\nwhich means that \\(| y - fl(y) | \\le \\frac{1}{2} \\beta^{e-t}\\).\n\nIt follow from \\(y &gt; x \\ge .100 \\ldots 00 \\times \\beta^e = \\beta^{e-1}\\) that\n\\[ \\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\frac{\\beta^{e-t}}{\\beta^{e-1}} =\n\\frac{1}{2} \\beta^{1-t}, \\]\nand this provides a bound on the relative error: for any \\(y\\)\n\\[ \\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\beta^{1-t}. \\]\nThe last term is known as machine precision or unit roundoff and is often called \\(eps\\). This is obtained in Python with\n\neps = np.finfo(np.double).eps\nprint(eps)\n\n2.220446049250313e-16\n\n\n\nExample 3.4  \n\nThe number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-2} = 0.05. \\]\nThe number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-3} = 0.005. \\]\nThe number system \\((\\beta, t, L, U) = (10, 7, 2, 10)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-7} = 0.000005. \\]\nFor some common types in python, we see the following values:\n\n\nfor dtype in [np.half, np.single, np.double]:\n    print(dtype.__name__, np.finfo(dtype).eps)\n\nfloat16 0.000977\nfloat32 1.1920929e-07\nfloat64 2.220446049250313e-16\n\n\n\n\nMachine precision epsilon (\\(eps\\)) gives us an upper bound for the error in the representation of a floating point number in a particular system. We note that this is different to the smallest possible numbers that we are able to store!\n\neps = np.finfo(np.double).eps\nprint(f\"{eps=}\")\n\nsmaller = eps\nfor _ in range(5):\n    smaller = smaller / 10.0\n    print(f\"{smaller=}\")\n\neps=np.float64(2.220446049250313e-16)\nsmaller=np.float64(2.2204460492503132e-17)\nsmaller=np.float64(2.220446049250313e-18)\nsmaller=np.float64(2.220446049250313e-19)\nsmaller=np.float64(2.2204460492503132e-20)\nsmaller=np.float64(2.2204460492503133e-21)\n\n\n\nArithmetic operations are usually carried out as though infinite precision is available, after which the result is rounded to the nearest representable number.\nThis means that arithmetic cannot be completely trusted and the usual rules don’t necessarily apply!\n\nExample 3.5 Consider the number system \\((\\beta, t, L, U) = (10, 2,\n-1, 2)\\) and take\n\\[ x = .10 \\times 10^2, \\quad y = .49 \\times 10^0, \\quad z = .51 \\times 10^0. \\]\n\nIn exact arithmetic \\(x + y = 10 + 0.49 = 10.49\\) and \\(x + z = 10 + 0.51 =\n10.51\\).\nIn this number system rounding gives\n\\[ fl(x+y) = .10 \\times 10^2 = x, \\qquad fl(x+z) = .11 \\times 10^2 \\neq x.\n  \\]\n\n(Note that \\(\\frac{y}{x} &lt; eps\\) but \\(\\frac{z}{x} &gt; eps\\).)\nEvaluate the following expression in this number system.\n\\[ x+(y+y), \\quad (x+y)+y, \\quad x+(z+z), \\quad (x+z) +z. \\]\n(Also note the benefits of adding the smallest terms first!)\n\n\nExample 3.6 (Computing derivatives with floating point numbers) Suppose we want to compute the derivative of \\(f(x) = x^3\\) at \\(x = 1\\) using the definition of limits and floating point numbers: \\[ f'(x) = \\lim_{\\Delta x \\to\n0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}. \\] We know that \\(f'(x) = 3 x^2\\) so \\(f'(1) = 3\\). We hope that using floating point numbers gives something similar:\n\ndef f(x):\n    return x**3\n\n\nx0 = np.double(1.0)\n\nprint(\"Delta_x   Approx  Abs Error  Rel Error\")\n\nfor j in range(20):\n    Delta_x = 10 ** (-j)\n\n    deriv_approx = (f(x0 + Delta_x) - f(x0)) / Delta_x\n    abs_error = abs(3.0 - deriv_approx)\n    rel_error = abs_error / 3.0\n\n    print(\n        f\" {Delta_x:.1e}   {deriv_approx:.4f}  {abs_error:.4e}  {rel_error:.4e}\"\n    )\n\nDelta_x   Approx  Abs Error  Rel Error\n 1.0e+00   7.0000  4.0000e+00  1.3333e+00\n 1.0e-01   3.3100  3.1000e-01  1.0333e-01\n 1.0e-02   3.0301  3.0100e-02  1.0033e-02\n 1.0e-03   3.0030  3.0010e-03  1.0003e-03\n 1.0e-04   3.0003  3.0001e-04  1.0000e-04\n 1.0e-05   3.0000  3.0000e-05  1.0000e-05\n 1.0e-06   3.0000  2.9998e-06  9.9993e-07\n 1.0e-07   3.0000  3.0151e-07  1.0050e-07\n 1.0e-08   3.0000  3.9720e-09  1.3240e-09\n 1.0e-09   3.0000  2.4822e-07  8.2740e-08\n 1.0e-10   3.0000  2.4822e-07  8.2740e-08\n 1.0e-11   3.0000  2.4822e-07  8.2740e-08\n 1.0e-12   3.0003  2.6670e-04  8.8901e-05\n 1.0e-13   2.9976  2.3978e-03  7.9928e-04\n 1.0e-14   2.9976  2.3978e-03  7.9928e-04\n 1.0e-15   3.3307  3.3067e-01  1.1022e-01\n 1.0e-16   0.0000  3.0000e+00  1.0000e+00\n 1.0e-17   0.0000  3.0000e+00  1.0000e+00\n 1.0e-18   0.0000  3.0000e+00  1.0000e+00\n 1.0e-19   0.0000  3.0000e+00  1.0000e+00\n\n\nWe see that if Delta_x is not too small, we do an okay job. But if Delta_x is too small we start to have problems!\n\n\nExercise 3.2 (More examples)  \n\nVerify that a similar problem arises for the numbers\n\\[ x = .85 \\times 10^0, \\quad y = .3 \\times 10^{-2}, \\quad z = .6 \\times\n  10^{-2}, \\]\n\nin the system \\((\\beta, t, L, U) = (10, 2, -3, 3)\\).\n\nGiven the number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) and \\(x =\n.100\\times 10^3\\), find nonzero numbers \\(y\\) and \\(z\\) from this system for which \\(fl(x+y) = x\\) and \\(fl(x+z) &gt; x\\).\n\n\nIt is sometimes helpful to think of another machine precision epsilon in other way: Machine precision epsilon is the smallest positive number \\(eps\\) such that \\(1 + eps &gt; 1\\), i.e. it is half the difference between \\(1\\) and the next largest representable number.\nExamples:\n\nFor the number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\),\n\\[ \\begin{array}{ccl} & .11 \\times 10^1 & \\qquad \\leftarrow {\\text{next\n  number}} \\\\\n   - & .10 \\times 10^1 & \\qquad \\leftarrow 1 \\\\ \\hline & .01 \\times 10^1 &\n     \\qquad \\leftarrow 0.1 \\end{array} \\]\n\nso \\(eps = \\frac{1}{2}(0.1) = 0.05\\).\n\nVerify that this approaches gives the previously calculated value for \\(eps\\) in the number system given by \\((\\beta, t, L, U) = (10, 3, -3, 3)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#other-features-of-finite-precision",
    "href": "src/lec02.html#other-features-of-finite-precision",
    "title": "3  Floating point number systems",
    "section": "3.4 Other “Features” of finite precision",
    "text": "3.4 Other “Features” of finite precision\nWhen working with floating point numbers there are other things we need to worry about too!\n\nOverflow\n\nthe number is too large to be represented, e.g. multiply the largest representable number by 10. This gives inf (infinity) with numpy.doubles and is usually “fatal”.\n\nUnderflow\n\nthe number is too small to be represented, e.g. divide the smallest representable number by 10. This gives \\(0\\) and may not be immediately obvious.\n\nDivide by zero\n\ngives a result of inf, but \\(\\frac{0}{0}\\) gives nan (not a number)\n\nDivide by inf\n\ngives \\(0.0\\) with no warning",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#is-this-all-academic",
    "href": "src/lec02.html#is-this-all-academic",
    "title": "3  Floating point number systems",
    "section": "3.5 Is this all academic?",
    "text": "3.5 Is this all academic?\nNo! There are many examples of major software errors that have occurred due to programmers not understanding the issues associated with computer arithmetic…\n\nIn February 1991, a basic rounding error within software for the US Patriot missile system caused it to fail, contributing to the loss of 28 lives.\nIn June 1996, the European Space Agency’s Ariane Rocket exploded shortly after take-off: the error was due to failing to handle overflow correctly.\nIn October 2020, a driverless car drove straight into a wall due to faulty handling of a floating point error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#summary",
    "href": "src/lec02.html#summary",
    "title": "3  Floating point number systems",
    "section": "3.6 Summary",
    "text": "3.6 Summary\n\nThere is inaccuracy in almost all computer arithmetic.\nCare must be taken to minimise its effects, for example:\n\nadd the smallest terms in an expression first;\navoid taking the difference of two very similar terms;\neven checking whether \\(a = b\\) is dangerous!\n\nThe usual mathematical rules no longer apply.\nThere is no point in trying to compute a solution to a problem to a greater accuracy than can be stored by the computer.\n\n\n3.6.1 Further reading\n\nWikipedia: Floating-point arithmetic\nDavid Goldberg, What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys, Volume 23, Issue 1, March 1991.\nJohn D Cook, Floating point error is the least of my worries, online, November 2011.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec03.html",
    "href": "src/lec03.html",
    "title": "4  Introduction to systems of linear equations",
    "section": "",
    "text": "4.1 Definition of systems of linear equations\nGiven an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies: \\[\\begin{equation}\n\\label{eq:sle}\nA \\vec{x} = \\vec{b}.\n\\end{equation}\\]\nWe can also write \\(\\eqref{eq:sle}\\) as a system of linear equations: \\[\\begin{align*}\n\\text{Equation 1:} &&\na_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} &&\na_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} &&\na_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} &&\na_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nNotes:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#definition-of-systems-of-linear-equations",
    "href": "src/lec03.html#definition-of-systems-of-linear-equations",
    "title": "4  Introduction to systems of linear equations",
    "section": "",
    "text": "The values \\(a_{ij}\\) are known as coefficients.\nThe right hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#can-we-do-it",
    "href": "src/lec03.html#can-we-do-it",
    "title": "4  Introduction to systems of linear equations",
    "section": "4.2 Can we do it?",
    "text": "4.2 Can we do it?\nOur first question might be - is it possible to solve \\(\\eqref{eq:sle}\\)?\nWe know a few simple cases where we can answer this question very quickly:\n\nIf \\(A = I_n\\), the \\(n \\times n\\) identity matrix, then we can solve this problem: \\[\\vec{x} = \\vec{b}.\\]\nIf \\(A = O\\), the \\(n \\times n\\) zero matrix, and \\(\\vec{b} \\neq \\vec{0}\\), the zero vector, then we cannot solve this problem: \\[O \\vec{x} = \\vec{0} \\neq\n\\vec{b} \\quad \\text{for any vector} \\quad \\vec{x}.\\]\nIf \\(A\\) is invertible, with inverse \\(A^{-1}\\), then we can solve this problem: \\[\\vec{x} = A^{-1} \\vec{b}.\\] But, in general, this is a very bad idea and we will see algorithms that are more efficient than finding the inverse of \\(A\\).\n\n\nRemark 4.1. One way to solve a system of linear equations is to compute the inverse of \\(A\\), \\(A^{-1}\\), directly, then the solution is found through matrix multiplication: \\(\\vec{x} = A^{-1} \\vec{b}\\). This turns out to be an inefficient approach and we can do better with specialised algorithms.\n\n\n\n\nTrying different approaches for problem size 100\nApproach 1 - inverting the matrix and applying the inverse. Time = 0.000989675521850586\nApproach 2 - solving the system of linear equations. Time = 0.0001552104949951172\nApproach 2 is faster by a factor of  6.376344086021505\n\n\nTrying different approaches for problem size 10000\nApproach 1 - inverting the matrix and applying the inverse. Time = 31.502981901168823\nApproach 2 - solving the system of linear equations. Time = 8.1720290184021\nApproach 2 is faster by a factor of  3.854976754271082\n\n\n\nThere are tools to help us answer when a matrix is invertible which arise naturally when thinking about what \\(A \\vec{x} = \\vec{b}\\) means! We have to go back to the basic operations on vectors.\nThere are two fundamental operations you can do on vectors: addition and scalar multiplication. Consider the vectors: \\[\\begin{align*}\n\\vec{a} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}, \\quad\n\\vec{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}, \\quad\n\\vec{c} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 6 \\end{pmatrix}.\n\\end{align*}\\] Then, we can easily compute the following linear combinations \\[\\begin{align}\n\\label{eq:vector-linear-comb-a}\n\\vec{a} + \\vec{b} & = \\begin{pmatrix} 3 \\\\ 3 \\\\ 6 \\end{pmatrix} \\\\\n\\label{eq:vector-linear-comb-b}\n\\vec{c} - 2 \\vec{a} & = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} \\\\\n\\label{eq:vector-linear-comb-c}\n\\vec{a} + 2 \\vec{b} + 2 \\vec{c} &= \\begin{pmatrix} 12 \\\\ 9 \\\\ 22 \\end{pmatrix}.\n\\end{align}\\] Now if we write \\(A\\) for the \\(3 \\times 3\\)-matrix whose columns are \\(\\vec{a},\n\\vec{b}, \\vec{c}\\): \\[\\begin{align*}\nA = \\begin{pmatrix}\n&& \\\\ \\vec{a} & \\vec{b} & \\vec{c} \\\\ &&\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix},\n\\end{align*}\\] then the three equations \\(\\eqref{eq:vector-linear-comb-a}\\), \\(\\eqref{eq:vector-linear-comb-b}\\), \\(\\eqref{eq:vector-linear-comb-c}\\), can be written as \\[\\begin{align*}\n\\vec{a} + \\vec{b} & = 1 \\vec{a} + 1 \\vec{b} + 0 \\vec{c}\n= A \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\\\\n\\vec{c} - 2\\vec{a} & = -2 \\vec{a} + 0 \\vec{b} - 2 \\vec{c}\n= A \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\\\\n\\vec{a} + 2 \\vec{b} + 2 \\vec{c} & = 1 \\vec{a} + 2 \\vec{b} + 2 \\vec{c}\n= A \\begin{pmatrix} 1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\end{align*}\\] In other words,\n\nWe can write any linear combination of vectors as a matrix-vector multiply,\n\nor if we reverse the process,\n\nWe can write matrix-vector multiplication as a linear combination of the columns of the matrix.\n\nThis rephrasing means, solving the system \\(A \\vec{x} = \\vec{b}\\) is equivalent to finding a linear combination of the columns of \\(A\\) which is equal to \\(b\\). So, our question about whether we can solve \\(\\eqref{eq:sle}\\), can also be rephrased as: does there exist a linear combination of the columns of \\(A\\) which is equal to \\(\\vec{b}\\)? We will next write this condition mathematically using the concept of span.\n\n4.2.1 The span of a set of vectors\n\nDefinition 4.1 Given a set of vectors of the same size, \\(S = \\{ \\vec{v}_1,\n\\ldots, \\vec{v}_k \\}\\), we say the span of \\(S\\) is the set of all vectors which are linear combinations of vectors in \\(S\\): \\[\\begin{equation}\n\\mathrm{span}(S) = \\left\\{ \\sum_{i=1}^k x_i \\vec{v}_i : x_i \\in \\mathbb{R}\n\\text{ for } i = 1, \\ldots, k \\right\\}.\n\\end{equation}\\]\n\n\nExample 4.1 Consider three new vectors \\[\\begin{align*}\n\\vec{a} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} \\quad\n\\vec{b} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\quad\n\\vec{c} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\end{align*}\\]\n\n\n\n\n\nThree example vectors in a plane.\n\n\n\n\n\nLet \\(S = \\{ \\vec{a} \\}\\), then \\(\\mathrm{span}(S) = \\{ x \\vec{a} : x \\in\n\\mathbb{R} \\}\\). Geometrically, we can think of the span of a single vector to be an infinite straight line which passes through the origin and \\(\\vec{a}\\).\nLet \\(S = \\{ \\vec{a}, \\vec{b} \\}\\), then \\(\\mathrm{span}(S) = \\mathbb{R}^2\\). To see this is true, we first see that \\(\\mathrm{span}(S)\\) is contained in \\(\\mathbb{R}^2\\) since any \\(2\\)-vectors added together and the scalar multiplication of a \\(2\\)-vector also form a \\(2\\)-vector. For the opposite inclusion, consider an arbitrary point \\(\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2\n\\end{pmatrix} \\in \\mathbb{R}^2\\) then \\[\\begin{align}\n\\nonumber\n\\frac{2 y_1 + y_2}{7} \\vec{a} + \\frac{-3 y_1 + 2 y_2}{7} \\vec{b}\n= \\frac{2 y_1 + y_2}{7} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n- \\frac{-3 y_1 + 2 y_2}{7} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\\\\n= \\begin{pmatrix}\n\\frac{4 y_1 + 2 y_2}{7} + \\frac{3 y_1 - 2 y_2}{7} \\\\\n\\label{eq:y-combo}\n\\frac{6 y_1 + 3 y_2}{7} + \\frac{-6 y_1 + 4 y_2}{7}\n\\end{pmatrix}\n= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\vec{y}.\n\\end{align}\\] This calculation shows, that we can always form a linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) which results in \\(\\vec{y}\\).\nLet \\(S = \\{ \\vec{a}, \\vec{b}, \\vec{c} \\}\\), then \\(\\mathrm{span}(S) =\n\\mathbb{R}^2\\). Since \\(\\vec{c} \\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\), any linear combination of \\(\\vec{a}, \\vec{b}, \\vec{c}\\) has an equivalent combination of just \\(\\vec{a}\\) and \\(\\vec{b}\\). In formulae, we can see that by applying the formula from Example 2 (TODO make ref), we have \\[\\begin{align*}\n\\vec{c} = \\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b}.\n\\end{align*}\\] So we have, if \\(\\vec{y} \\in \\mathrm{span}(S)\\), then \\[\\begin{align*}\n\\vec{y} = x_1 \\vec{a} + x_2 \\vec{b} + x_3 \\vec{c} & \\Rightarrow \\vec{y}\n= (x_1 + \\frac{1}{7} x_3) \\vec{a} + (x_2 - \\frac{5}{7} x_3) \\vec{b},\n\\end{align*}\\] so \\(\\vec{y} \\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\). Conversely, if \\(\\vec{y}\n\\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\), then \\[\\begin{align*}\n\\vec{y} = x_1 \\vec{a} + x_2 \\vec{b} & \\Rightarrow \\vec{y}\n= x_1 \\vec{a} + x_2 \\vec{b} + 0\n\\vec{c}.\n\\end{align*}\\] So the span of \\(S = \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\n= \\mathbb{R}^2\\). Notice that we this final linear combination of \\(\\vec{a},\n\\vec{b}\\) and \\(\\vec{c}\\) to form \\(\\vec{y}\\) is not unique.\n\n\nSo our first statement is that \\(\\eqref{eq:sle}\\) has a solution if \\(\\vec{b}\\) is in the span of the columns of \\(A\\). However, as we saw with Example 4.1, Part 3, we are not guaranteed that the linear combination is unique! For this we need a further condition.\n\n\n4.2.2 Linear independence\n\nDefinition 4.2 Given a set of vectors of the same size, \\(S = \\{\\vec{v}_1, \\ldots, \\vec{v}_k\n\\}\\), we say that \\(S\\) is linearly dependent, if there exists numbers \\(x_1, x_2,\n\\ldots x_k\\), not all zero, such that \\[\\begin{align*}\n\\sum_{i=1}^k x_i \\vec{v}_i = \\vec{0}.\n\\end{align*}\\] The set \\(S\\) is linearly independent if it is not linearly dependent.\n\nExercise 4.1 Can you write the definition of a linearly independent set of vectors explicitly?\n\n\n\nExample 4.2 Continuing from Example 4.1.\n\nLet \\(S = \\{ \\vec{a}, \\vec{b} \\}\\), then \\(S\\) is linearly independent. Indeed, let \\(x_1, x_2\\) be real numbers such that \\[\\begin{align*}\nx_1 \\vec{a} + x_2 \\vec{b} = \\vec{0},\n\\end{align*}\\] then, \\[\\begin{align*}\n2 x_1 - x_2 & = 0 && 3 x_1 + 2 x_2 & = 0\n\\end{align*}\\] The first equation says that \\(x_2 = 2 x_1\\), which when substituted into the second equation gives \\(3 x_1 + 4 x_1 = 7 x_1 = 0\\). Together this implies that \\(x_1 = x_2 = 0\\). Put simply this means that if we do have a linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) which is equal zero, then the corresponding scalar multiples are all zero.\nLet \\(S = \\{ \\vec{a}, \\vec{b}, \\vec{c} \\}\\), then \\(S\\) is linearly dependent. We have previously seen that: \\[\\begin{align*}\n\\vec{c} = \\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b},\n\\end{align*}\\] which we can rearrange to say that \\[\\begin{align*}\n\\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b} - \\vec{c} = \\vec{0}.\n\\end{align*}\\] We see that the definition of linear dependence is satisfied for \\(x_1 = \\frac{1}{7}, x_2 = -\\frac{5}{7}, x_3 = -1\\) which are all nonzero.\n\n\nSo linear independence removes the multiplicity (or non-uniqueness) in how we form linear combinations! This leads us to the final definition of this section.\n\n\n4.2.3 When vectors form a basis\n\nDefinition 4.3 We say that a set of \\(n\\)-vectors \\(S\\) is a basis of a set of \\(n\\)-vectors \\(V\\) if the span of \\(S\\) is \\(V\\) and \\(S\\) is linearly independent.\n\n\nExample 4.3  \n\nFrom Example 4.1, we have that \\(S = \\{ \\vec{a}, \\vec{b} \\}\\) is a basis of \\(\\mathbb{R}^2\\).\nAnother (perhaps simpler) basis of \\(\\mathbb{R}^2\\) are the coordinate axes: \\[\\begin{align*}\n\\vec{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad\n\\text{and} \\quad \\vec{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n\\end{align*}\\]\nWhen we look at eigenvectors and eigenvalues we will see that there are other convenient bases (plural of basis) to work with.\n\n\nWe phrase the idea that the existence and uniqueness of linear combinations together depend on the underlying set being a basis mathematically in the following Theorem:\n\nTheorem 4.1 Let \\(S\\) be a basis of \\(V\\). Then any vector in \\(V\\) can be written uniquely as a linear combination of entries in \\(S\\).\n\n\nExample 4.4  \n\nFrom the main examples in this section, we have that \\(S = \\{ \\vec{a}, \\vec{b}\n\\}\\) is a basis of \\(\\mathbb{R}^2\\) and we already know the formula for how to write \\(\\vec{y}\\) as a unique combination of \\(\\vec{a}\\) and \\(\\vec{b}\\): it’s given in \\(\\eqref{eq:y-combo}\\).\nFor the simpler example of the coordinate axes: \\[\\begin{align*}\n\\vec{e}_1 =\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad\n\\vec{e}_2 =\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n\\end{align*}\\] we have that for any \\(\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n= y_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} +\ny_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = y_1 \\vec{e}_1 + y_2 \\vec{e}_2.\n\\end{align*}\\]\n\n\n\nProof (Proof of Theorem 4.1). Let \\(\\vec{y}\\) be a vector in \\(V\\) and label \\(S = \\{ \\vec{v}_1, \\ldots \\vec{v}_k\n\\}\\). Since \\(S\\) forms a basis of \\(V\\), \\(\\vec{y} \\in \\mathrm{span}(S)\\) so there exists numbers \\(x_1, \\ldots, x_k\\) such that \\[\\begin{align}\n\\label{eq:basis_pf_x}\n\\vec{y} = \\sum_{i=1}^k x_i \\vec{v}_i.\n\\end{align}\\] Suppose that there exists another set of number \\(z_1, \\ldots z_k\\) such that \\[\\begin{align}\n\\label{eq:basis_pf_z}\n\\vec{y} = \\sum_{i=1}^k z_i \\vec{v}_i.\n\\end{align}\\] Taking the difference of \\(\\eqref{eq:basis_pf_x}\\) and \\(\\eqref{eq:basis_pf_z}\\), we see that \\[\\begin{align}\n\\vec{0} = \\sum_{i=1}^k (x_i - z_i) \\vec{v}_i.\n\\end{align}\\] Since \\(S\\) is linearly independent, this implies \\(x_i = z_i\\) for \\(i = 1, \\ldots,\nk\\), and we have shown that there is only one linear combination of the vectors \\(\\{ \\vec{v}_i \\}\\) to form \\(\\vec{y}\\).\n\nThere is a theorem that says that the number of vectors in any basis of a given ‘nice’ set of vectors \\(V\\) is the same but is beyond the scope of this module!\n\nTheorem 4.2 Let \\(A\\) be a \\(n \\times n\\)-matrix. If the columns of \\(A\\) form a basis of \\(\\mathbb{R}^n\\), then there exists a unique \\(n\\)-vector \\(\\vec{x}\\) which satisfies \\(A \\vec{x} = \\vec{b}\\).\n\nWe do not give full details of the proof here since all the key ideas are already given above.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#special-types-of-matrices",
    "href": "src/lec03.html#special-types-of-matrices",
    "title": "4  Introduction to systems of linear equations",
    "section": "4.3 Special types of matrices",
    "text": "4.3 Special types of matrices\nThe general matrix \\(A\\) before the examples is known as a full matrix: any of its components \\(a_{ij}\\) might be nonzero.\nAlmost always the problem being solved leads to a matrix with a particular structure of entries: Some entries may be known to be zero. If this is the case then it is often possible to use this knowledge to improve the efficiency of the algorithm (in terms of both speed and/or storage).\n\nExample 4.5 (Triangular matrix) One common (and important) structure takes the form\n\\[\nA = \\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\ a_{21} & a_{22} & 0 & \\cdots &\n0 \\\\ a_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots\n& \\vdots \\\\ a_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn} \\end{pmatrix}.\n\\]\n\nA is a lower triangular matrix. Every entry above the leading diagonal is zero:\n\\[ a_{ij} = 0 \\quad \\text{ for } \\quad j &gt; i. \\]\nThe transpose of this matrix is an upper triangular matrix and can be treated in a very similar manner.\n\n\n\nExample 4.6 (Sparse matrices) Sparse matrices are extremely common in any application which relies on some form of graph structure (see both the temperature, Example 2.3, and traffic network examples, Example 2.4).\n\nThe \\(a_{ij}\\) typically represents some form of “communication” between vertices \\(i\\) and \\(j\\) of the graph, so the element is only nonzero if the vertices are connected.\nThere is no generic pattern for these entries, though there is usually one that is specific to the problem solved.\nUsually \\(a_{ii} \\neq 0\\) - the diagonal is nonzero.\nA “large” portion of the matrix is zero.\n\nA full \\(n \\times n\\) matrix has \\(n^2\\) nonzero entries.\nA sparse \\(n \\times n\\) has \\(\\alpha n\\) nonzero entries, where \\(\\alpha \\ll\n    n\\).\n\nMany special techniques exist for handling sparse matrices, some of which can be used automatically within Python (scipy.sparse documentation)\n\n\nWhat is the significance of these special examples?\n\nIn the next section we will discuss a general numerical algorithm for the solution of linear systems of equations.\nThis will involve reducing the problem to one involving a triangular matrix which, as we show below, is relatively easy to solve.\nIn subsequent lectures, we will see that, for sparse matrix systems, alternative solution techniques are available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#further-reading",
    "href": "src/lec03.html#further-reading",
    "title": "4  Introduction to systems of linear equations",
    "section": "4.4 Further reading",
    "text": "4.4 Further reading\n\nWikipedia: Systems of linear equations (includes a nice geometric picture of what a system of linear equations means).\nMaths is fun: Systems of linear equations (very basic!)\nGregory Gundersen Why shouldn’t I invert that matrix?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html",
    "href": "src/lec04.html",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "",
    "text": "5.1 Reminder of the problem\nThis is the first method we will use to solve systems of linear equations. We will see that by using the approach of Gaussian elimination (and variations of that method) we can solve any system of linear equations that have a solution.\nRecall the problem is to solve a set of \\(n\\) linear equations for \\(n\\) unknown values \\(x_j\\), for \\(j=1, 2, \\ldots, n\\).\nNotation:\n\\[\\begin{align*}\n\\text{Equation } 1:\n&& a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation } 2:\n&& a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation } i:\n&& a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation } n:\n&& a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nWe can also write the system of linear equations in general matrix-vector form:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\na_{31} & a_{32} & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nRecall the \\(n \\times n\\) matrix \\(A\\) represents the coefficients that multiply the unknowns in each equation (row), while the \\(n\\)-vector \\(\\vec{b}\\) represents the right-hand-side values.\nOur strategy will be to reduce the system to a triangular system of matrices which is then easy to solve!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#elementary-row-operations",
    "href": "src/lec04.html#elementary-row-operations",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.2 Elementary row operations",
    "text": "5.2 Elementary row operations\nConsider equation \\(p\\) of the above system:\n\\[\na_{p1} x_1 + a_{p2} x_2 + a_{p3} x_3 + \\cdots + a_{pn} x_n = b_p,\n\\]\nand equation \\(q\\):\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q.\n\\]\nNote three things…\n\nThe order in which we choose to write the \\(n\\) equations is irrelevant\nWe can multiply any equation by an arbitrary real number (\\(k \\neq 0\\) say):\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p.\n\\]\nWe can add any two equations:\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p\n\\]\n\nadded to\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q\n\\]\nyields\n\\[\n(k a_{p1} + a_{q1}) x_1 + (k a_{p2} + a_{q2}) x_2 + \\cdots + (k a_{pn} +\na_{qn}) x_n = k b_p + b_q.\n\\]\n\nExample 5.1 Consider the system \\[\\begin{align}\n2 x_1 + 3 x_2 & = 4  \\label{eq:eg1a} \\\\\n-3 x_1 + 2 x_2 & = 7 \\label{eq:eg1b}.\n\\end{align}\\] Then we have: \\[\\begin{align*}\n4 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 8 x_1 + 12 x_2 & = 16 \\\\\n-15. \\times \\text{\\eqref{eq:eg1b}} & \\rightarrow & 4.5 x_2 - 3 x_2 & = -10.5 \\\\\n\\text{\\eqref{eq:eg1a}} + \\text{\\eqref{eq:eg1b}} & \\rightarrow &\n-x_1 + 5 x_2 & = 11 \\\\\n\\text{\\eqref{eq:eg1b}} + 1.5 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow &\n0 + 6.5 x_2 & = 13.\n\\end{align*}\\]\n\n\nExercise 5.1 Consider the system \\[\\begin{align}\nx_1 + 2 x_2 & = 1 \\label{eq:eg2a} \\\\\n4 x_1 + x_2 & = -3 \\label{eq:eg2b}.\n\\end{align}\\] Work out the result of these elementary row operations: \\[\\begin{align*}\n2 \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n0.25 \\times \\text{\\eqref{eq:eg2b}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-1) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-4) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow\n\\end{align*}\\]\n\nFor a system written in matrix form our three observations mean the following:\n\nWe can swap any two rows of the matrix (and corresponding right-hand side entries). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n-3 & 2\\\\ 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n7 \\\\ 4\n\\end{pmatrix}\n\\]\nWe can multiply any row of the matrix (and corresponding right-hand side entry) by a scalar. For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & \\frac{3}{2} \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7\n\\end{pmatrix}\n\\]\nWe can replace row \\(q\\) by row \\(q + k \\times\\) row \\(p\\). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n2 & 3 \\\\ 0 & 6.5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 13\n\\end{pmatrix}\n\\]\n(here we replaced row \\(w\\) by row \\(2 + 1.5 \\times\\) row \\(1\\))\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\ 4 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & 2 \\\\ 0 & -7\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -7\n\\end{pmatrix}\n\\]\n\n(here we replaced row \\(2\\) by row \\(2 + (-4) \\times\\) row \\(1\\))\nOur strategy for solving systems of linear equations using Gaussian elimination is based on the following ideas:\n\nThree types of operation described above are called elementary row operations (ERO).\nWe will applying a sequence of ERO to reduce an arbitrary system to a triangular form, which, we will see, can be easily solved.\nThe algorithm for reducing a general matrix to upper triangular form is known as forward elimination or (more commonly) as Gaussian elimination.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#gaussian-elimination",
    "href": "src/lec04.html#gaussian-elimination",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.3 Gaussian elimination",
    "text": "5.3 Gaussian elimination\nThe algorithm of Gaussian elimination is a very old method that you may have already met at school - perhaps by a different name. The details of the method may seem quite confusing at first. Really we are following the ideas of eliminating systems of simultaneous equations, but in a way a computer understands. This is an important point in this section. You will have seen different ideas of how to solve systems of simulations equations where the first step is to “look at the equations to decide the easiest first step”. When there are \\(10^9\\) equations, its not effective for a computer to try and find an easy way through the problem. It must instead of a simple set of instructions to follow: this will be our algorithm.\nThe method is so old, in fact we have evidence of Chinese mathematicians using Gaussian elimination in 179CE (From Wikipedia):\n\nThe method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE. It was commented on by Liu Hui in the 3rd century.\nThe method in Europe stems from the notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems. The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.\n\n\n\n\n\n\nImage from Nine Chapter of the Mathematical art. By Yang Hui(1238-1298) - mybook, Public Domain, https://commons.wikimedia.org/w/index.php?curid=10317744\n\n\n\n\n\n5.3.1 The algorithm\nThe following algorithm systematically introduces zeros into the system of equations, below the diagonal.\n\nSubtract multiples of row 1 from the rows below it to eliminate (make zero) nonzero entries in column 1.\nSubtract multiplies of the new row 2 from the rows below it to eliminate nonzero entries in column 2.\nRepeat for row \\(3, 4, \\ldots, n-1\\).\n\nAfter row \\(n-1\\) all entities below the diagonal have been eliminated, so \\(A\\) is now upper triangular and the resulting system can be solved by backward substitution.\n\nExample 5.2 Use Gaussian eliminate to reduce the following system of equations to upper triangular form:\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ 1 & 2 & 2 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nFirst, use the first row to eliminate the first column below the diagonal:\n\n(row 2) \\(- 0.5 \\times\\) (row 1) gives\n\\[\n\\begin{pmatrix}\n  2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 22\n\\end{pmatrix}\n\\]\n(row 3) \\(-\\) (row 1) then gives\n\\[\n\\begin{pmatrix}\n  2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & 3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 10\n\\end{pmatrix}\n\\]\n\nNow use the second row to eliminate the second column below the diagonal.\n\n(row 3) \\(- 2 \\times\\) (row 2) gives\n\\[\n  \\begin{pmatrix}\n   2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & \\mathbf{0} & 2\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  x_1 \\\\ x_2 \\\\ x_3\n  \\end{pmatrix} =\n  \\begin{pmatrix}\n  12 \\\\ 3 \\\\ 4\n  \\end{pmatrix}\n  \\]\n\n\n\nExercise 5.2 Use Gaussian elimination to reduce the following system of linear equations to upper triangular form.\n\\[\n\\begin{pmatrix}\n4 & -1 & -1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n9 \\\\ -6 \\\\ 3\n\\end{pmatrix}.\n\\]\n\n\nRemark. \n\nEach row \\(i\\) is used to eliminate the entries in column \\(i\\) below \\(a_{ii}\\), i.e. it forces \\(a_{ji} = 0\\) for \\(j &gt; i\\).\nThis is done by subtracting a multiple of row \\(i\\) from row \\(j\\):\n\\[\n  (\\text{row } j) \\leftarrow (\\text{row } j) - \\frac{a_{ji}}{a_{ii}}\n  (\\text{row } i).\n  \\]\nThis guarantees that \\(a_{ji}\\) becomes zero because\n\\[\n  a_{ji} \\leftarrow a_{ji} - \\frac{a_{ji}}{a_{ii}} a_{ii} = a_{ji} - a_{ji} =\n  0.\n  \\]\n\n\n\nExercise 5.3 Solve the system\n\\[\n\\begin{pmatrix}\n4 & 3 & 2 & 1 \\\\ 1 & 2 & 2 & 2 \\\\\n1 & 1 & 3 & 0 \\\\ 2 & 1 & 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n\\end{pmatrix} =\n\\begin{pmatrix}\n10 \\\\ 7 \\\\ 5 \\\\ 8\n\\end{pmatrix}.\n\\] The solution is \\(\\vec{x} = (1, 1, 1, 1)^T\\).\n\n\n\n5.3.2 Python version\nWe start with some helper code which determines the size of the system we are working with:\n\ndef system_size(A, b):\n    \"\"\"\n    for a system of linear equations, returns the size and errors if sizes are\n    not compatible\n    \"\"\"\n    n, m = A.shape\n    nb = b.shape[0]\n\n    assert n == m, f\"matrix A is not square {A.shape=}\"\n    assert n == nb, f\"system shapes are not compatible {A.shape=} {b.shape=}\"\n\n    return n\n\nThen we can implement the elementary row operations\n\ndef row_swap(A, b, p, q):\n    \"\"\"\n    swap the rows p and q for the system of linear equations given by Ax = b.\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n    # swap rows of A\n    for j in range(n):\n        A[p, j], A[q, j] = A[q, j], A[p, j]\n    # swap rows of b\n    b[p, 0], b[q, 0] = b[q, 0], b[p, 0]\n\n\ndef row_scale(A, b, p, k):\n    \"\"\"\n    scale the entries in row p by k for the system of linear equations given by\n    Ax = b. updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    # scale row p of A\n    for j in range(n):\n        A[p, j] = k * A[p, j]\n    # scale row p of b\n    b[p, 0] = b[p, 0] * k\n\n\ndef row_add(A, b, p, k, q):\n    \"\"\"\n    add rows for the system of linear equations given by Ax = b\n    this operation is row p |-&gt; row p + k * row q\n    updates entries in place\n    \"\"\"\n    n = system_size(A, b)\n\n    for j in range(n):\n        A[p, j] = A[p, j] + k * A[q, j]\n    b[p, 0] = b[p, 0] + k * b[q, 0]\n\nLet’s test we are ok so far:\nTest 1: swapping rows\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"swapping rows 0 and 1\")\nrow_swap(A, b, 0, 1)  # remember numpy arrays are indexed starting from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nswapping rows 0 and 1\n\nnew arrays:\nA = [ -3.0,  2.0 ]\n    [  2.0,  3.0 ]\nb = [  7.0 ]\n    [  4.0 ]\n\n\n\nTest 2: scaling one row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 0 |-&gt; 0.5 * row 0\")\nrow_scale(A, b, 0, 0.5)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 0 |-&gt; 0.5 * row 0\n\nnew arrays:\nA = [  1.0,  1.5 ]\n    [ -3.0,  2.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n\n\n\nTest 3: replacing a row by that adding a multiple of another row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 1 |-&gt; row 1 + 1.5 * row 0\")\nrow_add(A, b, 1, 1.5, 0)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 1 |-&gt; row 1 + 1.5 * row 0\n\nnew arrays:\nA = [  2.0,  3.0 ]\n    [  0.0,  6.5 ]\nb = [  4.0 ]\n    [ 13.0 ]\n\n\n\nNow we can define our Gaussian elimination function. We update the values in-place to avoid extra memory allocations.\n\ndef gaussian_elimination(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation to reduce the system of linear equations Ax=b to\n    upper triangular form. use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"  row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print()\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nWe can try our code on Example 1:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"final system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\n# test that A is really upper triangular\nprint(\"Is A really upper triangular?\", np.allclose(A, np.triu(A)))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nfinal system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nIs A really upper triangular? True",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#solving-triangular-systems-of-equations",
    "href": "src/lec04.html#solving-triangular-systems-of-equations",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.4 Solving triangular systems of equations",
    "text": "5.4 Solving triangular systems of equations\nA general lower triangular system of equations has \\(a_{ij} = 0\\) for \\(j &gt; i\\) and takes the form:\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nNote the first equation is\n\\[\na_{11} x_1 = b_1.\n\\]\nThen \\(x_i\\) can be found by calculating\n\\[\nx_i = \\frac{1}{a_{ii}} \\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j \\right)\n\\]\nfor each row \\(i = 1, 2, \\ldots, n\\) in turn.\n\nEach calculation requires only previously computed values \\(x_j\\) (and the sum gives a loop for \\(j &lt; i\\).\nThe matrix \\(A\\) must have nonzero diagonal entries\ni.e. \\(a_{ii} \\neq 0\\) for \\(i = 1, 2, \\ldots, n\\).\nUpper triangular systems of equations can be solved in a similar manner.\n\n\nExample 5.3 Solve the lower triangular system of equations given by\n\\[\n\\begin{aligned}\n2 x_1 && && &= 2 \\\\\nx_1 &+& 2 x_2 && &= 7 \\\\\n2 x_1 &+& 4 x_2 &+& 6 x_3 &= 26\n\\end{aligned}\n\\]\nor, equivalently,\n\\[\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7 \\\\ 26\n\\end{pmatrix}.\n\\]\nThe solution can be calculated systematically from\n\\[\n\\begin{aligned}\nx_1 &= \\frac{b_1}{a_{11}} = \\frac{2}{2} = 1 \\\\\nx_2 &= \\frac{b_2 - a_{21} x_1}{a_{22}}\n= \\frac{7 - 1 \\times 1}{2} = \\frac{6}{2} = 3 \\\\\nx_3 &= \\frac{b_3 - a_{31} x_1 - a_{32} x_2}{a_33}\n= \\frac{26 - 2 \\times 1 - 4 \\times 3}{6}  = \\frac{12}{6}\n= 2\n\\end{aligned}\n\\]\nwhich gives the solution \\(\\vec{x} = (1, 3, 2)^T\\).\n\n\nExercise 5.4 Solve the upper triangular linear system given by\n\\[\n\\begin{aligned}\n2 x_1 &+& x_2 &+& 4 x_3 &=& 12 \\\\\n&& 1.5 x_2 && &=& 3 \\\\\n&& && 2 x_3 &=& 4\n\\end{aligned}.\n\\]\n\n\nRemark 5.1. \n\nIt is simple to solve a lower (upper) triangular system of equations (provided the diagonal is non-zero).\nThis process is often referred to as forward (backward) substitution.\nA general system of equations (i.e. a full matrix \\(A\\)) can be solved rapidly once it has been reduced to upper triangular form. This is the idea of using Gaussian elimination with backward substitution.\n\n\nWe can define functions to solve both upper or lower triangular form systems of linear equations:\n\ndef forward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is lower\n    triangular. returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is lower triangular\n    assert np.allclose(A, np.tril(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform forwards solve\n    for i in range(n):\n        partial_sum = 0.0\n        for j in range(0, i):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\n\ndef backward_substitution(A, b):\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is upper\n    triangular. returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is upper triangular\n    assert np.allclose(A, np.triu(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform backwards solve\n    for i in range(n - 1, -1, -1):  # iterate over rows backwards\n        partial_sum = 0.0\n        for j in range(i + 1, n):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\nAnd we can then test it out!\n\nA = np.array([[2.0, 0.0, 0.0], [1.0, 2.0, 0.0], [2.0, 4.0, 6.0]])\nb = np.array([[2.0], [7.0], [26.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using forward substitution\")\nx = forward_substitution(A, b)\nprint()\n\nprint(\"The solution using forward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  0.0,  0.0 ]\n    [  1.0,  2.0,  0.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n    [ 26.0 ]\n\nSolving the system using forward substitution\n\nThe solution using forward substitution is:\nx = [  1.0 ]\n    [  3.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True\n\n\nWe can also do a backward substitution test:\n\nA = np.array([[2.0, 1.0, 4.0], [0.0, 1.5, 0.0], [0.0, 0.0, 2.0]])\nb = np.array([[12.0], [3.0], [4.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"The solution using backward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nThe solution using backward substitution is:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "href": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.5 Combining Gaussian elimination and backward substitution",
    "text": "5.5 Combining Gaussian elimination and backward substitution\nOur grant strategy can now come together so we have a method to solve systems of linear equations:\nGiven a system of linear equations \\(A\\vec{x} = \\vec{b}\\);\n\nFirst perform Gaussian elimination to give an equivalent system of equations in upper triangular form;\nThen use backward substitution to produce a solution \\(\\vec{x}\\)\n\nWe can use our code to test this:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\nprint(\"Does x really solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nupper triangular system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nsolution using backward substitution:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the original system? True",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#the-cost-of-gaussian-elimination",
    "href": "src/lec04.html#the-cost-of-gaussian-elimination",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.6 The cost of Gaussian Elimination",
    "text": "5.6 The cost of Gaussian Elimination\nGaussian elimination (GE) is unnecessarily expensive when it is applied to many systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).\n\nThe forward elimination process is the most computationally expensive part at \\(O(n^3)\\) but is exactly the same for any choice of \\(\\vec{b}\\).\nIn contrast, the solution of the resulting upper triangular system only requires \\(O(n^2)\\) operations.\n\n\n\n\n\n\nRuntime cost of applying Gaussian elimination (GE) and backwards solution (BS).\n\n\n\n\nWe can use this information to improve the way in which we solve multiple systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#sec-LU-factorisation",
    "href": "src/lec04.html#sec-LU-factorisation",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.7 LU factorisation",
    "text": "5.7 LU factorisation\nOur next algorithm, called LU factorisation, is a way to try to speed up Gaussian elimination by reusing information. This can be used when we solve systems of equations with the same matrix \\(A\\) but different right hand sides \\(\\vec{b}\\) - this is more common than you would think!\nRecall the elementary row operations (EROs) from above. Note that the EROs can be produced by left multiplication with a suitable matrix:\n\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c \\\\ g & h & i \\\\ d & e & f\n\\end{pmatrix}\n\\]\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c & d \\\\ e & f & g & h \\\\ i & j & k & l \\\\ m & n & o & p\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c & d \\\\ i & j & k & l \\\\ e & f & g & h \\\\ m & n & o & p\n\\end{pmatrix}\n\\]\nMultiply row by \\(\\alpha\\):\n\\[\n\\begin{pmatrix}\n\\alpha & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\alpha a & \\alpha b & \\alpha c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n\\]\n\\(\\alpha \\times \\text{row } p + \\text{row } q\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\alpha & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ \\alpha a + g & \\alpha b + h & \\alpha c + i\n\\end{pmatrix}\n\\]\n\nSince Gaussian elimination (GE) is just a sequence of EROs and each ERO just multiplication by a suitable matrix, say \\(E_k\\), forward elimination applied to the system \\(A \\vec{x} = \\vec{b}\\) can be expressed as \\[\n(E_m \\cdots E_1) A \\vec{x} = (E_m \\cdots E_1) \\vec{b},\n\\] here \\(m\\) is the number of EROs required to reduce the upper triangular form.\nLet \\(U = (E_m \\cdots E_1) A\\) and \\(L = (E_m \\cdots E_1)^{-1}\\). Now the original system \\(A \\vec{x} = \\vec{b}\\) is equivalent to\n\\[\\begin{equation}\n\\label{eq:LU}\nL U \\vec{x} = \\vec{b}\n\\end{equation}\\]\nwhere \\(U\\) is upper triangular (by construction) and \\(L\\) may be shown to be lower triangular (provided the EROs do not include any row swaps).\nOnce \\(L\\) and \\(U\\) are known it is easy to solve \\(\\eqref{eq:LU}\\)\n\nSolve \\(L \\vec{z} = \\vec{b}\\) in \\(O(n^2)\\) operations.\nSolve \\(U \\vec{x} = \\vec{z}\\) in \\(O(n^2)\\) operations.\n\n\\(L\\) and \\(U\\) may be found in \\(O(n^3)\\) operations by performing GE and saving the \\(E_i\\) matrices, however it is more convenient to find them directly (also \\(O(n^3)\\) operations).\n\n5.7.1 Computing \\(L\\) and \\(U\\)\nConsider a general \\(4 \\times 4\\) matrix \\(A\\) and its factorisation \\(LU\\):\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\nl_{21} & 1 & 0 & 0 \\\\\nl_{31} & l_{32} & 1 & 0 \\\\\nl_{41} & l_{42} & l_{43} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & u_{14} \\\\\n0 & u_{22} & u_{23} & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}\n\\]\nFor the first column,\n\\[\n\\begin{aligned}\na_{11} & = (1, 0, 0, 0) (u_{11}, 0, 0, 0)^T && = u_{11}\n& \\rightarrow u_{11} & = a_{11} \\\\\na_{21} & = (l_{21}, 1, 0, 0)(u_{11}, 0, 0, 0)^T && = l_{21} u_{11}\n& \\rightarrow l_{21} & = a_{21} / u_{11} \\\\\na_{31} & = (l_{31}, l_{32}, 1, 0)(u_{11}, 0, 0, 0)^T && = l_{31} u_{11}\n& \\rightarrow l_{31} & = a_{31} / u_{11} \\\\\na_{41} & = (l_{41}, l_{42}, l_{43}, 1)(u_{11}, 0, 0, 0)^T && = l_{41} u_{11}\n& \\rightarrow l_{41} & = a_{41} / u_{11}\n\\end{aligned}\n\\]\nThe second, third and fourth columns follow in a similar manner, giving all the entries in \\(L\\) and \\(U\\).\n\nRemark 5.2. \n\n\\(L\\) is assumed to have 1’s on the diagonal, to ensure that the factorisation is unique.\nThe process involves division by the diagonal entries \\(u_{11}, u_{22}\\), etc., so they must be non-zero.\nIn general the factors \\(l_{ij}\\) and \\(u_{ij}\\) are calculated for each column \\(j\\) in turn, i.e.,\nfor j in range(n):\n  for i in range(j+1):\n      # Compute factors u_{ij}\n      ...\n  for i in range(j+1, n):\n      # Compute factors l_{ij}\n      ...\n\n\n\nExample 5.4 Use \\(LU\\) factorisation to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nThis can be rewritten in the form \\(A = LU\\) where\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}.\n\\]\nColumn 1 of \\(A\\) gives\n\\[\n\\begin{aligned}\n2 & = u_{11} && \\rightarrow & u_{11} & = 2 \\\\\n1 & = l_{21} u_{11} && \\rightarrow & l_{21} & = 0.5 \\\\\n2 & = l_{31} u_{11} && \\rightarrow & l_{31} & = 1.\n\\end{aligned}\n\\]\nColumn 2 of \\(A\\) gives\n\\[\n\\begin{aligned}\n1 & = u_{12} && \\rightarrow & u_{12} & = 1 \\\\\n2 & = l_{21} u_{12} + u_{22} && \\rightarrow & u_{22} & = 1.5 \\\\\n4 & = l_{31} u_{12} + l_{32} u_{22} && \\rightarrow & l_{32} & = 2.\n\\end{aligned}\n\\]\nColumn 3 of \\(A\\) gives\n\\[\n\\begin{aligned}\n4 & = u_{13} && \\rightarrow & u_{13} & = 4 \\\\\n2 & = l_{21} u_{13} + u_{23} && \\rightarrow & u_{23} & = 0 \\\\\n6 & = l_{31} u_{13} + l_{32} u_{23} + u_{33} && \\rightarrow & u_{33} & = 2.\n\\end{aligned}\n\\]\nSolve the lower triangular system \\(L \\vec{z} = \\vec{b}\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\nSolve the upper triangular system \\(U \\vec{x} = \\vec{z}\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\]\n\n\nExercise 5.5 Rewrite the matrix \\(A\\) as the product of lower and upper triangular matrices where\n\\[\nA =\n\\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & 1 & 2.5\n\\end{pmatrix}.\n\\]\n\n\nRemark. The first example gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nNote that\n\nthe matrix \\(U\\) is the same as the fully eliminated upper triangular form produced by Gaussian elimination;\n\\(L\\) contains the multipliers that were used at each stage to eliminate the rows.\n\n\n\n\n5.7.2 Python code for LU factorisation\nWe can implement computation of the LU factorisation:\n\ndef lu_factorisation(A):\n    \"\"\"\n    compute the LU factorisation of A\n    returns the factors L and U\n    \"\"\"\n    n, m = A.shape\n    assert n == m, f\"Matrix A is not square {A.shape=}\"\n\n    # construct arrays of zeros\n    L, U = np.zeros_like(A), np.zeros_like(A)\n\n    # fill entries\n    for i in range(n):\n        L[i, i] = 1\n        # compute entries in U\n        for j in range(i, n):\n            U[i, j] = A[i, j] - sum(L[i, k] * U[k, j] for k in range(i))\n        # compute entries in L\n        for j in range(i + 1, n):\n            L[j, i] = (A[j, i] - sum(L[j, k] * U[k, i] for k in range(i))) / U[\n                i, i\n            ]\n\n    return L, U\n\nand test our implementation:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\n\nprint(\"matrix:\")\nprint_array(A)\nprint()\n\nprint(\"performing factorisation\")\nL, U = lu_factorisation(A)\nprint()\n\nprint(\"factorisation:\")\nprint_array(L)\nprint_array(U)\nprint()\n\nprint(\"Is L lower triangular?\", np.allclose(L, np.tril(L)))\nprint(\"Is U lower triangular?\", np.allclose(U, np.triu(U)))\nprint(\"Is LU a factorisation of A?\", np.allclose(L @ U, A))\n\nmatrix:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\n\nperforming factorisation\n\nfactorisation:\nL = [  1.0,  0.0,  0.0 ]\n    [  0.5,  1.0,  0.0 ]\n    [  1.0,  2.0,  1.0 ]\nU = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\n\nIs L lower triangular? True\nIs U lower triangular? True\nIs LU a factorisation of A? True\n\n\nand then add LU factorisation times to our plot:\n\n\n\n\n\nRuntime cost of applying Gaussian elimination (GE) and backwards solving (BS) equations as well as LU factorisation.\n\n\n\n\nWe see that LU factorisation is still \\(O(n^3)\\) and that the run times are similar to Gaussian elimination. But, importantly, we can reuse this factorisation more cheaply for different right-hand sides \\(\\vec{b}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "href": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.8 Effects of finite precision arithmetic",
    "text": "5.8 Effects of finite precision arithmetic\n\nExample 5.5 Consider the following linear system of equations\n\\[\n\\begin{pmatrix}\n0 & 2 & 1 \\\\\n2 & 1 & 0 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 4 \\\\ 5\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the first column by the diagonal by adding multiples of row 1 to rows 2 and 3 respectively.\nSolution. Swap the order of the equations!\n\nSwap rows 1 and 2:\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 \\\\ 7 \\\\ 5\n\\end{pmatrix}\n\\]\nNow apply Gaussian elimination\n\\[\\begin{align*}\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 1.5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n&= \\begin{pmatrix}\n4 \\\\ 7 \\\\ 3\n\\end{pmatrix} \\\\\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & -0.75\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n&= \\begin{pmatrix}\n4 \\\\ 7 \\\\ -2.25\n\\end{pmatrix}.\n\\end{align*}\\]\n\n\n\nExample 5.6 Consider another system of equations\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n4 & 2 & 1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 5 \\\\ 2\n\\end{pmatrix}\n\\]\n\nApply Gaussian elimination as usual:\n\\[\\begin{align*}\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ 2\n\\end{pmatrix}\n\\\\\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\end{align*}\\]\nProblem. We cannot eliminate the second column below the diagonal by adding a multiple of row 2 to row 3.\nAgain this problem may be overcome simply by swapping the order of the equations - this time swapping rows 2 and 3:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nWe can now continue the Gaussian elimination process as usual.\n\n\nIn general. Gaussian elimination requires row swaps to avoid breaking down when there is a zero in the “pivot” position. This might be a familiar aspect of Gaussian elimination, but there is an additional reason to apply pivoting when working with floating point numbers:\n\nExample 5.7 Consider using Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 + \\varepsilon \\\\ 3\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true, unique solution is \\((x_1, x_2)^T = (1, 2)^T\\).\nIf \\(\\varepsilon \\neq 0\\), Gaussian elimination gives\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n0 & 1 - \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 + \\varepsilon \\\\ 3 - \\frac{2 + \\varepsilon}{\\varepsilon}\n\\end{pmatrix}\n\\]\nProblems occur not only when \\(\\varepsilon = 0\\) but also when it is very small, i.e. when \\(\\frac{1}{\\varepsilon}\\) is very large, this will introduce very significant rounding errors into the computation.\n\nUse Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n\\varepsilon & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 2 + \\varepsilon\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true solution is still \\((x_1, x_2)^T = (1, 2)^T\\).\nGaussian elimination now gives\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1 - \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 2 - 2\\varepsilon\n\\end{pmatrix}\n\\]\nThe problems due to small values of \\(\\varepsilon\\) have disappeared.\n\nThis is a genuine problem we see in the code versions too!\n\nprint(\"without row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nprint(\"with row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nwithout row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.00009,  2.00000 ], Solution? False\neps=1.0e-14, x.T = [  1.0214,  2.0000 ], Solution? False\n\nwith row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-14, x.T = [  1.0,  2.0 ], Solution? True\n\n\n\n\n\nRemark 5.3. \n\nWriting the equations in a different order has removed the previous problem.\nThe diagonal entries are now always relatively larger.\nThe interchange of the order of equations is a simple example of row pivoting. This strategy avoids excessive rounding errors in the computations.\n\n\n\n5.8.1 Gaussian elimination with pivoting\nKey idea:\n\nBefore eliminating entries in column \\(j\\):\n\nfind the entry in column \\(j\\), below the diagonal, of maximum magnitude;\nif that entry is larger in magnitude than the diagonal entry then swap its row with row \\(j\\).\n\nThen eliminate column \\(j\\) as before.\n\nThis algorithm will always work when the matrix \\(A\\) is invertible/non-singular. Conversely, if all of the possible pivot values are zero this implies that the matrix is singular and a unique solution does not exist. At each elimination step the row multiplies used are guaranteed to be at most one in magnitude so any errors in the representation of the system cannot be amplified by the elimination process. As always, solving \\(A \\vec{x} = \\vec{b}\\) requires that the entries in \\(\\vec{b}\\) are also swapped in the appropriate way. Pivoting can be applied in an equivalent way to LU factorisation. The sequence of pivots is independent of the vector \\(\\vec{b}\\) and can be recorded and reused. The constraint imposed on the row multipliers means that for LU factorisation every entry in \\(L\\) satisfies \\(| l_{ij} | \\le 1\\).\n\nExample 5.8 Consider the linear system of equations given by\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n-3 & 2.1 - \\varepsilon & 6 \\\\\n5 & -1 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 9.9 + \\varepsilon \\\\ 11\n\\end{pmatrix}\n\\]\nwhere \\(0 \\le \\varepsilon \\ll 1\\), and solve it using\n\nGaussian elimination without pivoting\nGaussian elimination with pivoting.\n\nThe exact solution is \\(\\vec{x} = (0, -1, 2)^T\\) for any \\(\\varepsilon\\) in the given range.\n1. Solve the system using Gaussian elimination with no pivoting.\nEliminating the first column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nand then the second column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 0 & 5 + 15/\\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5 + 2.5(12 + \\varepsilon)/\\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to\n\\[\nx_3 = \\frac{3 + \\frac{12 + \\varepsilon}{\\varepsilon}}{2 + \\frac{6}{\\varepsilon}}\n\\qquad\nx_2 = \\frac{(12 + \\varepsilon) - 6x_3}{-\\varepsilon} \\qquad\nx_1 = \\frac{7+ 7x_2}{10}.\n\\]\nThere are many divisions by \\(\\varepsilon\\), so we will have problems if \\(\\varepsilon\\) is (very) small.\n2. Solve the system using Gaussian elimination with pivoting.\nThe first stage is identical (because \\(a_{11} = 10\\) is largest).\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nbut now \\(|a_{22}| = \\varepsilon\\) and \\(|a_{32}| = 2.5\\) so we swap rows 2 and 3 to give\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & -\\varepsilon & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + \\varepsilon\n\\end{pmatrix}\n\\]\nNow we may eliminate column 2:\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & 0 & 6 + 2 \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + 4 \\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to the exact answer:\n\\[\nx_3 = \\frac{12 + 4\\varepsilon}{6 + 2 \\varepsilon} = 2 \\qquad\nx_2 = \\frac{7.5 - 5x_3}{2.5} = -1 \\qquad\nx_1 = \\frac{7 + 7 x_2}{10} = 0.\n\\]\n\n\n\n5.8.2 Python code for Gaussian elimination with pivoting\n\ndef gaussian_elimination_with_pivoting(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation with pivoting to reduce the system of linear\n    equations Ax=b to upper triangular form.\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n\n        # find largest entry in column i\n        largest = abs(A[i, i])\n        j_max = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) &gt; largest:\n                largest, j_max = abs(A[j, i]), j\n\n        # swap rows j_max and i\n        row_swap(A, b, i, j_max)\n        if verbose:\n            print(f\"swapped system ({i} &lt;-&gt; {j_max})\")\n            print_array(A)\n            print_array(b)\n            print()\n\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nGaussian elimination without pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination without pivoting\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination without pivoting\neliminating column 0\n  row 1 |-&gt; row 1 - -0.3 * row 0\n  row 2 |-&gt; row 2 - 0.5 * row 0\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - -244760849313613.9 * row 1\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [ -0.030435 ]\n    [ -1.043478 ]\n    [  2.000000 ]\n\nDoes x solve the original system? False\n\n\nGaussian elimination with pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination with pivoting\")\ngaussian_elimination_with_pivoting(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination with pivoting\neliminating column 0\nswapped system (0 &lt;-&gt; 0)\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nrow 1 |-&gt; row 1 - -0.3 * row 0\nrow 2 |-&gt; row 2 - 0.5 * row 0\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\nswapped system (1 &lt;-&gt; 2)\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0, -0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nrow 2 |-&gt; row 2 - -4.085620730620576e-15 * row 1\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [  0.0 ]\n    [ -1.0 ]\n    [  2.0 ]\n\nDoes x solve the original system? True",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#further-reading",
    "href": "src/lec04.html#further-reading",
    "title": "5  Direct solvers for systems of linear equations",
    "section": "5.9 Further reading",
    "text": "5.9 Further reading\nSome basic reading:\n\nWikipedia: Gaussian elimination\nJoseph F. Grcar. How ordinary elimination became Gaussian elimination. Historia Mathematica. Volume 38, Issue 2, May 2011. (More history)\n\nSome reading on LU factorisation:\n\n\\(A = LU\\) and solving systems [pdf]\nWikipedia: LU decomposition\nWikipedia: Matrix decomposition (Other examples of decompositions).\nNick Higham: What is an LU factorization? (a very mathematical treatment with additional references)\n\nSome reading on using Gaussian elimination with pivoting:\n\nGaussian elimination with Partial Pivoting [pdf]\nGaussian elimination with partial pivoting example [pdf]\n\nA good general reference for this area:\n\nTrefethen, Lloyd N.; Bau, David (1997), Numerical linear algebra, Philadelphia: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9.\n\nSome implementations:\n\nNumpy numpy.linalg.solve\nScipy scipy.linalg.lu\nLAPACK Gaussian elimination (uses LU factorisation): dgesv()\nLAPACK LU Factorisation: dgetrf().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html",
    "href": "src/lec05.html",
    "title": "6  Iterative solutions of linear equations",
    "section": "",
    "text": "6.1 Iterative methods\nIn the previous section we looked at what are known as direct methods for solving systems of linear equations. They are guaranteed to produce a solution with a fixed amount of work (we can even prove this in exact arithmetic!), but this fixed amount of work may be very large.\nFor a general \\(n \\times n\\) system of linear equations \\(A \\vec{x} = \\vec{b}\\), the computation expense of all direct methods if \\(O(n^3)\\). The amount of storage required for these approaches is \\(O(n^2)\\) which is dominated by the cost of storing the matrix \\(A\\). As \\(n\\) becomes larger the storage and computation work required limit the practicality of direct approaches.\nAs an alternative, we will propose some iterative methods. Iterative methods produce a sequence \\((\\vec{x}^{(k)})\\) of approximations to the solution of the linear system of equations \\(A \\vec{x} = \\vec{b}\\). The iteration is defined recursively and is typically of the form: \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)}),\n\\] where \\(\\vec{x}^{(k)}\\) is now a vector of values and \\(\\vec{F}\\) is some vector function (which needs to be defined to define the method). We will need to choose a starting value \\(\\vec{x}^{(k)}\\) but there is often a reasonable approximation which can be used. Once all this is defined, we still need to decide when we need to stop!\nThe key point here is we want a method which is both cheap to compute but converges quickly to the solution. One way to do this is to construct iteration given by\n\\[\\begin{equation}\n\\label{eq:general-iteration}\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + P (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{equation}\\]\nfor some matrix \\(P\\) such that\nWe call \\(\\vec{b} - A \\vec{x}^{(k)} = \\vec{r}\\) the residual. Note in the above examples we would have \\(P = O\\) (the zero matrix) or \\(P = A^{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#iterative-methods",
    "href": "src/lec05.html#iterative-methods",
    "title": "6  Iterative solutions of linear equations",
    "section": "",
    "text": "Example 6.1 (Some very bad examples)  \n\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)}.\n\\]\nEach iteration is very cheap to compute but very inaccurate - it never converges!\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + A^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\]\nEach iteration is very expensive to compute - you have to invert \\(A\\)! - but it converges in just one step since\n\\[\n\\begin{aligned}\nA \\vec{x}^{(k+1)} & = A \\vec{x}^{(k)} + A A^{-1} (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = A \\vec{x}^{(k)} + \\vec{b} - A \\vec{x}^{(k)} \\\\\n& = \\vec{b}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\\(P\\) is easy to compute, or the matrix vector product \\(P \\vec{r}\\) is easy to compute,\n\\(P\\) approximates \\(A^{-1}\\) well enough that the algorithm converges in few iterations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#jacobi-iteration",
    "href": "src/lec05.html#jacobi-iteration",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.2 Jacobi iteration",
    "text": "6.2 Jacobi iteration\nOne simple choice for \\(P\\) is given by the Jacobi method where we take \\(P =\nD^{-1}\\) where \\(D\\) is the diagonal of \\(A\\): \\[\nD_{ii} = A_{ii} \\quad \\text{and} \\quad D_{ij} = 0 \\text{ for } i \\neq j.\n\\]\nThe Jacobi iteration is given by\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)})\n\\]\n\\(D\\) is a diagonal matrix, so \\(D^{-1}\\) is trivial to form (as long as the diagonal entries are all nonzero): \\[\n(D^{-1})_{ii} = \\frac{1}{D_{ii}}\n\\quad \\text{and} \\quad\n(D^{-1})_{ij} = 0 \\text{ for } i \\neq j.\n\\]\n\nRemark. \n\nThe cost of one iteration is \\(O(n^2)\\) for a full matrix, and this is dominated by the matrix-vector product \\(A \\vec{x}^{(k)}\\).\nThis cost can be reduced to \\(O(n)\\) if the matrix \\(A\\) is sparse - this is when iterative methods are especially attractive (Example 4.6).\nThe amount of work also depends on the number of iterations required to get a “satisfactory” solution.\n\nThe number of iterations depends on the matrix;\n\nFewer iterations are needed for a less accurate solution;\n\nA good initial estimate \\(\\vec{x}^{(0)}\\) reduces the required number of iterations.\n\nUnfortunately, the iteration might not converge!\n\n\nThe Jacobi iteration updates all elements of \\(\\vec{x}^{(k)}\\) simultaneously to get \\(\\vec{x}^{(k+1)}\\). Writing the method out component by component gives\n\\[\n\\begin{aligned}\nx_1^{(k+1)} &= x_1^{(k)} + \\frac{1}{A_{11}} \\left( b_1 - \\sum_{j=1}^n A_{1j}\nx_j^{(k)} \\right) \\\\\nx_2^{(k+1)} &= x_2^{(k)} + \\frac{1}{A_{22}} \\left( b_2 - \\sum_{j=1}^n A_{2j}\nx_j^{(k)} \\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)} &= x_n^{(k)} + \\frac{1}{A_{nn}} \\left( b_n - \\sum_{j=1}^n A_{nj}\nx_j^{(k)} \\right).\n\\end{aligned}\n\\]\nNote that once the first step has been taken, \\(x_1^{(k+1)}\\) is already known, but the Jacobi iteration does not make use of this information!\n\nExample 6.2 Take two iterations of Jacobi iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\): \\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\), the first iteration is \\[\n\\begin{aligned}\nx_1^{(1)} &= x_1^{(0)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(0)}\n- A_{12} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{2} (3.5 - 2 \\times 1 - 1 \\times 1) = 1.25 \\\\\nx_2^{(1)} &= x_2^{(0)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(0)}\n- A_{22} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{4} (0.5 - (-1) \\times 1 - 4 \\times 1) = 0.375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(1)} = (1.25, 0.375)^T\\). Then the second iteration is \\[\n\\begin{aligned}\nx_1^{(2)} &= x_1^{(1)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(1)} -\nA_{12} x_2^{(1)} \\right) \\\\\n&= 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.375) = 1.5625 \\\\\nx_2^{(2)} &= x_2^{(1)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(1)} -\nA_{22} x_2^{(1)} \\right) \\\\\n&= 0.375 + \\frac{1}{4} (0.5 - (-1) \\times 1.25 - 4 \\times 0.375) = 0.4375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(2)} = (1.5625, 0.4375)\\).\nNote the only difference between the formulae for Iteration 1 and 2 is the iteration number, the superscript in brackets. The exact solution is given by \\(\\vec{x} = (1.5, 0.5)^T\\).\n\nWe note that we can also slightly simplify the way the Jacobi iteration is written. We can expand \\(A\\) into \\(A = L + D + U\\), where \\(L\\) and \\(U\\) are the parts of the matrix from below and above the diagonal respectively: \\[\nL_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &lt; j \\\\\n0 &\\quad \\text{if } i \\ge j,\n\\end{cases}\n\\qquad\nU_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &gt; j \\\\\n0 &\\quad \\text{if } i \\le j.\n\\end{cases}\n\\] The we can calculate that: \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} D \\vec{x}^{(k)} + D^{-1}(\\vec{b}\n- (L + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}).\n\\end{aligned}\n\\] In this formulation, we do not explicitly form the residual as part of the computations. In practical situations, this may be a simpler formulation we can use if we have knowledge of the coefficients of \\(A\\), but this is not always true!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#gauss-seidel-iteration",
    "href": "src/lec05.html#gauss-seidel-iteration",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.3 Gauss-Seidel iteration",
    "text": "6.3 Gauss-Seidel iteration\nAs an alternative to Jacobi iteration, the iteration might use \\(x_i^{(k+1)}\\) as soon as it is calculated (rather than using the previous iteration), giving\n\\[\n\\begin{aligned}\nx_1^{(k+1)}\n& = x_1^{(k)} + \\frac{1}{A_{11}} \\left(\nb_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)}\n\\right) \\\\\nx_2^{(k+1)}\n& = x_2^{(k)} + \\frac{1}{A_{22}} \\left(\nb_2 - A_{21} x_1^{(k+1)} - \\sum_{j=2}^n A_{2j} x_j^{(k)}\n\\right) \\\\\nx_3^{(k+1)}\n& = x_3^{(k)} + \\frac{1}{A_{33}} \\left(\nb_3 - \\sum_{j=1}^2 A_{3j} x_j^{(k+1)} - \\sum_{j=3}^n A_{3j} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_i^{(k+1)}\n& = x_i^{(k)} + \\frac{1}{A_{ii}} \\left(\nb_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i}^n A_{ij} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)}\n& = x_n^{(k)} + \\frac{1}{A_{nn}} \\left(\nb_n - \\sum_{j=1}^{n-1} A_{nj} x_j^{(k+1)} - A_{nn} x_n^{(k)}\n\\right).\n\\end{aligned}\n\\]\nConsider the system \\(A \\vec{x}= b\\) with the matrix \\(A\\) split as \\(A = L + D + U\\) where \\(D\\) is the diagonal of \\(A\\), \\(L\\) contains the elements below the diagonal and \\(U\\) contains the elements above the diagonal. The componentwise iteration above can be written in matrix form as \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} (\\vec{b} - L \\vec{x}^{(k+1)} - (D + U)\n\\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} (\\vec{b} - (D + U)\n\\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k)} + D^{-1}\n(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} L\n\\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\nD^{-1} (D + L) \\vec{x}^{(k+1)}  & = D^{-1} (D + L) \\vec{x}^{(k)} + D^{-1}\n(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n(D + L) \\vec{x}^{(k+1)} &= D D^{-1} (D + L) \\vec{x}^{(k)} + D D^{-1} (\\vec{b}\n- A \\vec{x}^{(k)}) \\\\\n& = (D + L) \\vec{x}^{(k)} + (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} &= (D + L)^{-1} (D + L) \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} -\nA \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{aligned}\n\\]\n…and hence the Gauss-Seidel iteration\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\] That is, we use \\(P = (D+L)^{-1}\\) in \\(\\eqref{eq:general-iteration}\\).\nIn general, we don’t form the inverse of \\(D + L\\) explicitly here since it is more complicated to do so than for simply computing the inverse of \\(D\\).\n\nExample 6.3 Take two iterations of Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\) we have\nIteration 1:\n\\[\n\\begin{aligned}\nx^{(1)}_1 & = x^{(0)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(0)}_1 -\nA_{12} x^{(0)}_2) \\\\\n          & = 2 + \\frac{1}{2} (3.5 - 1 \\times 2 - 1 \\times 1) = 2.25 \\\\\nx^{(1)}_2 & = x^{(0)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(1)}_1 -\nA_{22} x^{(0)}_2) \\\\\n          & = 1 + \\frac{1}{4} (0.5 - (-1) \\times 2.25 - 4 \\times 1) = 0.6875.\n\\end{aligned}\n\\]\nIteration 2:\n\\[\n\\begin{aligned}\nx^{(2)}_1 & = x^{(1)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(1)}_1 a\n- A_{12} x^{(1)}_2) \\\\\n          & = 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.4375)\n          = 1.53125 \\\\\nx^{(2)}_2 & = x^{(1)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(2)}_1\n- A_{22} x^{(1)}_2) \\\\\n          & = 0.4375 + \\frac{1}{4} (0.5 - (-1) \\times 1.53125 - 4 \\times 0.4375)\n          = 0.5078125.\n\\end{aligned}\n\\]\nAgain, note the changes in the iteration number on the right hand side of these equations, especially the differences against the Jacobi method.\n\nWhat happens if the initial estimate is altered to \\(\\vec{x}^{(0)} = (2, 1)^T\\) (homework).\n\n\n\nExercise 6.1 Take one iteration of (i) Jacobi iteration; (ii) Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 2, 3)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 10 \\\\ 6\n\\end{pmatrix}.\n\\]\nNote that the exact solution to this system is \\(x_1 = 2, x_2 = 2, x_3 = 2\\).\n\n\nRemark. \n\nHere both methods converge, but fairly slowly. They might not converge at all!\nWe will discuss convergence and stopping criteria in the next lecture.\nThe Gauss-Seidel iteration generally out-performs the Jacobi iteration.\nPerformance can depend on the order in which the equations are written.\nBoth iterative algorithms can be made faster and more efficient for sparse systems of equations (far more than direct methods).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#python-version-of-iterative-methods",
    "href": "src/lec05.html#python-version-of-iterative-methods",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.4 Python version of iterative methods",
    "text": "6.4 Python version of iterative methods\n\ndef jacobi_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    TODO\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(i):\n                Axi += A[i, j] * xnew[j]\n            for j in range(i, n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\nA = np.array([[2.0, 1.0], [-1.0, 4.0]])\nb = np.array([[3.5], [0.5]])\nx0 = np.array([[1.0], [1.0]])\n\nprint(\"jacobi iteration\")\nx = jacobi_iteration(A, b, x0, 5, verbose=True)\nprint()\n\nprint(\"gauss seidel iteration\")\nx = gauss_seidel_iteration(A, b, x0, 5, verbose=True)\nprint()\n\njacobi iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.250,  0.375 ]\nafter iter=1: x.T = [  1.5625,  0.4375 ]\nafter iter=2: x.T = [  1.53125,  0.51562 ]\nafter iter=3: x.T = [  1.49219,  0.50781 ]\nafter iter=4: x.T = [  1.49609,  0.49805 ]\n\ngauss seidel iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.2500,  0.4375 ]\nafter iter=1: x.T = [  1.53125,  0.50781 ]\nafter iter=2: x.T = [  1.49609,  0.49902 ]\nafter iter=3: x.T = [  1.50049,  0.50012 ]\nafter iter=4: x.T = [  1.49994,  0.49998 ]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#sparse-matrices",
    "href": "src/lec05.html#sparse-matrices",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.5 Sparse Matrices",
    "text": "6.5 Sparse Matrices\nWe met sparse matrices as an example of a special matrix format when we first thought about systems of linear equations. Sparse matrices are very common in applications and have a structure which is very useful when used with iterative methods. There are two main ways in which sparse matrices can be exploited in order to obtain benefits within iterative methods.\n\nThe storage can be reduced from \\(O(n^2)\\).\nThe cost per iteration can be reduced from \\(O(n^2)\\).\n\nRecall that a sparse matrix is defined to be such that it has at most \\(\\alpha n\\) non-zero entries (where \\(\\alpha\\) is independent of \\(n\\)). Typically this happens when we know there are at most \\(\\alpha\\) non-zero entries in any row.\nThe simplest way in which a sparse matrix is stored is using three arrays:\n\nan array of floating point numbers (A_real say) that stores the non-zero entries;\nan array of integers (I_row say) that stores the row number of the corresponding entry in the real array;\nan array of integers (I_col say) that stores the column numbers of the corresponding entry in the real array.\n\nThis requires just \\(3 \\alpha n\\) units of storage - i.e. \\(O(n)\\).\nGiven the above storage pattern, the following algorithm will execute a sparse matrix-vector multiplication (\\(\\vec{z} = A \\vec{y}\\)) in \\(O(n)\\) operations:\nz = np.zeros((n, 1))\nfor k in range(nonzero):\n    z[I_row[k]] = z[I_row[k]] + A_real[k] * y[I_col[k]]\n\nHere nonzero is the number of non-zero entries in the matrix.\nNote that the cost of this operation is \\(O(n)\\) as required.\n\n\n6.5.1 Python experiments\n\ndef system_size_sparse(A_real, I_row, I_col, b):\n    n = len(b)\n    nonzero = len(A_real)\n    assert nonzero == len(I_row)\n    assert nonzero == len(I_col)\n\n    return n, nonzero\n\nFirst let’s adapt our implementations to use this sparse matrix format:\n\ndef jacobi_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter, verbose=False\n):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    # determine diagonal\n    # D[i] should be A_{ii}\n    D = np.zeros_like(x)\n    for k in range(nonzero):\n        if I_row[k] == I_col[k]:\n            D[I_row[k]] = A_real[k]\n\n    for iter in range(max_iter):\n        # precompute Ax\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / D[i] * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter, verbose=False\n):\n    \"\"\"\n    TODO\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        # precompute Ax using xnew if i &lt; j\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            if I_row[k] &lt; I_col[k]:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * xnew[I_col[k]]\n            else:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\nThen we can test the two different implementations of the methods:\n\n# random matrix\nn = 4\nnonzero = 10\nA_real, I_row, I_col, b = random_sparse_system(n, nonzero)\nprint(\"sparse matrix:\")\nprint(\"A_real =\", A_real)\nprint(\"I_row = \", I_row)\nprint(\"I_col = \", I_col)\nprint()\n\n# convert to dense for comparison\nA_dense = to_dense(A_real, I_row, I_col)\nprint(\"dense matrix:\")\nprint_array(A_dense)\nprint()\n\n# starting guess\nx0 = np.zeros((n, 1))\n\nprint(\"jacobi with sparse matrix\")\nx_sparse = jacobi_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter=5, verbose=True\n)\nprint()\n\nprint(\"jacobi with dense matrix\")\nx_dense = jacobi_iteration(A_dense, b, x0, max_iter=5, verbose=True)\nprint()\n\nsparse matrix:\nA_real = [-22.75  51.25  -5.     4.    12.25 -22.75   8.5   -5.  ]\nI_row =  [0 0 1 1 2 2 3 3]\nI_col =  [2 0 3 1 2 0 3 1]\n\ndense matrix:\nA_dense = [ 51.25,  0.00, -22.75,  0.00 ]\n          [  0.00,  4.00,  0.00, -5.00 ]\n          [ -22.75,  0.00, 12.25,  0.00 ]\n          [  0.00, -5.00,  0.00,  8.50 ]\n\njacobi with sparse matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  0.556098, -0.250000, -0.857143,  0.411765 ]\nafter iter=1: x.T = [  0.175610,  0.264706,  0.175610,  0.264706 ]\nafter iter=2: x.T = [  0.634051,  0.080882, -0.531010,  0.567474 ]\nafter iter=3: x.T = [  0.32038,  0.45934,  0.32038,  0.45934 ]\nafter iter=4: x.T = [  0.69832,  0.32418, -0.26215,  0.68197 ]\n\njacobi with dense matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  0.556098, -0.250000, -0.857143,  0.411765 ]\nafter iter=1: x.T = [  0.175610,  0.264706,  0.175610,  0.264706 ]\nafter iter=2: x.T = [  0.634051,  0.080882, -0.531010,  0.567474 ]\nafter iter=3: x.T = [  0.32038,  0.45934,  0.32038,  0.45934 ]\nafter iter=4: x.T = [  0.69832,  0.32418, -0.26215,  0.68197 ]\n\n\n\nWe see that we get the same results!\nNow let’s see how long it takes to get a solution. The following plot shows the run times of using the two different implementations of the Jacobi method. We see that, as expected, the run time of the dense formulation is \\(O(n^2)\\) and the run time of the sparse formulation is \\(O(n)\\).\n\n\n\n\n\n\n\n\n\nWe say “as expected” because we have already counted the number of operations per iteration and these implementations compute for a fixed number of iterations. In the next section, we look at alternative stopping criteria.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#convergence-of-an-iterative-method",
    "href": "src/lec05.html#convergence-of-an-iterative-method",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.6 Convergence of an iterative method",
    "text": "6.6 Convergence of an iterative method\nWe have discussed the construction of iterations which aim to find the solution of the equations \\(A \\vec{x} = \\vec{b}\\) through a sequence of better and better approximations \\(\\vec{x}^{(k)}\\).\nIn general the iteration takes the form \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)})\n\\] here \\(\\vec{x}^{(k)}\\) is a vector of values and \\(\\vec{F}\\) is some vector-valued function which we have defined.\nHow can we decide if this iteration has converged? We need \\(\\vec{x} -\n\\vec{x}^{(k)}\\) to be small, but we don’t have access to the exact solution \\(\\vec{x}\\) so we have to do something else!\nHow do we decide that a vector/array is small? The most common measure is to use the “Euclidean norm” of an array (which you met last year!). This is defined to be the square root of the sum of squares of the entries of the array: \\[\n\\| \\vec{r} \\| = \\sqrt{ \\sum_{i=1}^n r_i^2 }.\n\\] where \\(\\vec{r}\\) is a vector with \\(n\\) entries.\n\nExample 6.4 Consider the following sequence \\(\\vec{x}^{(k)}\\):\n\\[\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix},\n\\begin{pmatrix}\n1.5 \\\\ 0.5\n\\end{pmatrix},\n\\begin{pmatrix}\n1.75 \\\\ 0.25\n\\end{pmatrix},\n\\begin{pmatrix}\n1.875 \\\\ 0.125\n\\end{pmatrix},\n\\begin{pmatrix}\n1.9375 \\\\ -0.0625\n\\end{pmatrix},\n\\begin{pmatrix}\n1.96875 \\\\ -0.03125\n\\end{pmatrix},\n\\ldots\n\\]\n\nWhat is \\(\\|\\vec{x}^{(1)} - \\vec{x}^{(0)}\\|\\)?\nWhat is \\(\\|\\vec{x}^{(5)} - \\vec{x}^{(4)}\\|\\)?\n\nLet \\(\\vec{x} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\\).\n\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(3)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(4)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(5)}\\|\\)?\n\n\nRather than decide in advance how many iterations (of the Jacobi or Gauss-Seidel methods) to use stopping criteria:\n\nThis could be a maximum number of iterations.\nThis could be the change in values is small enough:\n\\[\n  \\|x^{(k+1)} - \\vec{x}^{(k)}\\| &lt; tol,\n  \\]\nThis could be the norm of the residual is small enough:\n\n\\[\n\\| \\vec{r} \\| = \\| \\vec{b} - A \\vec{x}^{(k)} \\| &lt; tol\n\\]\nIn both cases, we call \\(tol\\) the convergence tolerance and the choice of \\(tol\\) will control the accuracy of the solution.\n\nExercise 6.2 (Discussion) What is a good convergence tolerance?\n\nIn general there are two possible reasons that an iteration may fail to converge.\n\nIt may diverge - this means that \\(\\|\\vec{x}^{(k)}\\| \\to \\infty\\) as \\(k\\) (the number of iterations) increases, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n4 \\\\ 2\n\\end{pmatrix},\n\\begin{pmatrix}\n16 \\\\ 4\n\\end{pmatrix},\n\\begin{pmatrix}\n64 \\\\ 8\n\\end{pmatrix},\n\\begin{pmatrix}\n256 \\\\ 16\n\\end{pmatrix},\n\\begin{pmatrix}\n1024 \\\\ 32\n\\end{pmatrix},\n\\ldots\n\\]\nIt may neither converge nor diverge, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 0\n\\end{pmatrix},\n\\ldots\n\\]\n\nIn addition to testing for convergence it is also necessary to include tests for failure to converge.\n\nDivergence may be detected by monitoring \\(\\|\\vec{x}^{(k)}\\|\\).\nImpose a maximum number of iterations to ensure that the loop is not repeated forever!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#summary",
    "href": "src/lec05.html#summary",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.7 Summary",
    "text": "6.7 Summary\nMany complex computational problems simply cannot be solved with today’s computers using direct methods. Iterative methods are used instead since they can massively reduce the computational cost and storage required to get a “good enough” solution.\nThese basic iterative methods are simple to describe and program but generally slow to converge to an accurate answer - typically \\(O(n)\\) iterations are required! Their usefulness for general matrix systems is very limited therefore - but we have shown their value in the solution of sparse systems however.\nMore advanced iterative methods do exist but are beyond the scope of this module - see Final year projects, MSc projects, PhD, and beyond!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#further-reading",
    "href": "src/lec05.html#further-reading",
    "title": "6  Iterative solutions of linear equations",
    "section": "6.8 Further reading",
    "text": "6.8 Further reading\nMore details on these basic (and related) methods:\n\nWikipedia: Jacobi method\nWikipedia: Gauss-Seidel method\nWikipedia: Iterative methods\n\nsee also Richardson method, Damped Jacobi method, Successive over-relaxation method (SOR), Symmetric successive over-relaxation method (SSOR) and Krylov subspace methods\n\n\nMore details on sparse matrices:\n\nWikipedia Sparse matrix - including a long detailed list of software libraries support sparse matrices.\nStackoverflow: Using a sparse matrix vs numpy array\nJason Brownlee: A gentle introduction to sparse matrices for machine learning, Machine learning mastery\n\nSome related textbooks:\n\nJack Dongarra Templates for the solution of linear systems: Stopping criteria\nJack Dongarra Templates for the solution of linear systems: Stationary iterative methods\nGolub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins, ISBN 978-0-8018-5414-9.\nSaad, Yousef (2003). Iterative Methods for Sparse Linear Systems (2nd ed.). SIAM. p. 414. ISBN 0898715342.\n\nSome software implementations:\n\nscipy.sparse custom routines specialised to sparse matrices\nSuiteSparse, a suite of sparse matrix algorithms, geared toward the direct solution of sparse linear systems\nscipy.sparse iterative solvers: Solving linear problems\nPETSc: Linear system solvers - a high performance linear algebra toolkit",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec06.html",
    "href": "src/lec06.html",
    "title": "7  Complex numbers",
    "section": "",
    "text": "7.1 Basic definitions\nThis section of the notes supports the final learning outcome around eigenvalues and eigenvectors.\nA complex number is an element of a number system which extends our familiar real number system. Our motivation will be for finding eigenvalues and eigenvectors which is the next topic in this module. In order to find eigenvalues, we will need to find solutions of polynomial equations and it will turn out to be useful to always be able to get a solution to any polynomial equation.\nThe key idea of complex numbers is to create a new symbol, that we will call \\(i\\), or the imaginary unit which satisfies: \\[\\begin{equation*}\ni^2 = -1 \\qquad \\sqrt{-1} = i.\n\\end{equation*}\\] By taking multiples of this imaginary unit, we can create many more new numbers, like \\(3i, \\sqrt{5} i\\) or \\(-12 i\\). These are examples of imaginary numbers.\nWe form complex numbers by adding real and imaginary numbers whilst keeping each part separate. For example, \\(2 + 3i\\), \\(\\frac{1}{2} + \\sqrt{5} i\\) or \\(12 -\n12i\\).\nWe notice that all real numbers \\(x\\) must also be complex numbers since \\(x = x +\n0 i\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#basic-definitions",
    "href": "src/lec06.html#basic-definitions",
    "title": "7  Complex numbers",
    "section": "",
    "text": "Example 7.1 Consider the polynomial equation \\[\\begin{equation}\nx^2 + 1 = 0.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nWe can see that this equation has no solution over the real numbers.\n\n\n\n\nDefinition 7.1 Any number that can be written as \\(z = a + bi\\) with \\(a, b\\) real numbers and \\(i\\) the imaginary unit are called complex numbers. In this format, we call \\(a\\) the real part of \\(z\\) and \\(b\\) the imaginary part of \\(z\\).\n\n\n\nRemark 7.1. Among the first recorded use of complex numbers in European mathematics is by an Italian mathematician Gerolamo Cardano in around 1545. He later described complex numbers as being “as subtle as they are useless” and “mental torture”.\nThe term imaginary was coined by Rene Descartes in 1637:\n\n… sometimes only imaginary, that is one can imagine as many as I said in each equation, but sometimes there exist no quantity that matches that which we imagine.\n\n\n\nExercise 7.1 What are the real and imaginary parts of these numbers?\n\n3 + 6i\n-3.5 + 2i\n5\n7i",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#calculations-with-complex-numbers",
    "href": "src/lec06.html#calculations-with-complex-numbers",
    "title": "7  Complex numbers",
    "section": "7.2 Calculations with complex numbers",
    "text": "7.2 Calculations with complex numbers\nWe can perform the basic operations, addition, subtraction, multiplication and division, on complex numbers.\nAddition and subtraction is relatively straight forward: we simple treat the real and imaginary parts separately.\n\nExample 7.2 We can compute that \\[\\begin{align*}\n& (2 + 3i) + (12 - 12i) = (2 + 12) + (3 - 12)i = 14 - 9i \\\\\n& (2 + 3i) - (12 - 12i) = (2 - 12) + (3 - (-12))i = -10 + 15i.\n\\end{align*}\\]\n\n\nExercise 7.2 Compute \\((3+6i) + (-3.5 + 2i)\\) and \\((3 + 6i) - (-3.5 + 2i)\\).\n\nFor multiplying and dividing, we first say that applying these operations between complex and real numbers again follows the usual rules. Let \\(x\\) be a real number and \\(z = (a + bi)\\) be a complex number, then \\[\\begin{align*}\n& x \\times (a + bi) = (a + bi) \\times x = (x \\times a) + (x \\times b) \\\\\n& \\frac{a + bi}{x} = \\frac{a}{x} + \\frac{b}{x} i.\n\\end{align*}\\]\nFor multiplication between complex numbers, things are a bit harder. We expand out brackets and apply the rule that \\(i^2 = -1\\):\n\nExample 7.3 \\[\\begin{align*}\n& (2 + 3i) \\times (12 - 12i)  \\\\\n& = 2 \\times 12 + 3i \\times 12 + 2 \\times - 12 i + 3i \\times - 12 i\n&& \\text{(expand brackets)} \\\\\n& = 2 \\times 12 + (12 \\times 3) i + (2 \\times - 12) i + (3 \\times - 12) \\times\ni^2 && \\text{(rearrange)} \\\\\n& = 24 + 36 i - 24 i - 36 \\times i^2 && \\text{(compute products)} \\\\\n& = 24 + 36 i - 24 i + 36 && \\text{(use $i^2 = -1$)} \\\\\n& = 60 + 12 i && \\text{(collect terms)}.\n\\end{align*}\\]\n\nWe see that we have a general formula: \\[\\begin{equation*}\n(a + bi) \\times (c + di) = (ac - bd) + (ad + bc) i.\n\\end{equation*}\\]\n\nExercise 7.3 Compute \\((3 + 6i) \\times (-3.5 + 2i)\\).\n\nDivision is harder - you may want to skip this on first reading since it is not so important for what follows in these notes. When we divide complex numbers, we try to rewrite the fraction to have a real denominator by “rationalising the denominator”.\n\nExample 7.4 Suppose we want to find \\((2 + 3i) / (12 - 12i)\\). Our idea is to find a numbers so that we can write \\[\\begin{equation*}\n\\frac{2 + 3i}{12 - 12i} = \\frac{2 + 3i}{12 - 12i} \\times \\frac{z}{z}\n= frac{(2 + 3i)z}{(12 - 12i)z} = \\frac{\\text{something}}{\\text{something real}}.\n\\end{equation*}\\] The answer is to use \\(z = 12 + 12 i\\) - that is the denominator with the sign of the imaginary part flipped (we will give this a name later on).\nWe can compute that \\[\\begin{align*}\n& (12 - 12i) \\times (12 + 12i) \\\\\n& = (12 \\times 12) + (12 \\times 12i) + (-12i \\times 12) + (-12i \\times 12i)\n&& \\text{(expand bracket)}\\\\\n& = 12 \\times 12 + (12 \\times 12) i + (-12 \\times 12) i + (-12 \\times 12) i^2\n&& \\text{(rearrange)} \\\\\n& = 144 + 144i - 144i -144 i^2 && \\text{(compute products)} \\\\\n& = 144 + 144i - 144i + 144 && \\text{(use $i^2 = -1$)} \\\\\n& = 288 + 0 i && \\text{(collect terms)}.\n\\end{align*}\\] So we have that \\((12 - 12i) \\times (12 + 12i) = 288\\) is a real number.\nWe continue by computing that \\[\\begin{align*}\n& (2 + 3i) \\times (12 + 12i) \\\\\n& = (2 \\times 12) + (2 \\times 12i) + (3i \\times 12) + (3i \\times 12i) \\\\\n& = 2 \\times 12 + (2 \\times 12) i  + (3 \\times 12) i + (3 \\times 12) i^2 \\\\\n& = 24 + 24 i + 36 i + 36 i^2 \\\\\n& = 24 + 24 i + 36 i - 36 \\\\\n& = -12 + 60i.\n\\end{align*}\\]\nThus we infer that \\[\\begin{align*}\n\\frac{2 + 3i}{12 - 12i} &= \\frac{2 + 3i}{12 - 12i} \\times\n\\frac{12 + 12i}{12 + 12i} \\\\\n& = \\frac{(2 + 3i)(12 + 12i)}{(12 - 12i)(12 + 12i)} \\\\\n& = \\frac{-12 + 60i}{288} \\\\\n& = \\frac{-12}{288} + \\frac{60}{288} i \\\\\n& = -\\frac{1}{24} + \\frac{5}{24} i.\n\\end{align*}\\]\nTo check we have not done anything silly, we should also check that \\((12 + 12i)\n/ (12 + 12i) = 1\\). This is left as an exercise.\n\n\nExercise 7.4 Find \\((3 + 6i) / (-3.5 + 2i)\\) and \\((12 + 12i) / (12 + 12i)\\).\n\n\nRemark 7.2. One thing to be careful of when considering products is that the identity \\(i^2\n= -1\\) appears to break one rule of arithmetic of square roots: \\[\\begin{equation*}\ni^2 = (\\sqrt{-1})^2 = \\sqrt{-1} \\sqrt{-1} \\neq \\sqrt{(-1) \\times (-1)} = \\sqrt{1}\n= 1.\n\\end{equation*}\\] In fact, we have that \\(\\sqrt{x} \\sqrt{y} = \\sqrt{xy}\\) only if \\(x, y &gt; 0\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#a-geometric-picture",
    "href": "src/lec06.html#a-geometric-picture",
    "title": "7  Complex numbers",
    "section": "7.3 A geometric picture",
    "text": "7.3 A geometric picture\nThe idea of adding complex numbers by considering real and imaginary parts separately is reminiscent of adding two dimensional vectors. For this reason, it is often helpful to think of complex numbers as points in the complex plane.\nThe complex plane is the two dimensional space formed by considering the real and imaginary parts of a complex number as two different coordinate axes.\n\nExample 7.5  \n\n\n\n\n\n\n\n\n\n\nWe can see that adding complex numbers looks just like adding two dimensional vectors! We can also use this geometric picture to help with some further operations.\nThe complex conjugate of a complex number \\(z = a + bi\\) is given by \\(\\bar{z} =\na - bi\\). (The complex conjugate is that number we used before when working out how to divide complex numbers).\n\nExample 7.6 The complex conjugate of \\(2 + 3i\\) is \\(2 - 3i\\). The complex conjugate of \\(12 - 12i\\) is \\(12 + 12i\\).\n\nExercise 7.5 Find the complex conjugates of \\(3 + 6i\\) and \\(-3.5 + 2i\\).\n\n\nWe have already seen that the complex conjugate of a complex number is helpful when performing division of complex numbers. The reason is that computing the product of a number and its conjugate always gives a real, positive number: \\[\\begin{align*}\n& (a + bi) \\times (a - bi) \\\\\n& = (a \\times a) + (a \\times - b i) + (b i \\times a) + (bi \\times - bi) \\\\\n& = (a \\times a) + (a \\times -b) i + (b \\times a) i + (b \\times - b) i^2 \\\\\n& = (a \\times a) + (a \\times -b + b \\times a) i - (b \\times - b) \\\\\n& = a^2 + b^2 + 0 i.\n\\end{align*}\\]\nIn fact, we use this same calculation to define the absolute value (sometimes called the modulus) of a complex number \\(z = a + bi\\) \\[\\begin{equation*}\n|z| = |a + bi| = \\sqrt{a^2 + b^2} = \\sqrt{z \\bar{z}}.\n\\end{equation*}\\]\n\nExample 7.7  \n\n\n\n\n\n\n\n\n\n\nExercise 7.6 Find the value of \\[\\begin{equation}\n|3 + 6i| \\quad \\text{and} \\quad |-3.5 + 2i|.\n\\end{equation}\\]\n\n\nConsider two complex numbers \\(z = a + bi\\) and \\(y = c + di\\). Then, we have already seen that \\[\\begin{align*}\nz y\n& = (a + bi) \\times (c + di) = (ac - bd) + (ad + bc)i.\n\\end{align*}\\] We can compute the square of the modulus of the product \\(zy\\) as \\[\\begin{align*}\n|zy|^2\n& = (ac - bd)^2 + (ad + bc)^2 \\\\\n& = a^2 c^2 - 2 abcd + b^2 d^2 + a^2 d^2 + 2 abcd + b^2 c^2 \\\\\n& = a^2 c^2 + b^2 d^2 + a^2 d^2 + b^2 c^2 \\\\\n& = (a^2 + b^2) (c^2 + d^2),\n\\end{align*}\\] and we have computed that \\(|zy|= |z| |y|\\).\nIn particular, if \\(y\\) has modulus 1, then \\(|zy| = |z|\\). This means that \\(zy\\) and \\(z\\) are the same distance from the origin but ‘point’ in different directions. We can write the real and imaginary parts as \\(y = c + di = \\cos(\\theta) + i\n\\sin(\\theta)\\), where \\(\\theta\\) is the angle between the positive real axis and the line between \\(0\\) and \\(y\\). Then \\[\\begin{align*}\nz y\n& = (ac - bd) + (ad + bc)i \\\\\n& = (a \\cos(\\theta) - b \\sin(\\theta)) + (a \\sin(\\theta) + b\\cos(\\theta)) i.\n\\end{align*}\\] Recalling the example of a rotation matrix from (TODO is it there?), we see that multiplying by \\(y\\) is the same as rotating the complex point (\\(z\\)) by an angle of \\(\\theta\\) radians in the anticlockwise direction.\nThis leads us to thinking polar coordinates for the complex plane. Polar coordinates are a different form of coordinates that replace the usual \\(x\\) and \\(y\\)-directions (up and across) by two values which represent the distance to the origin (that we call radius) and angle to the positive \\(x\\)-axis (that we call the angle). When talking about a complex numbers \\(z\\) represented in the complex plane, we know that the modulus \\(|z|\\) represents the radius. The idea of \\(\\theta\\) above represents the angle of a complex number that we know call the argument.\n\nDefinition 7.2 Let \\(z\\) be a complex number. The polar form of \\(z\\) is \\(R (cos \\theta + i\n\\sin \\theta)\\). We call \\(R\\) the modulus of \\(z\\) and \\(\\theta\\) is the argument of \\(z\\).\n\nThe representation of the angle only unique up to adding integer multiples of \\(2\n\\pi\\), since rotating a point by \\(2 \\pi\\) about the origin leaves it unchanged.\n\nExample 7.8 Let \\(z = 12 -12i\\). Then \\[\\begin{align*}\n|z| = |12 - 12i| = \\sqrt{12^2 + 12^2} = \\sqrt{2 \\times 144} = 12 \\sqrt{2}.\n\\end{align*}\\] We have \\(\\arg{z} = -\\pi / 4\\) since \\[\\begin{equation*}\n\\cos(-\\pi/4) = \\frac{1}{\\sqrt{2}}, \\quad \\text{and} \\quad \\sin(-\\pi/4)\n= \\frac{-1}{\\sqrt{2}},\n\\end{equation*}\\] so\n\\[\\begin{equation*}\n12 \\sqrt{2} (\\cos(-\\pi/4) + i \\sin(-\\pi/4))\n= 12 \\sqrt{2} \\left( \\frac{1}{\\sqrt{2}} + i \\frac{-1}{\\sqrt{2}} \\right)\n= 12 - 12i.\n\\end{equation*}\\]\n\nExercise 7.7 Compute the modulus and argument of \\(2\\), \\(3i\\) and \\(4 + 4i\\).\n\n\n\nExample 7.9  \n\n\n\n\n\n\n\n\n\n\nThe polar representation of complex numbers then gives us a nice way to understand multiplication of complex numbers. If \\(y \\neq 0\\), then we can check that \\(\\left| \\frac{y}{|y|} \\right| = 1\\) and \\(\\arg{y} = \\arg{\\frac{y}{|y|}}\\). Then writing \\(zy = z \\frac{y}{|y|} |y|\\), we can use our calculations above to infer that multiplying by \\(y\\) corresponds to a anti-clockwise rotation by \\(\\arg{y}\\) then scaling by \\(|y|\\).\n\nExercise 7.8 Check that for any non-zero complex number \\(y\\), that \\(\\left| \\frac{y}{|y|}\n\\right| = 1\\) and \\(\\arg{y} = \\arg{\\frac{y}{|y|}}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#solving-polynomial-equations",
    "href": "src/lec06.html#solving-polynomial-equations",
    "title": "7  Complex numbers",
    "section": "7.4 Solving polynomial equations",
    "text": "7.4 Solving polynomial equations\nAs we have mentioned above, we will be using complex numbers when solving polynomial equations to work out eigenvalues and eigenvectors of matrices later in the section. The reason complex numbers are useful here is this very important Theorem:\n\nTheorem 7.1 (The Fundamental Theorem of Algebra) For any complex numbers \\(a_0, \\ldots, a_n\\) not all zero, there is at least one complex number \\(z\\) which satisfies: \\[\\begin{equation*}\na_n z^n + \\cdots + a_1 z + a_0 = 0\n\\end{equation*}\\]\n\nIt is really important to note here that this is not true if we want \\(z\\) to be a real number. Let’s revisit Example 7.1.\n\nExample 7.10 Consider the polynomial equation \\[\\begin{equation}\nx^2 + 1 = 0.\n\\end{equation}\\]\nWe saw before that this equation has no solution over the real numbers, but the Fundamental Theorem of Algebra tells us there must be at least one solution which is a complex number. In fact it has two solutions - \\(i\\) and \\(-i\\): \\[\\begin{align*}\n& i^2 + 1 = 0 \\\\\n& (-i)^2 + 1 = (-1)^2 i^2 + 1 = i^2 + 1 = 0.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNotice that along the real line (Imaginary part \\(=0\\)), the value of the function is always above 1.\n\nIn general, to find complex roots of other quadratic equations, we can apply the quadratic formula:\n\nExample 7.11 To find the values of \\(z\\) which satisfy \\(z^2 - 2z + 2 = 0\\), we see: \\[\\begin{align*}\nz = \\frac{+2 \\pm \\sqrt{(-2)^2 - 4 \\times 1 \\times 2}}{2} =\n\\frac{2 \\pm \\sqrt{-4}}{2} = \\frac{2 \\pm 2 \\sqrt{-1}}{2} = 1 \\pm i.\n\\end{align*}\\]\n\nExercise 7.9 Find the value of \\(z\\) which satisfy \\(z^2 - 4z + 20 = 0\\).\n\n\nWe will see in later sections that although this is one possible solution to compute the eigenvalues for \\(2 \\times 2\\) matrices this approach becomes impossible for larger size matrices and we need another approach!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec07.html",
    "href": "src/lec07.html",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "",
    "text": "8.1 Key definitions\nThis section of the notes will introduce our second big linear algebra problem. Throughout, we will be considering a square (\\(n\\times n\\)) matrix.\nFor this problem, we will think of a matrix \\(A\\) acting on functions \\(\\vec{x}\\): \\[\\begin{equation*}\n\\vec{x} \\mapsto A \\vec{x}.\n\\end{equation*}\\] We are interested in when is the output vector \\(A \\vec{x}\\) is parallel to \\(\\vec{x}\\).\nIf \\(A \\vec{x} = \\vec{0}\\), then \\(\\vec{x}\\) is an eigenvector associated to the eigenvalue \\(0\\). In fact, we know that \\(0\\) is an eigenvalue of \\(A\\) if, and only if, \\(A\\) is singular.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#key-definitions",
    "href": "src/lec07.html#key-definitions",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "",
    "text": "Definition 8.1 We say that any vector \\(\\vec{x}\\) where \\(A \\vec{x}\\) is parallel is \\(\\vec{x}\\) is called an eigenvector of \\(A\\). Here by parallel, we mean that there exists a number \\(\\lambda\\) (can be positive, negative or zero) such that \\[\\begin{equation}\n\\label{eq:evalues}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We call the associated number \\(\\lambda\\) an eigenvalue of \\(A\\).\nWe will later see that an \\(n \\times n\\) square matrix always has \\(n\\) eigenvalues (which may not always be distinct).\n\n\n\nExample 8.1 Let \\(P\\) be the 3x3 matrix that represents projection on to a plane \\(\\pi\\). What are the eigenvalues and eigenvectors of \\(p\\)?\n\n\n\n\n\n\n\n\n\n\nif \\(\\vec{x}\\) is in the plane \\(\\Pi\\), then \\(P \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the associated eigenvalue is \\(1\\).\nif \\(\\vec{y}\\) is perpendicular to the plane \\(\\Pi\\), then \\(P \\vec{y} = \\vec{0}\\). this means that \\(\\vec{y}\\) is an eigenvector and the associated eigenvalue is \\(0\\).\n\nLet \\(\\vec{y}\\) be perpendicular to \\(\\Pi\\) (so that \\(P \\vec{y} = \\vec{0}\\) and \\(\\vec{y}\\) is an eigenvector of \\(P\\)), then for any number \\(s\\), we can compute \\[\\begin{equation*}\nP (s \\vec{y}) = s P \\vec{y} = s \\vec{0} = \\vec{0}.\n\\end{equation*}\\] This means that \\(s \\vec{y}\\) is also an eigenvector of \\(P\\) associated to the eigenvalue \\(0\\). As a consequence when we compute eigenvectors, we need to take care to normalise the vector to ensure we get a unique answer.\nWe see we end up with a two-dimensional space of eigenvectors (i.e., the plane \\(\\Pi\\)) associated to eigenvalue \\(1\\) and a one-dimensional space of eigenvectors (i.e., the line perpendicular to \\(\\Pi\\)) eigenvalue \\(0\\). We use the term eigenspace the space of eigenvectors associated to a particular eigenvalue.\n\n\nExample 8.2 Let \\(A\\) be the permuatation matrix which takes an input two-vector and outputs a two-vector with the components swapped. The matrix is given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n0 & 1 \\\\ 1 & 0 \\\\\n\\end{pmatrix}.\n\\end{equation*}\\] What are the eigenvectors and eigenvalues of \\(A\\)?\n\nLet \\(\\vec{x} = (1, 1)^T\\), the swapping the components of \\(\\vec{x}\\) gives back the same vector \\(\\vec{x}\\). In equations, we can write \\(A \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the eigenvalue is \\(1\\).\nLet \\(\\vec{x} = (-1, 1)^T\\), the swapping the components of \\(\\vec{x}\\) gives back \\((1, -1)^T\\) which we can see is \\(-\\vec{x}\\). In equations, we can write \\(A\n\\vec{x} = -\\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector of \\(A\\) and the associated eigenvalue is \\(-1\\).\n\nHere we see that again we actually have two one-dimensional eigenspaces.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#properties-of-eigenvalues-and-eigenvectors",
    "href": "src/lec07.html#properties-of-eigenvalues-and-eigenvectors",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "8.2 Properties of eigenvalues and eigenvectors",
    "text": "8.2 Properties of eigenvalues and eigenvectors\nTODO\nEigenvalues and eigenvectors can be used to completely describe the transformation described by \\(A\\).\nLet \\(A\\) and \\(B\\) be two \\(n \\times n\\) matrices. Then, we cannot use the eigenvalues of \\(A\\) and \\(B\\) to work out the eigenvalues of \\(A + B\\) or the eigenvalues of \\(A B\\), in general.\nSum of eigenvalues is trace Product of eigenvalues is determinant\n\nRemark 8.1. If we add up all the eigenvalues of a \\(n \\times n\\) matrix, we get the trace of the matrix \\(A\\). We can also find the trace by adding up the diagonal components of the matrix: \\[\\begin{equation*}\n\\lambda_1 + \\cdots + \\lambda_n = a_{11} + a_{22} + \\cdots + a_{nn} = \\mathrm{trace}(A).\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#how-to-find-eigenvalues-and-eigenvectors",
    "href": "src/lec07.html#how-to-find-eigenvalues-and-eigenvectors",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "8.3 How to find eigenvalues and eigenvectors",
    "text": "8.3 How to find eigenvalues and eigenvectors\nTo compute eigenvalues and eigenvectors, we start from \\(\\eqref{eq:evalues}\\) and move everything to the left-hand side and use the identity matrix \\((I_n)\\): \\[\\begin{equation*}\n(A - \\lambda I_n) \\vec{x} = \\vec{0}.\n\\end{equation*}\\] This tells us that if we want to find an eigenvalue of \\(A\\), then we need to find a number \\(\\lambda\\) such that \\((A - \\lambda I_n)\\) can multiply a matrix and give us back the zero-vector. This happens when \\((A - \\lambda I)\\) is singular.\nOne way to test if a matrix is singular, is if the determinant is 0. This gives us a test we can use to determine eigenvalues: \\[\\begin{equation}\n\\label{eq:char}\n\\det( A - \\lambda I_n ) = 0.\n\\end{equation}\\] In fact, this equation no longer depends on the eigenvector \\(\\vec{x}\\), and if we can find solutions \\(\\lambda\\) to this equation then \\(\\lambda\\) is an eigenvalue of \\(A\\).\nWe call \\(\\eqref{eq:char}\\) the characteristic equation or eigenvalue equation. We will see that \\(\\eqref{eq:char}\\) gives us a degree \\(n\\) polynomial equation in \\(\\lambda\\).\nOnce we have found an eigenvalue by solving the characteristic equation for a value \\(\\lambda^*\\), we need to find a vector \\(\\vec{x}\\) such that \\[\\begin{equation*}\n(A - \\lambda^*) \\vec{x} = \\vec{0}.\n\\end{equation*}\\] In general, this is possible using a variation of Gaussian elimination with pivoting, but we do not explore this method in this module.\n\nExample 8.3 Let \\(A\\) be the matrix given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n3 & 1 \\\\ 1 & 3 \\\\\n\\end{pmatrix}.\n\\end{equation*}\\]\nThen, we can compute that \\[\\begin{align*}\n\\det(A - \\lambda I_n)\n& = \\det \\begin{pmatrix}\n3 - \\lambda & 1 \\\\ 1 & 3 - \\lambda\n\\end{pmatrix} \\\\\n& = (3 - \\lambda)(3 - \\lambda) - 1 \\times 1 \\\\\n& = \\lambda^2 + 6 \\lambda + 8.\n\\end{align*}\\] So we want to find values \\(\\lambda\\) such that \\[\\begin{equation*}\n\\det(A - \\lambda I_n) =\\lambda^2 + 6 \\lambda + 8 = 0.\n\\end{equation*}\\] We can read off, by factorisation, that the values of \\(\\lambda\\) are \\(4\\) and \\(2\\).\nWe can now start computing the associated eigenvectors.\nTo find the eigenvector associated with the eigenvalue \\(4\\). We see that \\[\\begin{equation*}\nA - 4 I_n = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}.\n\\end{equation*}\\] We can identify that \\((A - 4I_n) (1, 1)^T = \\vec{0}\\). So \\((1, 1)\\) is an eigenvector associated with \\(4\\).\nTo find the eigenvector associated with the eigenvalue \\(2\\). We see that \\[\\begin{equation*}\nA - 2I_n = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n\\end{equation*}\\] We can identify that \\((A - 2 I_n) (-1, 1)^T = \\vec{0}\\). So \\((-1, 1)\\) is an eigenvector associated with \\(2\\).\n\nWe note that this example is actually surprisingly similar to Example 8.2. We see that the eigenvectors are actually the same! We can see that the matrices are related too: \\[\\begin{equation*}\n\\begin{pmatrix}\n0 & 1 \\\\ 1 & 0 \\\\\n\\end{pmatrix}\n-3 I\n= \\begin{pmatrix}\n3 & 1 \\\\ 1 & 3\n\\end{pmatrix}.\n\\end{equation*}\\] So we can compute that if \\(A \\vec{x} = \\lambda \\vec{x}\\) then \\[\\begin{equation*}\n(A + 3 I) \\vec{x} = \\lambda \\vec{x} + 3 \\vec{x} = (\\lambda + 3) \\vec{x}.\n\\end{equation*}\\] So we see that \\(\\vec{x}\\) is also an eigenvector of \\(A\\) and the associated eigenvector is \\(\\lambda + 3\\).\nAlthough this procedure is robust to finding eigenvalues there are cases where we have to be a bit careful. We have seen one example above with a two-dimensional eigenspace associated with one eigenvector (Example 8.1). Here are two other cases we must be careful:\n\nExample 8.4 Let \\(Q\\) denote the \\(2 \\times 2\\) matrix that rotates any vector by \\(\\pi/2\\) (\\(=90^\\circ\\)): \\[\\begin{equation*}\nQ = \\begin{pmatrix}\n0 & -1 \\\\ 1 & 0\n\\end{pmatrix}.\n\\end{equation*}\\] Our intuition says that there can be no vectors that when rotated by \\(\\pi/2\\) give something parallel to the input vector, but we can still compute: \\[\\begin{equation*}\n\\det Q = \\det \\begin{pmatrix}\n-\\lambda & -1 \\\\ 1 & -\\lambda\n\\end{pmatrix}\n= \\lambda^2 + 1.\n\\end{equation*}\\] So we can find eigenvalues by finding the values \\(\\lambda\\) such that \\[\\begin{equation*}\n\\lambda^2 + 1 = 0.\n\\end{equation*}\\] We saw in the section on Complex Numbers (@sec:complex-numbers) that the solutions to this equation are \\(\\pm i\\). This means our algorithms for finding eigenvalues and eigenvectors need to handle complex numbers too.\n\n\nExample 8.5 Let \\(A\\) be the \\(2 \\times 2\\) matrix given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n3 & 1 \\\\ 0 & 3\n\\end{pmatrix}.\n\\end{equation*}\\] If we follow our procedure above we get a single repeated eigenvalue \\(3\\).\nLooking at the shifted matrix, \\(A - 3 I_n\\): \\[\\begin{equation*}\nA - 3 I_n = \\begin{pmatrix}\n0 & 1 \\\\ 0 & 0\n\\end{pmatrix}.\n\\end{equation*}\\] we can identify one eigenvector \\((1, 0)^T\\), but there is no other eigenvector (in a different direction)! Indeed, we can compute that: \\[\\begin{align*}\n(A - 3 I_n) \\begin{pmatrix}\nx \\\\ y\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & 1 \\\\ 0 & 0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y\n\\end{pmatrix}\n= \\begin{pmatrix}\ny \\\\ 0\n\\end{pmatrix}.\n\\end{align*}\\] This tells us that if \\((A - 3 I_n) (x, y)^T = \\vec{0}\\) if, and only if, \\(y = 0\\). Thus all eigenvector have the form \\((x, 0)^T\\) and point in the same direction as \\((1, 0)^T\\).\n\n\nExercise 8.1 Find the eigenvalues and eigenvectors for the matrices\n\\[\\begin{equation*}\nA = \\begin{pmatrix} 9 & -2 \\\\ -2 & 6 \\end{pmatrix}.\n\\end{equation*}\\]\nTODO add another complex example",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#important-theory",
    "href": "src/lec07.html#important-theory",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "8.4 Important theory",
    "text": "8.4 Important theory\nWe have established a way to identify eigenvalues and eigenvectors for an arbitrary square matrix. It turns out this method can be used to prove the existance of eigenvalues.\n\nTheorem 8.1 Any square \\(n \\times n\\) matrix has \\(n\\) complex eigenvalues (possibly not distinct).\n\n\nProof. For any matrix the characteristic equation \\(\\eqref{eq:char}\\) is a degree \\(n\\) polynomial. The Fundamental Theorem of Algebra (Theorem 7.1) tells us that any degree \\(n\\) polynomial has \\(n\\) roots over the complex numbers. The \\(n\\) roots of the characteristic equation are the \\(n\\) eigenvalues.\n\nThe Abel-Ruffini theorem states that there is no solution in the radicals for a general polynomial of degree 5 or higher with arbitrary coefficients. This implies that there is no ‘nice’ closed form for roots of polynomials of degree 5 or higher. So, if we want an algorithm to find eigenvalues and eigenvectors of larger matrices then we need to do something else!\nLet’s suppose that we have an \\(n \\times n\\) matrix \\(A\\) and we have found \\(n\\) eigenvectors and \\(n\\) eigenvalues (all distinct). Let’s call the eigenvectors by \\(\\vec{x}_{1}, \\ldots \\vec{x}_{n}\\) and the eigenvalues \\(\\lambda_1, \\ldots\n\\lambda_n\\) then we have the equation: \\[\\begin{equation*}\nA \\vec{x}_j = \\lambda_j \\vec{x}_j.\n\\end{equation*}\\] So if we form the matrices \\(S\\) to have columns equal to each eigenvector in turn and \\(\\Lambda\\) (pronounced lambda) to be the diagonal matrix with the eigenvalues listed along the diagonal we see that we have: \\[\\begin{equation*}\nA S = S \\Lambda.\n\\end{equation*}\\] If \\(S\\) is invertible, we can multiply on the right by \\(S^{-1}\\) to see that we have \\[\\begin{equation}\n\\label{eq:SLamSinv}\nA = S \\Lambda S^{-1}.\n\\end{equation}\\] This formula shows another factorisation of the matrix \\(A\\) into simpler matrices, very much like we had when we computed the LU-factorisation matrix (Section 5.7).\nThe equation \\(\\eqref{eq:SLamSinv}\\) is an example of a more general idea of similar matrices. We say that two matrices \\(A\\) and \\(B\\) are similar if there exists an invertible \\(n \\times n\\) matrix \\(P\\) such that \\[\\begin{equation*}\nB = P^{-1} A P.\n\\end{equation*}\\] Since \\(P\\) is invertible, we can pre-multiply this equation by \\(P\\) and post-multiply by \\(P^{-1}\\) and see that being similar is a symmetric property.\n\nLemma 8.1 The matrix \\(A\\) is similar to the diagonal matrix \\(\\Lambda\\) formed by the eigenvalues of \\(A\\).\n\n\nProof. From \\(\\eqref{eq:SLamSinv}\\), we have that \\[\\begin{equation*}\n\\Lambda = S^{-1} A S.\n\\end{equation*}\\]\n\nThis leads to a nice theorem which we will use to help compute eigenvectors and eigenvalues of larger matrices:\n\nTheorem 8.2 If \\(A\\) and \\(B\\) are similar matrices then \\(A\\) and \\(B\\) have the same eigenvalues.\n\n\nProof. We start by writing \\(B = P^{-1} A P\\). Then we can compute that \\[\\begin{equation}\n\\label{eq:Bsim-alt}\nB P^{-1} = P^{-1} A.\n\\end{equation}\\] Let \\(\\lambda\\) be an eigenvalue of \\(A\\) with eigenvector \\(\\vec{x}\\) and write \\(\\vec{y} = P^{-1} \\vec{x}\\). Then we have that \\[\\begin{align*}\nB \\vec{y} & = B P^{-1} \\vec{x} && \\text{(definition of $\\vec{y}$)} \\\\\n& = P^{-1} A \\vec{x}  && \\text{(from \\eqref{eq:Bsim-alt})} \\\\\n& = P^{-1} (\\lambda \\vec{x}) && \\text{(since $\\vec{x}$ is an eigenvector)} \\\\\n& = \\lambda P^{-1} \\vec{x} && \\text{(rearranging)} \\\\\n& = \\lambda \\vec{y} && \\text{(definition of $\\vec{y}$)}.\n\\end{align*}\\] This shows that any eigenvalue of \\(A\\) is an eigenvalue of \\(B\\). It also gives a formula for how eigenvectors change between \\(A\\) and \\(B\\).\nTo show any eigenvalue of \\(B\\) is an eigenvalue of \\(A\\), we simply repeat the calculation with \\(A\\) and \\(B\\) swapped.\n\nThe key idea of the methods we will use to compute eigenvalues to apply a sequence of matrices to convert a matrix \\(A\\) into a form similar to \\(A\\) for which reading off the eigenvalues is easier. However, the quality of the algorithms we apply depend heavily on properties of the matrix \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#sec-symmetric-nice",
    "href": "src/lec07.html#sec-symmetric-nice",
    "title": "8  Eigenvectors and eigenvalues",
    "section": "8.5 Why symmetric matrices are nice",
    "text": "8.5 Why symmetric matrices are nice\nFor the inner product of two complex vectors, we additionally take the complex conjugate of the entries of \\(\\vec{b}\\). We call this the Hermitian product and write \\(\\langle \\vec{a}, \\vec{b} \\rangle = \\vec{a} \\cdot \\bar{\\vec{b}} =\n\\sum_{i=1}^n a_i \\bar{b_i}\\). We denote the conjugate transform of a matrix \\(A\\) by \\(A^H = \\bar{A}^T\\). We say that \\(A\\) is Hermitian if \\(A^H = A\\) and say that if \\(\\langle \\vec{a}, \\vec{b} \\rangle = 0\\) that \\(\\vec{a}, \\vec{b}\\) are orthogonal.\nWe notice that for any matrix \\(A\\) \\[\\begin{equation}\n\\label{eq:orthog}\n\\begin{aligned}\n\\langle A \\vec{a}, \\vec{b} \\rangle\n& = \\sum_{i=1}^n (A \\vec{a})*i \\bar{b}*i \\\\\n& = \\sum*{i, j=1}^n (A*{ij} a_j) \\bar{b}*i \\\\\n& = \\sum*{i, j=1}^n a_j (A_{ij} \\bar{b}*i) \\\\\n& = \\sum*{i, j=1}^n a_j \\bar{((A^H)*{ji} b_i)} \\\\\n& = \\sum*{i=1}^n a_i \\bar{(A^H \\vec{b})_i} \\\\\n& = \\langle \\vec{a}, A^H \\vec{b} \\rangle\n\\end{aligned}\n\\end{equation}\\] In particular this means that if \\(A = A^H\\) then \\(\\langle A \\vec{a}, \\vec{b}\n\\rangle = \\langle \\vec{a}, A \\vec{b} \\rangle\\). We call a matrix Hermitian if \\(A = A^H\\).\nFor any complex vector \\(\\vec{a}\\), we see that \\[\\begin{align*}\n\\langle \\vec{a}, \\vec{a} \\rangle = \\sum_{i=1}^n a_i \\bar{a_i} = \\sum_{i=1}^n\na_i|^2.\n\\end{align*}\\] Since \\(|a_i|\\) is always a non-negative real number, we have that \\(\\langle\n\\vec{a}, \\vec{a} \\rangle \\ge 0\\). Furthermore, we see that \\(\\langle \\vec{a},\n\\vec{a} \\rangle = 0\\) if, and only if, \\(\\vec{a} = 0\\).\n\nTheorem 8.3 Let \\(A\\) be a symmetric matrix (\\(A^T = A\\)) with real entries. Then \\(A\\) has \\(n\\) real eigenvalues (zero imaginary part) and its eigenvectors are orthogonal.\n\n\nProof. Let \\(\\lambda\\) be an eigenvalue of \\(A\\) with eigenvector \\(\\vec{x}\\). Recall that \\(\\vec{x} \\neq 0\\). Then, since \\(A\\) has real values, we can compute that: \\[\\begin{equation*}\n\\bar{(A \\vec{x})*i} = \\bar{\\sum*{j=1}^n A_{ji} x_i} = \\sum_{j=1}^n A_{ji}\n\\bar{x_i} = (A \\bar{\\vec{x}})_i.\n\\end{equation*}\\] We also note that any real, symmetric matrix is automatically Hermitian.\nThen we see that \\[\\begin{align*}\n\\lambda \\langle \\vec{x}, \\vec{x} \\rangle\n& = \\langle (\\lambda \\vec{x}), \\vec{x} \\rangle \\\\\n& = \\langle (A \\vec{x}), \\vec{x} \\rangle \\\\\n& = \\langle \\vec{x}, A^H \\vec{x} \\rangle \\\\\n& = \\langle \\vec{x}, A \\vec{x} \\rangle \\\\\n& = \\langle \\vec{x}, \\lambda \\vec{x} \\rangle \\\\\n& = \\bar{\\lambda} \\langle \\vec{x}, \\vec{x} \\rangle.\n\\end{align*}\\] Since, \\(\\langle \\vec{x}, \\vec{x} \\rangle &gt; 0\\) (reccall \\(\\vec{x} \\neq 0\\)), we can divide by \\(\\langle \\vec{x}, \\vec{x} \\rangle\\) so infer that \\[\\begin{equation*}\n\\lambda = \\bar{\\lambda}.\n\\end{equation*}\\]\nNext, let \\(\\vec{x}\\) and \\(\\vec{y}\\) be eigenvectors of \\(A\\) with distinct, eigenvalues \\(\\lambda\\) and \\(\\mu\\), respectively. From the first part of the proof, we know that \\(\\lambda\\) and \\(\\mu\\) are real. We compute that \\[\\begin{align*}\n\\lambda \\langle \\vec{x}, \\vec{y} \\rangle\n& = \\langle \\lambda \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\langle A \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, A^H \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, A \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, \\mu \\vec{y} \\rangle \\\\\n& = \\bar{\\mu} \\langle \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\mu \\langle \\vec{x}, \\vec{y} \\rangle.\n\\end{align*}\\] Subtracting the right-hand side from the left hand side we see that \\[\\begin{equation*}\n(\\lambda - \\mu) \\langle \\vec{x}, \\vec{y} \\rangle = 0.\n\\end{equation*}\\] This implies that if \\(\\lambda\\) and \\(\\mu\\) are distinct, that \\(\\langle \\vec{x},\n\\vec{y} \\rangle = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec08.html",
    "href": "src/lec08.html",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "",
    "text": "9.1 Jacobi transformations of a symmetric matrix\nIn the previous lecture, we defined the eigenvalue problem for a matrix \\(A\\): Finding numbers \\(\\lambda\\) (eigenvalues) and vectors \\(\\vec{x}\\) (eigenvectors) which satisfy the equation: \\[\\begin{equation}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We saw one starting point for finding eigenvalues is to find the roots of the characteristic equation: a polynomial of degree \\(n\\) for an \\(n \\times n\\) matrix \\(A\\). But we already have seen that this approach will be infeasible for large matrices. Instead, we will find a sequence of similar matrices to \\(A\\) such that we can read off the eigenvalues from the final matrix.\nIn equations, we can say our “grand strategy” is to find a sequence of matrices \\(P_1, P_2, \\ldots\\) to form a sequence of matrices: \\[\\begin{equation}\n\\label{eq:similarity_transform}\nA, P_1^{-1} A P, P_2^{-1} P_1^{-1} A P_1 P_2, P_3^{-1} P_2^{-1} P_1^{-1} A P_1\nP_2 P_3, \\ldots\n\\end{equation}\\] If we get all the way to a diagonal matrix, at level \\(m\\) say, then the eigenvalues are the diagonal of the matrix \\[\\begin{equation*}\nP_m^{-1} P_{m-1}^{-1} \\cdots P_2^{-1} P_1^{-1} A P_1 P_2 \\cdots P_{m-1} P_m,\n\\end{equation*}\\] and the eigenvectors are the columns of the matrix \\[\\begin{equation*}\nS_m = P_1 P_2 \\cdots P_{m-1} P_m.\n\\end{equation*}\\] Sometimes, we only want to compute eigenvalues, and not eigenvectors, then it is sufficient to transform the matrix to be triangular (either upper or lower triangular). Then, we can read off that the eigenvalues are the diagonal entries (see ?exm-eigenvalue-triangular).\nThe Jacobi method forms a sequence of similarity transforms similar to \\(\\eqref{eq:similarity_transform}\\). It is a foolproof approach for real, symmetric matrices (@sim-symmetric-nice).\nThe key idea is to use the matrix \\(P_{pq}\\) which is the matrix of the form \\[\\begin{equation}\n\\label{eq:Pmatrix}\nP_{pq} = \\begin{pmatrix}\n  1 &&&&&&& \\\\\n  & \\ddots &&&&&& \\\\\n  && c & \\cdots & s &&& \\\\\n  && \\vdots & 1 & \\vdots &&& \\\\\n  && -s & \\cdots & c &&& \\\\\n  &&&&&& \\ddots & \\\\\n  &&&&&&& 1\n\\end{pmatrix}\n\\end{equation}\\] That is a matrix where all diagonal elements are one except for the two elements \\(c\\) is rows \\(p\\) and \\(q\\). All off-diagonal elements are zero except the two elements \\(s\\) and \\(-s\\) in positions \\((p, q)\\) and \\((q, p)\\). We will choose \\(c, s\\) to be the cosine and sine of a particular angle (so that \\(c^2 + s^2 = 1\\)) which we specify later. Handily this means that \\(P_{pq}^{-1} = P_{pq}^T\\).\nWe will apply the matrix \\(P_{pq}\\) as a similarity transformation: \\[\\begin{equation*}\nA' = P_{pq}^{T} A P.\n\\end{equation*}\\]\nThe general case follows in a similar way. Multiplying out \\(A' = P_{pq}^{T} A\nP_{pq}\\) for a general \\(n \\times n\\) matrix gives us the update formulae: \\[\\begin{align*}\nA'*{r, p} & = c A*{r, p} - s A_{r, q}\n&& \\text{for} \\quad r \\in \\{1, \\ldots n\\} \\setminus \\{p, q\\} \\\\\nA'*{r, q} & = c A*{r, q} + s A_{r, p}\n&& \\text{for} \\quad r \\in \\{1, \\ldots n\\} \\setminus \\{p, q\\} \\\\\nA'*{q, r} & = c A*{p, r} - s A_{q, r}\n&& \\text{for} \\quad r \\in \\{1, \\ldots n\\} \\setminus \\{p, q\\} \\\\\nA'*{q, r} & = c A*{q, r} + s A_{p, r}\n&& \\text{for} \\quad r \\in \\{1, \\ldots n\\} \\setminus \\{p, q\\} \\\\\nA'*{p, p} & = c^2 A*{p, p} + s^2 A_{q, q} - 2 c s A_{p, q} \\\\\nA'*{q, q} & = s^2 A*{p, p} + c^2 A_{q, q} + 2 c s A_{p, q} \\\\\nA'*{p, q} & = (c^2 - s^2) A*{p, q} + c s (A_{p, p} - A_{q, q}) \\\\\nA'*{q, p} & = (c^2 - s^2) A*{q, p} + c s (A_{p, p} - A_{q, q}) \\\\\n\\end{align*}\\] The equation we need to solve is to find \\(\\theta\\) such that \\[\\begin{equation*}\nA'*{p, q} = (\\cos^2\\theta - \\sin^2\\theta) A*{p, q} + \\cos\\theta \\sin\\theta\n(A_{p, p} - A_{q, q}) = 0\n\\end{equation*}\\] As in the example, we see that \\(\\pi/2 + m \\pi\\) is not a solution, so we are safe to assume that \\(\\cos\\theta \\neq 0\\). Further, since we want to set \\(A'_{p, q}\\) to zero, we can assume that \\(A_{p, q} \\neq 0\\) (otherwise we don’t need to address this pair of \\(p\\) and \\(q\\)). So we will divide by \\(-\\cos^2\\theta A_{p, q}\\) to get a quadratic equation in \\(\\tan\\theta\\): \\[\\begin{equation*}\n-1 + \\tan^2\\theta - \\tan\\theta \\frac{A_{p, p} - A_{q, q}}{A_{p, q}} = 0\n\\end{equation*}\\] We follow the example above and take the smallest root using the quadratic formula and apply the same trigonometric identities in order to compute \\(c\\) and \\(s\\) \\(\\eqref{eq:trig_id}\\).\nThe only remaining step is to decide which order to cover \\(p\\) and \\(q\\). The update formula only needs to be applied to the upper triangular part of the matrix away from the diagonal. In Jacobi’s original algorithm from 1846, he proposed to eliminate each of the largest possible values in turn. This turns out to be computationally expensive and not necessary. Instead we simply iterate through all possible nonzero values until we have eliminated all off-diagonal values.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#jacobi-transformations-of-a-symmetric-matrix",
    "href": "src/lec08.html#jacobi-transformations-of-a-symmetric-matrix",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "",
    "text": "Example 9.1 Let \\(A\\) be the \\(3 \\times 3\\) matrix given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n1 & 2 & \\frac{3 \\sqrt{2 - \\sqrt{2}}}{4}\\\\2 & 5 & \\frac{3 \\sqrt{\\sqrt{2} + 2}}{4}\n\\\\\\frac{3 \\sqrt{2 - \\sqrt{2}}}{4} & \\frac{3 \\sqrt{\\sqrt{2} + 2}}{4} & 2 \\sqrt{2}\n\\end{pmatrix}.\n\\end{equation*}\\]\nLet’s apply the matrix as a similarity transform with \\(p=1, q=2\\). Then we have \\[\\begin{equation*}\nP_{1,2} = \\begin{pmatrix}\nc & s & 0\\\\- s & c & 0\\\\0 & 0 & 1\n\\end{pmatrix},\n\\end{equation*}\\] where \\(c = \\cos(\\theta)\\) and \\(s = \\sin(\\theta)\\) and \\(\\theta\\) is still to be determined.\nWe can compute that \\[\\begin{equation*}\nA P_{1,2} =\n\\begin{pmatrix}\nc - 2 s & 2 c + s & \\frac{3 \\sqrt{2 - \\sqrt{2}}}{4}\\\\\n2 c - 5 s & 5 c + 2 s & \\frac{3 \\sqrt{\\sqrt{2} + 2}}{4}\\\\\n\\frac{3 c \\sqrt{2 - \\sqrt{2}}}{4} - \\frac{3 s \\sqrt{\\sqrt{2} + 2}}{4} &\n\\frac{3 c \\sqrt{\\sqrt{2} + 2}}{4} + \\frac{3 s \\sqrt{2 - \\sqrt{2}}}{4} &\n2 \\sqrt{2}\n\\end{pmatrix}\n\\end{equation*}\\] and \\[\\begin{equation*}\nA' = P_{1, 2}^T A P_{1,2} = \\begin{pmatrix}\nc^{2} - 4 c s + 5 s^{2} & 2 c^{2} - 4 c s - 2 s^{2} &\n\\frac{3 c \\sqrt{2 - \\sqrt{2}}}{4} - \\frac{3 s \\sqrt{\\sqrt{2} + 2}}{4}\\\\\n2 c^{2} - 4 c s - 2 s^{2} & 5 c^{2} + 4 c s + s^{2} &\n\\frac{3 c \\sqrt{\\sqrt{2} + 2}}{4} + \\frac{3 s \\sqrt{2 - \\sqrt{2}}}{4}\\\\\n\\frac{3 c \\sqrt{2 - \\sqrt{2}}}{4} - \\frac{3 s \\sqrt{\\sqrt{2} + 2}}{4} &\n\\frac{3 c \\sqrt{\\sqrt{2} + 2}}{4} + \\frac{3 s \\sqrt{2 - \\sqrt{2}}}{4} &\n2 \\sqrt{2}\n\\end{pmatrix}\n\\end{equation*}\\] Our grant aim is to nudge \\(A\\) towards being diagonal. Since, we’ve started with the \\((1, 2)\\) entry we might hope that we can make \\(A'_{1,2}\\) to be zero. To do this we need \\(\\theta\\) such that \\[\\begin{equation*}\n2 \\cos^{2}\\theta - 4 \\cos\\theta \\sin\\theta - 2 \\sin^2 \\theta = 0\n\\end{equation*}\\] We can see that if \\(\\theta = \\pi/2 + m \\pi\\) (for some \\(m \\in \\mathbb{Z}\\)), \\(\\cos\\theta = 0\\) but \\(\\sin\\theta = \\pm 1\\) and we don’t have a solution. Otherwise, \\(\\cos\\theta \\neq 0\\) so we can divide by \\(- 2 \\cos^2\\theta\\) to get \\[\\begin{equation*}\n\\tan^2\\theta + 2 \\tan(\\theta) - 1 = 0.\n\\end{equation*}\\] This is a quadratic equation in \\(\\tan\\theta\\) with roots: \\[\\begin{equation*}\n\\tan\\theta = -1 \\pm \\sqrt{2}.\n\\end{equation*}\\] Taking the smaller root (with \\(+\\) sign), we get (using some trigonometric identities): \\[\\begin{equation}\n\\label{eq:trig_id}\n\\begin{aligned}\n\\tan\\theta & = -1 + \\sqrt{2} \\\\\n\\cos\\theta & = 1 / \\sqrt{\\tan^2\\theta +1} = \\sqrt{\\sqrt{2}/4 + 1/2} \\\\\n\\sin\\theta & = \\tan\\theta \\cos\\theta\n=  \\sqrt{1/2 - \\sqrt{2}/4}.\n\\end{aligned}\n\\end{equation}\\]\nApplying these substitutions, we arrive at \\[\\begin{equation*}\nA' = \\begin{pmatrix}\n3 - 2 \\sqrt{2} & 0 & 0\\\\0 & 2 \\sqrt{2} + 3 & \\frac{3}{2}\\\\0 & \\frac{3}{2} &\n2 \\sqrt{2}\n\\end{pmatrix}\n\\end{equation*}\\]\n\nExercise 9.1 Continue this exercise with \\(P_{2, 3}\\). Show that \\(\\theta\\) can be chosen so that \\[\\begin{align*}\nA'' = P_{2, 3}^{-1} A' P_{2, 3}\n= \\begin{pmatrix}\n3 - 2 \\sqrt{2} & 0 & 0\\\\0 & \\frac{3}{2} + \\frac{7 \\sqrt{2}}{2} & 0\\\\0 & 0 &\n\\frac{\\sqrt{2}}{2} + \\frac{3}{2}\n\\end{pmatrix} \\\\\n\\approx\n\\begin{pmatrix}\n0.17157287525381 & 0 & 0\\\\0 & 6.44974746830583 & 0\\\\0 & 0 & 2.20710678118655\n\\end{pmatrix}.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#python-code",
    "href": "src/lec08.html#python-code",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "9.2 Python code",
    "text": "9.2 Python code\n\ndef jacobi_rotation(A, p, q):\n    \"\"\"\n    Computes one iteration of P_{pq}^T A P_{pq}.\n    Inputs: symmetric matrix A, indices p, q\n    Output: symmetric matrix A' with A'_{p, q} = 0 (and A'_{q, p} = 0).\n    \"\"\"\n\n    # copy matrix to new holder\n    A_new = A.copy()\n\n    # solve quadratic equation\n    b = (A[q, q] - A[p, p]) / A[p, q]\n    roots = (-b + np.sqrt(b * b + 4)) / 2, (-b - np.sqrt(b * b + 4)) / 2\n    t = min(roots, key=abs)  # take smallest in absolute value root\n\n    # determine c, s\n    c = 1.0 / np.sqrt(t * t + 1.0)\n    s = c * t\n\n    # do updates\n    A_new[:, p] = c * A[:, p] - s * A[:, q]\n    A_new[:, q] = c * A[:, q] + s * A[:, p]\n    A_new[p, :] = c * A[p, :] - s * A[q, :]\n    A_new[q, :] = c * A[q, :] + s * A[p, :]\n\n    A_new[p, p] = c**2 * A[p, p] + s**2 * A[q, q] - 2 * c * s * A[p, q]\n    A_new[q, q] = s**2 * A[p, p] + c**2 * A[q, q] + 2 * c * s * A[p, q]\n\n    A_new[p, q] = 0.0  # replace update formula with exact value\n    A_new[q, p] = 0.0  # replace update formula with exact value\n\n    return A_new\n\nLet’s try it out!\n\nA = np.array(\n    [\n        [1, 2, 3 * np.sqrt(2 - np.sqrt(2)) / 4],\n        [2, 5, 3 * np.sqrt(2 + np.sqrt(2)) / 4],\n        [\n            3 * np.sqrt(2 - np.sqrt(2)) / 4,\n            3 * np.sqrt(2 + np.sqrt(2)) / 4,\n            2 * np.sqrt(2),\n        ],\n    ]\n)\nnp_ev, _ = np.linalg.eig(A)\n\nprint(\"initial matrix\")\nprint(A)\n\nA = jacobi_rotation(A, 0, 1)\n\nprint(\"after one rotation (0, 1)\")\nprint(A)\n\nA = jacobi_rotation(A, 1, 2)\n\nprint(\"after second rotation (1, 2)\")\nprint(A)\n\nprint(\"Our estimate of the eigenvalues is\")\nprint(np.diag(A))\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_ev)\n\ninitial matrix\n[[1.         2.         0.57402515]\n [2.         5.         1.3858193 ]\n [0.57402515 1.3858193  2.82842712]]\nafter one rotation (0, 1)\n[[ 1.71572875e-01  0.00000000e+00 -1.11022302e-16]\n [ 0.00000000e+00  5.82842712e+00  1.50000000e+00]\n [-1.11022302e-16  1.50000000e+00  2.82842712e+00]]\nafter second rotation (1, 2)\n[[ 1.71572875e-01 -4.24863958e-17 -1.02571233e-16]\n [-4.24863958e-17  6.44974747e+00  0.00000000e+00]\n [-1.02571233e-16  0.00000000e+00  2.20710678e+00]]\nOur estimate of the eigenvalues is\n[0.17157288 6.44974747 2.20710678]\nNumpy's estimate of the eigenvalues is\n[6.44974747 0.17157288 2.20710678]\n\n\nLet’s try again with a harder problem\n\nn = 10\ntol = 1.0e-12\n\n# generate a random matrix\nnp.random.seed(42)\nS = special_ortho_group.rvs(n)\nD = np.diag(np.random.randint(-5, 5, (n,)))\nA = S.T @ D @ S\n\nprint(\"initial matrix A\")\nprint(A)\n\nnp_ev, _ = np.linalg.eig(A)\n\n# sweep over matrix several times\nfor sweep in range(10):\n    for i in range(n):\n        for j in range(i + 1, n):\n            if abs(A[i, j]) &lt; tol:\n                continue\n\n            A = jacobi_rotation(A, i, j)\n\n    non_diag_nonzeros = 0\n    non_diag_zeros = 0\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            if abs(A[i, j]) &lt; tol:\n                non_diag_zeros += 1\n            else:\n                non_diag_nonzeros += 1\n\n    print(f\"end of sweep {sweep}: {non_diag_zeros=} {non_diag_nonzeros=}\")\n    if non_diag_nonzeros == 0:\n        break\n\nprint(\"our estimate of the eigenvalues is\")\nprint(np.diag(A))\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_ev)\n\ninitial matrix A\n[[-1.37563753 -0.31873229 -2.41518457  0.00758962  0.43535083 -1.10245202\n  -0.23161883  1.40936996 -1.3653408   0.27180199]\n [-0.31873229 -2.28404485  0.78760996  0.22552486  0.14405941 -0.97513954\n   0.82696754  0.08329181 -0.50516593  0.02240545]\n [-2.41518457  0.78760996 -1.14147565 -0.04089376 -0.47524592 -1.04600716\n   1.1715812  -0.2484816  -0.01347433  0.16842364]\n [ 0.00758962  0.22552486 -0.04089376 -1.12774273 -0.11050405  0.67975844\n   0.89070664 -0.16419119  0.76955074 -1.12101653]\n [ 0.43535083  0.14405941 -0.47524592 -0.11050405 -0.34284795  0.38517199\n   2.23177433  0.10268787 -1.56549187 -1.4975995 ]\n [-1.10245202 -0.97513954 -1.04600716  0.67975844  0.38517199 -0.23096058\n   1.06186893  0.88399021 -0.0521502   0.82406591]\n [-0.23161883  0.82696754  1.1715812   0.89070664  2.23177433  1.06186893\n  -1.04669082  0.84140141 -2.19527708  1.56669622]\n [ 1.40936996  0.08329181 -0.2484816  -0.16419119  0.10268787  0.88399021\n   0.84140141 -2.13636108 -0.67823179  1.45259087]\n [-1.3653408  -0.50516593 -0.01347433  0.76955074 -1.56549187 -0.0521502\n  -2.19527708 -0.67823179 -1.26671335 -0.35833241]\n [ 0.27180199  0.02240545  0.16842364 -1.12101653 -1.4975995   0.82406591\n   1.56669622  1.45259087 -0.35833241 -0.04752548]]\nend of sweep 0: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 1: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 2: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 3: non_diag_zeros=32 non_diag_nonzeros=58\nend of sweep 4: non_diag_zeros=86 non_diag_nonzeros=4\nend of sweep 5: non_diag_zeros=90 non_diag_nonzeros=0\nour estimate of the eigenvalues is\n[-5. -3.  4.  2. -1.  1.  2. -5. -3. -3.]\nNumpy's estimate of the eigenvalues is\n[ 4. -1.  1.  2.  2. -3. -5. -5. -3. -3.]\n\n\nWe see that it takes 10 sweeps to find all the eigenvalues!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#computing-eigenvectors",
    "href": "src/lec08.html#computing-eigenvectors",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "9.3 Computing eigenvectors",
    "text": "9.3 Computing eigenvectors\nAfter enough sweeps, we end up with a diagonal matrix (at least to machine precision), let’s call it \\(D\\). But indeed, we have computed a factorisation: \\[\\begin{equation*}\nD = V^T A V,\n\\end{equation*}\\] where \\(V = P_1 P_2 \\cdots\\) and each \\(P_j\\) is a Jacobi rotation matrix. We can immediately identify the matrix \\(V\\) as having columns which correspond to the eigenvectors of \\(A\\).\nThe eigenvector matrix can be computed by successively computing: \\[\\begin{equation*}\nV' = V P_{pq},\n\\end{equation*}\\] starting from \\(V\\) is the identity matrix. We can see that componentwise, we have the update formulae: \\[\\begin{align*}\nv'*{rs} &= v*{rs} && s \\neq p, s \\neq q \\\\\nv'*{rp} &= c v*{rp} - s v_{rq} && ?? \\\\\nv'*{rq} &= s v*{rp} + c v_{rq} && ??.\n\\end{align*}\\]\n\nExample 9.2 Continuing Example 9.1. We computed that in the first iteration \\[\\begin{align*}\n\\cos\\theta & = 1 / \\sqrt{\\tan^2\\theta +1} = \\sqrt{\\sqrt{2}/4 + 1/2} \\\\\n\\sin\\theta & = \\tan\\theta \\cos\\theta\n=  \\sqrt{1/2 - \\sqrt{2}/4}.\n\\end{align*}\\]\nStarting from \\[\nV = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{pmatrix},\n\\] we can compute \\(V'\\) as \\[\nV' = \\begin{pmatrix}\nc & s & 0 \\\\\n-s & c & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sqrt{\\sqrt{2}/4 + 1/2} & \\sqrt{1/2 - \\sqrt{2}/4} & 0 \\\\\n-\\sqrt{1/2 - \\sqrt{2}/4} & \\sqrt{\\sqrt{2}/4 + 1/2} & 0 \\\\\n0 & 0 & 1 \\\\\n\\end{pmatrix}.\n\\]\n\nExercise 9.2 Continue this exercise with \\(P_{2, 3}\\) to show \\[\\begin{align*}\n& V'' = V' P_{2, 3}\n= \\begin{pmatrix}\n\\frac{\\sqrt{\\sqrt{2} + 2}}{2} & - \\frac{\\sqrt{2}}{4}\n& \\frac{1}{2} - \\frac{\\sqrt{2}}{4} \\\\ - \\frac{\\sqrt{2 - \\sqrt{2}}}{2} &\n-\\frac{1}{2} - \\frac{\\sqrt{2}}{4} & \\frac{\\sqrt{2}}{4} \\\\\n0 & - \\frac{\\sqrt{2 - \\sqrt{2}}}{2} & - \\frac{\\sqrt{\\sqrt{2} + 2}}{2}\n\\end{pmatrix} \\\\\n& \\approx\n\\begin{pmatrix}\n0.923879532511287 & -0.353553390593274 & 0.146446609406726\\\\\n-0.38268343236509 & -0.853553390593274 & 0.353553390593274\\\\\n0 & -0.38268343236509 & -0.923879532511287\\end{pmatrix}.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#python-code-again",
    "href": "src/lec08.html#python-code-again",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "9.4 Python code (again)",
    "text": "9.4 Python code (again)\nWe can try our methods on\n\ndef jacobi_rotation_with_V(A, V, p, q):\n    \"\"\"\n    Computes one iteration of P_{pq}^T A P_{pq} and V P_{pq}\n    Inputs: symmetric matrix A, matrix V, indices p, q\n    Output: symmetric matrix A' with A'_{p, q} = 0 (and A'_{q, p} = 0),\n            updated corresponding eigenvector matrix V\n    \"\"\"\n\n    # copy matrix to new holder\n    A_new = A.copy()\n    V_new = V.copy()\n\n    # solve quadratic equation\n    b = (A[q, q] - A[p, p]) / A[p, q]\n    roots = (-b + np.sqrt(b * b + 4)) / 2, (-b - np.sqrt(b * b + 4)) / 2\n    t = min(roots, key=abs)  # take smallest in absolute value root\n\n    # determine c, s\n    c = 1.0 / np.sqrt(t * t + 1.0)\n    s = c * t\n\n    # do updates\n    A_new[:, p] = c * A[:, p] - s * A[:, q]\n    A_new[:, q] = c * A[:, q] + s * A[:, p]\n    A_new[p, :] = c * A[p, :] - s * A[q, :]\n    A_new[q, :] = c * A[q, :] + s * A[p, :]\n\n    A_new[p, p] = c**2 * A[p, p] + s**2 * A[q, q] - 2 * c * s * A[p, q]\n    A_new[q, q] = s**2 * A[p, p] + c**2 * A[q, q] + 2 * c * s * A[p, q]\n\n    A_new[p, q] = 0.0  # replace update formula with exact value\n    A_new[q, p] = 0.0  # replace update formula with exact value\n\n    V_new[:, p] = c * V[:, p] - s * V[:, q]\n    V_new[:, q] = s * V[:, p] + c * V[:, q]\n\n    return A_new, V_new\n\nLet’s try it out!\n\nA = np.array(\n    [\n        [1, 2, 3 * np.sqrt(2 - np.sqrt(2)) / 4],\n        [2, 5, 3 * np.sqrt(2 + np.sqrt(2)) / 4],\n        [\n            3 * np.sqrt(2 - np.sqrt(2)) / 4,\n            3 * np.sqrt(2 + np.sqrt(2)) / 4,\n            2 * np.sqrt(2),\n        ],\n    ]\n)\nnp_eval, np_evec = np.linalg.eig(A)\n\nV = np.eye(A.shape[0])\n\nprint(\"initial matrices\")\nprint(A)\nprint(V)\n\nA, V = jacobi_rotation_with_V(A, V, 0, 1)\n\nprint(\"after one rotation (0, 1)\")\nprint(A)\nprint(V)\n\nA, V = jacobi_rotation_with_V(A, V, 1, 2)\n\nprint(\"after second rotation (1, 2)\")\nprint(A)\nprint(V)\n\nprint(\"Our estimate of the eigenvalues is\")\nprint(np.diag(A))\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_eval)\n\nprint(\"Our estimate of eigenvectors is\")\nprint(V)\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_evec)\n\ninitial matrices\n[[1.         2.         0.57402515]\n [2.         5.         1.3858193 ]\n [0.57402515 1.3858193  2.82842712]]\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nafter one rotation (0, 1)\n[[ 1.71572875e-01  0.00000000e+00 -1.11022302e-16]\n [ 0.00000000e+00  5.82842712e+00  1.50000000e+00]\n [-1.11022302e-16  1.50000000e+00  2.82842712e+00]]\n[[ 0.92387953  0.38268343  0.        ]\n [-0.38268343  0.92387953  0.        ]\n [ 0.          0.          1.        ]]\nafter second rotation (1, 2)\n[[ 1.71572875e-01 -4.24863958e-17 -1.02571233e-16]\n [-4.24863958e-17  6.44974747e+00  0.00000000e+00]\n [-1.02571233e-16  0.00000000e+00  2.20710678e+00]]\n[[ 0.92387953  0.35355339 -0.14644661]\n [-0.38268343  0.85355339 -0.35355339]\n [ 0.          0.38268343  0.92387953]]\nOur estimate of the eigenvalues is\n[0.17157288 6.44974747 2.20710678]\nNumpy's estimate of the eigenvalues is\n[6.44974747 0.17157288 2.20710678]\nOur estimate of eigenvectors is\n[[ 0.92387953  0.35355339 -0.14644661]\n [-0.38268343  0.85355339 -0.35355339]\n [ 0.          0.38268343  0.92387953]]\nNumpy's estimate of the eigenvalues is\n[[ 3.53553391e-01  9.23879533e-01 -1.46446609e-01]\n [ 8.53553391e-01 -3.82683432e-01 -3.53553391e-01]\n [ 3.82683432e-01 -1.29480716e-16  9.23879533e-01]]\n\n\nSuccess! We have the same values (up to reordering).\nWe can also apply the same code to the larger problem size:\n\nn = 10\ntol = 1.0e-12\n\n# generate a random matrix\nnp.random.seed(42)\nS = special_ortho_group.rvs(n)\nD = np.diag(np.random.randint(-5, 5, (n,)))\nA = S.T @ D @ S\nV = np.eye(n)\n\nprint(\"initial matrix A\")\nprint(A)\n\nnp_eval, np_evec = np.linalg.eig(A)\n\n# sweep over matrix several times\nfor sweep in range(10):\n    for i in range(n):\n        for j in range(i + 1, n):\n            if abs(A[i, j]) &lt; tol:\n                continue\n\n            A, V = jacobi_rotation_with_V(A, V, i, j)\n\n    non_diag_nonzeros = 0\n    non_diag_zeros = 0\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            if abs(A[i, j]) &lt; tol:\n                non_diag_zeros += 1\n            else:\n                non_diag_nonzeros += 1\n\n    print(f\"end of sweep {sweep}: {non_diag_zeros=} {non_diag_nonzeros=}\")\n    if non_diag_nonzeros == 0:\n        break\n\nprint(\"our estimate of the eigenvalues is\")\nprint(np.diag(A))\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_ev)\n\nprint(\"Our estimate of eigenvectors is\")\nprint(V)\nprint(\"Numpy's estimate of the eigenvalues is\")\nprint(np_evec)\n\ninitial matrix A\n[[-1.37563753 -0.31873229 -2.41518457  0.00758962  0.43535083 -1.10245202\n  -0.23161883  1.40936996 -1.3653408   0.27180199]\n [-0.31873229 -2.28404485  0.78760996  0.22552486  0.14405941 -0.97513954\n   0.82696754  0.08329181 -0.50516593  0.02240545]\n [-2.41518457  0.78760996 -1.14147565 -0.04089376 -0.47524592 -1.04600716\n   1.1715812  -0.2484816  -0.01347433  0.16842364]\n [ 0.00758962  0.22552486 -0.04089376 -1.12774273 -0.11050405  0.67975844\n   0.89070664 -0.16419119  0.76955074 -1.12101653]\n [ 0.43535083  0.14405941 -0.47524592 -0.11050405 -0.34284795  0.38517199\n   2.23177433  0.10268787 -1.56549187 -1.4975995 ]\n [-1.10245202 -0.97513954 -1.04600716  0.67975844  0.38517199 -0.23096058\n   1.06186893  0.88399021 -0.0521502   0.82406591]\n [-0.23161883  0.82696754  1.1715812   0.89070664  2.23177433  1.06186893\n  -1.04669082  0.84140141 -2.19527708  1.56669622]\n [ 1.40936996  0.08329181 -0.2484816  -0.16419119  0.10268787  0.88399021\n   0.84140141 -2.13636108 -0.67823179  1.45259087]\n [-1.3653408  -0.50516593 -0.01347433  0.76955074 -1.56549187 -0.0521502\n  -2.19527708 -0.67823179 -1.26671335 -0.35833241]\n [ 0.27180199  0.02240545  0.16842364 -1.12101653 -1.4975995   0.82406591\n   1.56669622  1.45259087 -0.35833241 -0.04752548]]\nend of sweep 0: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 1: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 2: non_diag_zeros=2 non_diag_nonzeros=88\nend of sweep 3: non_diag_zeros=32 non_diag_nonzeros=58\nend of sweep 4: non_diag_zeros=86 non_diag_nonzeros=4\nend of sweep 5: non_diag_zeros=90 non_diag_nonzeros=0\nour estimate of the eigenvalues is\n[-5. -3.  4.  2. -1.  1.  2. -5. -3. -3.]\nNumpy's estimate of the eigenvalues is\n[ 4. -1.  1.  2.  2. -3. -5. -5. -3. -3.]\nOur estimate of eigenvectors is\n[[ 0.51065887 -0.13311275 -0.1812081   0.5718129  -0.29621508 -0.20941776\n  -0.23530216 -0.41625872  0.02955701 -0.02430908]\n [ 0.1497469   0.87119484 -0.07721252 -0.14817269 -0.28909286 -0.26702604\n   0.1782095  -0.03048643 -0.0500966   0.02456002]\n [ 0.50218548 -0.30099823  0.03452595 -0.58755958 -0.0111315  -0.25729642\n   0.27574933 -0.09168642  0.28718522 -0.27847208]\n [ 0.07251175 -0.19247681  0.01407957 -0.02314526 -0.78840645  0.40107491\n   0.21255971  0.34191734 -0.10342981  0.04157102]\n [ 0.2443881   0.00724485 -0.42805002  0.25000527  0.29490886  0.09601399\n   0.47834543  0.29443737  0.30067141  0.43862465]\n [ 0.33692503  0.23683116 -0.24302346 -0.14936503  0.21841216  0.74304572\n  -0.20930051 -0.16094255 -0.06055934 -0.27536297]\n [-0.39183136 -0.14634552 -0.57554344 -0.27072551 -0.16659865  0.01740098\n   0.17339726 -0.56209203 -0.11008122  0.18525331]\n [-0.30014371  0.08493063 -0.28336596  0.04075787 -0.1878171  -0.02018258\n  -0.30314006  0.17061334  0.75561098 -0.30513883]\n [ 0.03562626  0.07984189  0.4809706  -0.12346684 -0.09029208  0.25648376\n  -0.09136763 -0.37477023  0.45243229  0.56422242]\n [ 0.2030366  -0.06713312 -0.27321259 -0.36097477 -0.03339419 -0.17305804\n  -0.62347841  0.31996943 -0.14701055  0.45336437]]\nNumpy's estimate of the eigenvalues is\n[[ 0.1812081  -0.29621508  0.20941776 -0.61833413 -0.26211936  0.1385047\n  -0.65882001  0.01543299  0.01838825  0.00742843]\n [ 0.07721252 -0.28909286  0.26702604  0.20484093 -0.01135791 -0.85228066\n  -0.1353326   0.07413412  0.03067527 -0.15588922]\n [-0.03452595 -0.0111315   0.25729642  0.64828794  0.24636739  0.39944087\n  -0.44717942  0.25663368  0.27479557 -0.16424828]\n [-0.01407957 -0.78840645 -0.40107491  0.10229181 -0.12669188  0.15561557\n   0.15982712  0.30700944 -0.12353739  0.0616354 ]\n [ 0.42805002  0.29490886 -0.09601399 -0.0491652  -0.50765179 -0.0197827\n  -0.0033952   0.38260671  0.27134089  0.47083702]\n [ 0.24302346  0.21841216 -0.74304572  0.05847969  0.25157218 -0.1922056\n  -0.36284189  0.0966046  -0.02025481 -0.3226918 ]\n [ 0.57554344 -0.16659865 -0.01740098  0.31634205  0.08217816  0.08464294\n  -0.05143066 -0.68186036 -0.13499389  0.1904727 ]\n [ 0.28336596 -0.1878171   0.02018259 -0.15304895  0.17497158  0.13317883\n   0.34044251 -0.06535305  0.77821578 -0.19134954]\n [-0.4809706  -0.09029208 -0.25648377  0.07940841  0.15274206 -0.07921144\n  -0.26440339 -0.26171187  0.42091564  0.60223992]\n [ 0.27321258 -0.03339419  0.17305804  0.09655655  0.68754591 -0.04642289\n   0.0447886   0.37514302 -0.18137684  0.42832468]]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#computational-cost",
    "href": "src/lec08.html#computational-cost",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "9.5 Computational cost",
    "text": "9.5 Computational cost\nA quick look at the cost of each rotation shows that the main cost is in updating all the entries. For a matrix of size \\(n\\), we must update \\(4n-4\\) entries (two rows and two columns) in \\(A\\) and \\(2n\\) entries in (two rows) in \\(V\\). We say that each rotation costs \\(O(n)\\).\nOne sweep corresponds to trying to eliminate all off diagonal entries (i.e. the strictly upper triangular portion of the matrix). So each sweep corresponds to \\(n (n-1)/2\\) rotations or \\((6n - 4) n (n-1) / 2\\). So each sweep is \\(O(n^3)\\).\nThe remaining question is how many sweeps are required? Let’s test this using our code.\n\n\n\n\n\n\n\n\n\nThis gives us an estimate that the number of sweeps is \\(O(n^{1/2})\\). So we would expect the time to run the algorithm to scale like \\(n^{7/2}\\). However, in practical situations, we see that the growth is closer to \\(n^{5/2}\\):",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#references",
    "href": "src/lec08.html#references",
    "title": "9  Eigenvectors and eigenvalues: practical solutions",
    "section": "9.6 References",
    "text": "9.6 References\nNumerical recipes, chapter XX?\nhttps://arxiv.org/abs/2509.00495",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  }
]