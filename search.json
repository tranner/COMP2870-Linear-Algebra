[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#contents-of-this-submodule",
    "href": "index.html#contents-of-this-submodule",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.1 Contents of this submodule",
    "text": "1.1 Contents of this submodule\nThis part of the module will deal with numerical algorithms that involve matrices. The study of this type of problem is called linear algebra. We will approach these problems using a combination of theoretical ideas and practical solutions, thinking through the lens of real-world applications. As a consequence, to succeed in linear algebra, you will do some programming (using Python) and some pen-and-paper theoretical work, too.\n\n1.1.1 Topics\nWe will have 7 double lectures, 3 tutorials, and 3 labs. We break up the topics as follows:\nLectures\n\nIntroduction and motivation, key problem statements (Week 4, Mon)\nWhen can we solve systems of linear equations? (Week 4, Wed)\nDirect methods for systems of linear equations (Week 5, Mon)\nIterative solution of linear equations (Week 5, Wed)\nComplex numbers (Week 6, Mon)\nEigenvalues and eigenvectors (Week 6, Wed)\nPractical solutions for eigenvalues and eigenvectors / Summary (Week 7, Mon)\n\nLabs\n\nFloating point numbers (Week 4)\nWhen can we solve systems of linear equations? (Week 5)\nSystems of linear equations (Week 6)\nEigenvalues and eigenvectors (Week 7)\n\nTutorials\n\nLinear independence, span, basis (Week 5)\nSolution of linear systems (Week 6)\nEigenvalue and eigenvectors (Week 7)\n\nLater this term, week 11, you will complete a project based on the things you’ve learnt from this section of the module.\n\n\n1.1.2 Learning outcomes\nCandidates should be able to:\n\nexplain practical challenges working with floating-point numbers;\ndefine and identify what it means for a set of vectors to be a basis, spanning set or linearly independent;\napply direct and iterative solvers to solve systems of linear equations; implement methods using floating point numbers and investigate computational cost using computer experiments;\napply algorithms to compute eigenvectors and eigenvalues of large matrices.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#textbooks-and-other-resources",
    "href": "index.html#textbooks-and-other-resources",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.2 Textbooks and other resources",
    "text": "1.2 Textbooks and other resources\nThere are many textbooks and other external resources which could help your learning:\n\nIntroduction to Linear Algebra (Fifth Edition), Gilbert Strang, Wellesley-Cambridge Press, 2016.  with MIT course material. Strongly recommended book and YouTube lecture series\nScientific Computing: An Introductory Survey, T.M. Heath, McGraw-Hill, 2002.  Some lecture notes based on the book\nEngineering Mathematics, K.A. Stroud, Macmillan, 2001. available online\nNumerical Recipes in C++/C/FORTRAN: The Art of Scientific Computing, W.H. Press, S.A. Teukolsky, W.T. Vetterling and B.P. Flannery, Cambridge University Press, 2002/1993/1993. A very practical view of the methods we develop in this module which could be used for library development\n\nOther links and resources are given at the end of each section of the notes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.3 Programming",
    "text": "1.3 Programming\nThis section of the notes links theoretical material with practical applications. We will have weekly lab sessions where you will see how the methods mentioned in the lecture notes work when implemented. More instructions on how we will do this will be covered in the lab sessions themselves.\nAn important theoretical aspect of translating the algorithms in linear algebra to computer implementations is the use of floating-point numbers. You will have already met floating-point numbers in your first year studies where you met how computer store numbers. You will already know that computer cannot store every possible real number exactly. The inexactness of floating-point numbers has real consequences when performing linear algebra computations.\nExploring and understanding the material in (Chapter 2) is really helpful for understanding many of the choices that we make when forming methods in linear algebra.\nWe will make extensive use of Python and numpy during this section of the module. It may help you to revise some of your notes from last year on these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#the-big-problems-in-linear-algebra",
    "href": "index.html#the-big-problems-in-linear-algebra",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.4 The big problems in linear algebra",
    "text": "1.4 The big problems in linear algebra\nWe will cover two big linear algebra problems in this section of the module. Linear algebra can be defined as the study of problems involving matrices and vectors.\n\n1.4.1 Reminder of matrices and vectors\nThere are two important objects we will work with that were defined in your first-year Theoretical Foundations module (COMP1870).\n\nDefinition 1.1 A matrix is a rectangular array of numbers called entries or elements of the matrix. A matrix with \\(m\\) rows and \\(n\\) columns is called an \\(m \\times n\\) matrix or \\(m\\)-by-\\(n\\) matrix. We may additionally say that the matrix is of order \\(m \\times n\\). If \\(m = n\\), then we say that the matrix is square.\n\n\nExample 1.1 \\(A\\) is a \\(4 \\times 4\\) matrix and \\(B\\) is a \\(3 \\times 4\\) matrix: \\[\\begin{align*}\nA = \\begin{pmatrix} 10 & 1 & 0 & 9 \\\\ 12.4 & 6 & 1 & 0 \\\\ 1 &\n3.14 & 1 & 0 \\end{pmatrix}\n\\quad\nB = \\begin{pmatrix} 0 & 6 & 3 & 1 \\\\ 1 & 4 & 1\n& 0 \\\\ 7 & 0 & 10 & 20 \\end{pmatrix} \\quad C = \\begin{pmatrix} 4 & 1 & 8 & -1 \\\\\n1.5 & 1 & 3 & 4 \\\\ 6 & -4 & 2 & 8 \\end{pmatrix}\n\\end{align*}\\]\n\nExercise 1.1  \n\nCompute, if defined, \\(A + B\\), \\(B + C\\).\nCompute, if defined, \\(A B\\), \\(B A\\), \\(B C\\) (here, by writing matrices next to each other we mean the matrix product).\n\n\n\nWhen considering systems of linear equations, the entries of the matrix will always be real numbers (later we will explore using complex numbers (Chapter 6) too)\n\nDefinition 1.2 A column vector, often just called a vector, is a matrix with a single column. A matrix with a single row is a row vector. The entries of a vector are called components. A vector with \\(n\\) rows is called an \\(n\\)-vector.\n\n\nExample 1.2 \\(\\vec{a}\\) is a row vector, \\(\\vec{b}\\) and \\(\\vec{c}\\) are (column) vectors. \\[\\begin{align*}\n\\vec{a} =\n\\begin{pmatrix} 0 & 1 & 7 \\end{pmatrix}\n\\quad\n\\vec{b} =\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 3.1 \\\\ 7 \\end{pmatrix}\n\\quad\n\\vec{c} = \\begin{pmatrix} 4 \\\\ 6 \\\\ -4 \\\\ 0 \\end{pmatrix}.\n\\end{align*}\\]\n\nExercise 1.2  \n\nCompute, if defined, \\(\\vec{b} + \\vec{c}\\), \\(0.25 \\vec{c}\\).\nWhat is the meaning of \\(\\vec{b}^T \\vec{c}\\)? (Here, we are interpreting the vectors as matrices).\nCompute, if defined, \\(B \\vec{b}\\).\n\n\n\n\nExample 1.3 (Some special matrices) It will be helpful to remember some special matrices during this seciton of the module.\nA rotation matrix is a \\(2 \\times 2\\) matrix given by \\[\\begin{equation}\nR(\\theta) = \\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{pmatrix}.\n\\end{equation}\\] This matrix describes an anti-clockwise rotation by \\(\\theta\\) (in radians).\nThe projection operator maps vectors onto either lines or planes which pass through the origin (\\(\\vec{0}\\)).\n\nFor projection onto a line \\(L = \\{ \\lambda \\vec{a} : \\lambda \\in \\mathbb{R} \\}\\), we can write this transformation as: \\[\\begin{equation*}\n\\vec{x} \\mapsto \\vec{a} \\cdot \\vec{x} \\vec{a}\n= (\\vec{a} \\otimes \\vec{a}) \\vec{x},\n\\end{equation*}\\] where \\(\\otimes\\) is the outer product. For two \\(n\\)-vectors \\(\\vec{a}\\) and \\(\\vec{b}\\), \\(\\vec{a} \\otimes \\vec{b}\\) is an \\(n \\times n\\) with entries: \\[\\begin{equation*}\n(\\vec{a} \\otimes \\vec{b})_{ij} = a_i b_j.\n\\end{equation*}\\]\nFor projection onto a plane \\(\\Pi = \\{ \\lambda \\vec{a} + \\mu \\vec{b} : \\lambda,\n\\mu \\in \\mathbb{R} \\}\\), we can rite the projection operator as: \\[\\begin{equation*}\n\\vec{x} \\mapsto (\\vec{a} \\otimes \\vec{a} + \\vec{b} \\otimes \\vec{b}) \\vec{x}.\n\\end{equation*}\\]\n\n\n\n\n1.4.2 Systems of linear equations\nGiven an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies: \\[\\begin{equation}\n\\label{eq:sle}\nA \\vec{x} = \\vec{b}.\n\\end{equation}\\]\nWe can also write \\(\\eqref{eq:sle}\\) as a system of linear equations: \\[\\begin{align*}\n\\text{Equation 1:} &&\na_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} &&\na_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} &&\na_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} &&\na_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nNotes:\n\nThe values \\(a_{ij}\\) are known as coefficients.\nThe right hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.\n\nMany computational algorithms require the solution of linear equations, e.g. in fields such as\n\nScientific computation;\nNetwork design and optimisation;\nGraphics and visualisation;\nMachine learning.\n\nTypically, these systems are very large (\\(n \\approx 10^9\\)).\nIt is therefore important that this problem can be solved\n\naccurately: we are allowed to make small errors but not big errors;\nefficiently: we need to find the answer quickly;\nreliably: we need to know that our algorithm will give us an answer that we are happy with.\n\n\nExample 1.4 (Temperature in a sealed room) Suppose we wish to estimate the temperature distribution inside an object:\n\n\n\n\n\nImage showing temperature sample points and relations in a room.\n\n\n\n\nWe can place a network of points inside the object and use the following model: the temperature at each interior point is the average of its neighbours.\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & -1/6 & -1/6 & 0 & -1/6 & 0 & 0 \\\\\n-1/6 & 1 & -1/6 & -1/6 & 0 & -1/6 & 0 \\\\\n-1/4 & -1/4 & 1 & 0 & -1/4 & -1/4 & 0 \\\\\n0 & -1/5 & 0 & 1 & 0 & -1/5 & 0 \\\\\n-1/6 & 0 & -1/6 & 0 & 1 & -1/6 & -1/6 \\\\\n0 & -1/8 & -1/8 & -1/8 & -1/8 & 1 & -1/8 \\\\\n0 & 0 & 0 & 0 & -1/5 & -1/5 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7\n\\end{pmatrix}\n= \\begin{pmatrix}\n400/6 \\\\ 100/6 \\\\ 0 \\\\ 0 \\\\ 100/6 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n\\]\n\n\nExample 1.5 (Traffic network) Suppose we wish to monitor the flow of traffic in a city centre:\n\n\n\n\n\nExample network showing traffic flow in a city\n\n\n\n\nAs the above example shows, it is not necessary to monitor every single road. If we know all of the \\(y\\) values, we can calculate the \\(x\\) values!\nThis example leads to the system:\n\\[\n\\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_5 \\\\ y_{12} - y_6 \\\\ y_8 - y_7 \\\\ y_{11} - y_4 \\\\\ny_{11} + y_{12} - y_{10} \\\\ y_9 \\\\ y_2 - y_3 \\\\ y_1\n\\end{pmatrix}.\n\\]\n\n\n\n1.4.3 Eigenvalues and eigenvectors\nFor this problem, we will think of a matrix \\(A\\) acting on functions \\(\\vec{x}\\): \\[\\begin{equation*}\n\\vec{x} \\mapsto A \\vec{x}.\n\\end{equation*}\\] We are interested in when is the output vector \\(A \\vec{x}\\) is parallel to \\(\\vec{x}\\)?\n\nDefinition 1.3 We say that any vector \\(\\vec{x}\\), where \\(A \\vec{x}\\) is parallel is \\(\\vec{x}\\), is called an eigenvector of \\(A\\). Here by parallel, we mean that there exists a number \\(\\lambda\\) (can be positive, negative or zero) such that \\[\\begin{equation}\n\\label{eq:evalues}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We call the associated number \\(\\lambda\\) an eigenvalue of \\(A\\).\nWe will later see that an \\(n \\times n\\) square matrix always has \\(n\\) eigenvalues (which may not always be distinct).\n\nTo help with our intuition here, we start with some simple examples:\n\nExample 1.6 Let \\(A\\) be the \\(2 \\times 2\\) matrix that scales any input vector by \\(a\\) in the \\(x\\)-direction and by \\(b\\) in the \\(y\\)-direction. We can write this matrix as \\[\\begin{equation*}\nA = \\begin{pmatrix}\na & 0 \\\\ 0 & b\n\\end{pmatrix},\n\\end{equation*}\\] since then for a 2-vector \\(\\vec{x} = (x, y)^T\\), we have \\[\\begin{equation*}\nA \\vec{x}\n= \\begin{pmatrix}\na & 0 \\\\ 0 & b\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\end{pmatrix}\n= \\begin{pmatrix} a x + 0 y \\\\ 0 x + b y \\end{pmatrix}\n= \\begin{pmatrix} a x \\\\ b y \\end{pmatrix}.\n\\end{equation*}\\]\nThen, we infer we have two eigenvalues and two eigenvectors: One eigenvalue is \\(a\\) with eigenvector \\((1, 0)^T\\) and the other is \\(b\\) with eigenvector \\((0, 1)^T\\) since: \\[\\begin{align*}\n\\begin{pmatrix}\na & 0 \\\\ 0 & b\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n& = \\begin{pmatrix} a \\\\ 0 \\end{pmatrix}\n= a \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\\\\n\\begin{pmatrix}\na & 0 \\\\ 0 & b\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n& = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}\n= b \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n\\end{align*}\\]\n\n\nExample 1.7 Let \\(P\\) be the 3x3 matrix that represents projection onto a plane \\(\\pi\\). What are the eigenvalues and eigenvectors of \\(p\\)?\n\n\n\n\n\n\n\n\n\n\nIf \\(\\vec{x}\\) is in the plane \\(\\Pi\\), then \\(P \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the associated eigenvalue is \\(1\\).\nIf \\(\\vec{y}\\) is perpendicular to the plane \\(\\Pi\\), then \\(P \\vec{y} = \\vec{0}\\). This means that \\(\\vec{y}\\) is an eigenvector and the associated eigenvalue is \\(0\\).\n\nLet \\(\\vec{y}\\) be perpendicular to \\(\\Pi\\) (so that \\(P \\vec{y} = \\vec{0}\\) and \\(\\vec{y}\\) is an eigenvector of \\(P\\)), then for any number \\(s\\), we can compute \\[\\begin{equation*}\nP (s \\vec{y}) = s P \\vec{y} = s \\vec{0} = \\vec{0}.\n\\end{equation*}\\] This means that \\(s \\vec{y}\\) is also an eigenvector of \\(P\\) associated to the eigenvalue \\(0\\). As a consequence, when we compute eigenvectors, we need to take care to normalise the vector to ensure we get a unique answer.\nWe see we end up with a two-dimensional space of eigenvectors (i.e., the plane \\(\\Pi\\)) associated to eigenvalue \\(1\\) and a one-dimensional space of eigenvectors (i.e., the line perpendicular to \\(\\Pi\\)) eigenvalue \\(0\\). We use the term eigenspace the space of eigenvectors associated to a particular eigenvalue.\n\n\nExample 1.8 Let \\(A\\) be the permuatation matrix which takes an input two-vector and outputs a two-vector with the components swapped. The matrix is given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n0 & 1 \\\\ 1 & 0 \\\\\n\\end{pmatrix}.\n\\end{equation*}\\] What are the eigenvectors and eigenvalues of \\(A\\)?\n\nLet \\(\\vec{x} = (1, 1)^T\\), then swapping the components of \\(\\vec{x}\\) gives back the same vector \\(\\vec{x}\\). In equations, we can write \\(A \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the eigenvalue is \\(1\\).\nLet \\(\\vec{x} = (-1, 1)^T\\), then swapping the components of \\(\\vec{x}\\) gives back \\((1, -1)^T\\), which we can see is \\(-\\vec{x}\\). In equations, we can write \\(A\n\\vec{x} = -\\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector of \\(A\\) and the associated eigenvalue is \\(-1\\).\n\nHere we see that again we actually have two one-dimensional eigenspaces.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#comments-on-these-notes",
    "href": "index.html#comments-on-these-notes",
    "title": "COMP2870 Theoretical Foundations: Linear Algebra",
    "section": "1.5 Comments on these notes",
    "text": "1.5 Comments on these notes\nThere are two versions of these notes available: as an online website and as a pdf. I expect minor adjustments to be made to both throughout the progress of delivering the materials.\nYou can always find the latest versions at:\n\nhttps://comp2870-2526.github.io/linear-algebra-notes/ (html version)\nhttps://comp2870-2526.github.io/linear-algebra-notes/COMP2870-Theoretical-Foundations--Linear-Algebra.pdf (pdf version)\n\nPlease either email me (T.Ranner@leeds.ac.uk) or use the github repository to report any corrections.\n\n\nThis version of the notes is 45f9a1d+dirty.\n\n\nCopyright 2024-2025, Thomas Ranner and University of Leeds, licensed under CC BY 4.0.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/lec02.html",
    "href": "src/lec02.html",
    "title": "2  Floating point number systems",
    "section": "",
    "text": "2.1 Finite precision number systems\nThis topic will not be presented explicitly in lectures; instead, you will study the material yourself in Lab Session 1.\nComputers store numbers with finite precision, i.e. using a finite set of bits (binary digits), typically 32 or 64 of them. You learned how to store numbers as floating-point numbers last year in the module COMP1860 Building our Digital World.\nYou will recall that many numbers cannot be stored exactly.\nThe inaccuracies inherent in finite precision arithmetic must be modelled in order to understand:\nThe examples shown here will be in decimal by the issues apply to any base, e.g. binary.\nThis is important when trying to solve problems with floating-point numbers, so that we learn how to avoid the key pitfalls.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#finite-precision-number-systems",
    "href": "src/lec02.html#finite-precision-number-systems",
    "title": "2  Floating point number systems",
    "section": "",
    "text": "Some numbers cannot be represented precisely using any finite set of digits: e.g. \\(\\sqrt{2} = 1.141 42\\ldots\\), \\(\\pi = 3.141 59\\ldots\\), etc.\nSome cannot be represented precisely in a given number base:\ne.g. \\(\\frac{1}{9} = 0.111\\ldots\\) (decimal), \\(\\frac{1}{5} = 0.0011 0011\n  \\ldots\\) (binary).\nOthers can be represented by a finite number of digits but only using more than are available: e.g. \\(1.526 374 856 437\\) cannot be stored exactly using 10 decimal digits.\n\n\n\nhow the numbers are represented (and the nature of associated limitations);\nthe errors in their representation;\nthe errors which occur when arithmetic operations are applied to them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#normalised-systems",
    "href": "src/lec02.html#normalised-systems",
    "title": "2  Floating point number systems",
    "section": "2.2 Normalised systems",
    "text": "2.2 Normalised systems\nTo understand how this works in practice, we introduce an abstract way to think about the practicalities of floating-point numbers, but our examples will have fewer digits.\nAny finite precision number can be written using the floating point representation:\n\\[\nx = \\pm 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e.\n\\]\n\nThe digits \\(b_i\\) are integers satisfying \\(0 \\le b_i \\le \\beta - 1\\).\nThe mantissa, \\(b_1 b_2 b_3 \\ldots b_{t-1} b_t\\), contains \\(t\\) digits.\n\\(\\beta\\) is the base (always a positive integer).\n\\(e\\) is the integer exponent and is bounded (\\(L \\le e \\le U\\)).\n\n\\((\\beta, t, L, U)\\) fully defines a finite precision number system.\nNormalised finite precision systems will be considered here for which\n\\[ b_1 \\neq 0 \\quad (0 &lt; b_1 \\le \\beta -1). \\]\n\nExample 2.1  \n\nIn the case \\((\\beta, t, L, U) = (10, 4, -49, 50)\\) (base 10), \\[ 10 000 =\n.1000 \\times 10^5, \\quad 22.64 = .2264 \\times 10^2, \\quad 0.000 056 7 =\n.5670 \\times 10^{-4} \\]\nIn the case \\((\\beta, t, L, U) = (2, 6, -7, 8)\\) (binary), \\[ 1 0000 = .1000 00\n\\times 2^5, \\quad 1011.11 = .1011 11 \\times 2^4,\\] \\[ 0.0000 11 = .1100 00\n\\times 2^{-4}. \\]\nZero is always taken to be a special case e.g., \\(0 = \\pm .00\\ldots 0\n\\times \\beta^0\\).\n\nOur familiar floating point numbers can be represented using this format too:\n\nThe IEEE single-precision standard is \\((\\beta, t, L, U) = (2, 23, -127, 128)\\). This is available via numpy.single.\nThe IEEE double-precision standard is \\((\\beta, t, L, U) = (2, 52, -1023, 1024)\\). This is available via numpy.double.\n\n\nimport numpy as np\n\na = np.double(1.1)\nprint(type(a))\nb = np.single(1.2)\nprint(type(b))\nc = np.half(1.3)\nprint(type(c))\n\n&lt;class 'numpy.float64'&gt;\n&lt;class 'numpy.float32'&gt;\n&lt;class 'numpy.float16'&gt;\n\n\n\nYou can create double-precision floating point numbers without using numpy (i.e. 64 bit with the same \\(\\beta, t, L, U\\) as above) by calling the float function, or just giving the number in decimal format:\n\n(np.double(5.1) == 5.1, np.double(5.1) == 5.1)\n\n(np.True_, np.True_)\n\n\nHowever, in some extreme edge cases, you are unlikely to meet, Python floats do not follow the IEEE double-precision standard in some extreme edge cases.\n\nExample 2.2 Consider the number system given by \\((\\beta, t, L, U) =\n(10, 2, -1, 2)\\) which gives\n\\[ x = \\pm .b_1 b_2 \\times 10^e \\text{ where } -1 \\le e \\le 2. \\]\n\n\nHow many numbers can be represented by this normalised system?\n\n\n\nThe sign can be positive or negative\n\\(b_1\\) can take on the values \\(1\\) to \\(9\\) (9 options)\n\\(b_2\\) can take on the values \\(0\\) to \\(9\\) (10 options)\n\\(e\\) can take on the values \\(-1, 0, 1, 2\\) (4 options)\n\nOverall this gives us: \\[ 2 \\times 9 \\times 10 \\times 4 \\text{ options } = 720\n\\text{ options}. \\]\n\n\nWhat are the two largest positive numbers in this system?\n\n\nThe largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 9\\) and \\(e = 2\\) which gives \\[+ 0.99 \\times 10^{2} = 99. \\]\nThe second largest value uses \\(+\\) as a sign, \\(b_1 = 9\\), \\(b_2 = 8\\) and \\(e = 2\\) which gives \\[+ 0.98 \\times 10^{2} = 98. \\]\n\n\nWhat are the two smallest positive numbers?\n\n\nThe smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 =0\\) and \\(e=-1\\) which gives \\[+ 0.10 \\times 10^{-1} = 0.01. \\]\nThe second smallest positive number has \\(+\\) sign, \\(b_1 = 1\\), \\(b_2 = 1\\) and \\(e =\n-1\\) which gives \\[+ 0.11 \\times 10^{-1} = 0.011. \\]\n\n\nWhat is the smallest possible difference between two numbers in this system?\n\n\nThe smallest difference will be between numbers of the form \\(+0.10 \\ times\n10^{-1}\\) and \\(+0.11 \\times 10^{-1}\\) which gives \\[ 0.11 \\times 10^{-1} - 0.10\n\\times 10^{-1} = 0.011 - 0.010 = 0.001. \\]\nAlternatively, we can brute-force search for this:\n\n\nThe minimum difference min_diff=0.0010\nat x=+0.57 x 10^{-1} y=+0.58 x 10^{-1}.\n\n\n\n\nExercise 2.1 Consider the number system given by \\((\\beta, t, L, U) =\n(10, 3, -3, 3)\\) which gives\n\\[ x = \\pm .b_1 b_2 b_3 \\times 10^e \\text{ where } -3 \\le e \\le 3. \\]\n\nHow many numbers can be represented by this normalised system?\nWhat are the two largest positive numbers in this system?\nWhat are the two smallest positive numbers?\nWhat is the smallest possible difference between two numbers in this system?\nWhat is the smallest possible difference in this system, \\(x\\) and \\(y\\), for which \\(x &lt; 100 &lt; y\\)?\n\n\n\nExample 2.3 (What about in Python) We find that even with double-precision floating point numbers, we see some funniness when working with decimals:\n\na = np.double(0.0)\n\n# add 0.1 ten times to get 1?\nfor _ in range(10):\n    a = a + np.double(0.1)\n    print(a)\n\nprint(\"Is a = 1?\", a == 1.0)\n\n0.1\n0.2\n0.30000000000000004\n0.4\n0.5\n0.6\n0.7\n0.7999999999999999\n0.8999999999999999\n0.9999999999999999\nIs a = 1? False\n\n\n\nWhy is this output not a surprise?\n\nWe also see that even adding up numbers can have different results depending on what order we add them:\n\nx = np.double(1e30)\ny = np.double(-1e30)\nz = np.double(1.0)\n\nprint(f\"{(x + y) + z=:.16f}\")\n\n(x + y) + z=1.0000000000000000\n\n\n\nprint(f\"{x + (y + z)=:.16f}\")\n\nx + (y + z)=0.0000000000000000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#errors-and-machine-precision",
    "href": "src/lec02.html#errors-and-machine-precision",
    "title": "2  Floating point number systems",
    "section": "2.3 Errors and machine precision",
    "text": "2.3 Errors and machine precision\nFrom now on \\(fl(x)\\) will be used to represent the (approximate) stored value of \\(x\\). The error in this representation can be expressed in two ways.\n\\[ \\begin{aligned} \\text{Absolute error} &= | fl(x) - x | \\\\ \\text{Relative\nerror} &= \\frac{| fl(x) - x |}{|x|}. \\end{aligned} \\]\nThe number \\(fl(x)\\) is said to approximate \\(x\\) to \\(t\\) significant digits (or figures) if \\(t\\) is the largest non-negative integer for which\n\\[ \\text{Relative error} &lt; 0.5 \\times \\beta^{1-t}. \\]\nIt can be proved that if the relative error is equal to \\(\\beta^{-d}\\), then \\(fl(x)\\) has \\(d\\) correct significant digits.\nIn the number system given by \\((\\beta, t, L, U)\\), the nearest (larger) representable number to \\(x = 0.b_1 b_2 b_3 \\ldots b_{t-1} b_t \\times \\beta^e\\) is\n\\[ \\tilde{x} = x + .\\underbrace{000\\ldots01}_{t \\text{ digits}} \\times \\beta^e =\nx + \\beta^{e-t} \\]\nAny number \\(y \\in (x, \\tilde{x})\\) is stored as either \\(x\\) or \\(\\tilde{x}\\) by rounding to the nearest representable number, so\n\nthe largest possible error is \\(\\frac{1}{2} \\beta^{e-t}\\),\nwhich means that \\(| y - fl(y) | \\le \\frac{1}{2} \\beta^{e-t}\\).\n\nIt follow from \\(y &gt; x \\ge .100 \\ldots 00 \\times \\beta^e = \\beta^{e-1}\\) that\n\\[ \\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\frac{\\beta^{e-t}}{\\beta^{e-1}} =\n\\frac{1}{2} \\beta^{1-t}, \\]\nand this provides a bound on the relative error: for any \\(y\\)\n\\[ \\frac{|y - fl(y)|}{|y|} &lt; \\frac{1}{2} \\beta^{1-t}. \\]\nThe last term is known as machine precision or unit roundoff and is often called \\(eps\\). This is obtained in Python with\n\neps = np.finfo(np.double).eps\nprint(eps)\n\n2.220446049250313e-16\n\n\n\nExample 2.4  \n\nThe number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-2} = 0.05. \\]\nThe number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-3} = 0.005. \\]\nThe number system \\((\\beta, t, L, U) = (10, 7, 2, 10)\\) gives\n\\[ eps = \\frac{1}{2} \\beta^{1-t} = \\frac{1}{2} 10^{1-7} = 0.000005. \\]\nFor some common types in python, we see the following values:\n\n\nfor dtype in [np.half, np.single, np.double]:\n    print(dtype.__name__, np.finfo(dtype).eps)\n\nfloat16 0.000977\nfloat32 1.1920929e-07\nfloat64 2.220446049250313e-16\n\n\n\n\nMachine precision epsilon (\\(eps\\)) gives us an upper bound for the error in the representation of a floating-point number in a particular system. We note that this is different to the smallest possible numbers that we can store!\n\neps = np.finfo(np.double).eps\nprint(f\"{eps=}\")\n\nsmaller = eps\nfor _ in range(5):\n    smaller = smaller / 10.0\n    print(f\"{smaller=}\")\n\neps=np.float64(2.220446049250313e-16)\nsmaller=np.float64(2.2204460492503132e-17)\nsmaller=np.float64(2.220446049250313e-18)\nsmaller=np.float64(2.220446049250313e-19)\nsmaller=np.float64(2.2204460492503132e-20)\nsmaller=np.float64(2.2204460492503133e-21)\n\n\n\nArithmetic operations are usually carried out as though infinite precision is available, after which the result is rounded to the nearest representable number.\nThis approach means that arithmetic cannot be completely trusted and the usual rules do not necessarily apply!\n\nExample 2.5 Consider the number system \\((\\beta, t, L, U) = (10, 2,\n-1, 2)\\) and take\n\\[ x = .10 \\times 10^2, \\quad y = .49 \\times 10^0, \\quad z = .51 \\times 10^0. \\]\n\nIn exact arithmetic \\(x + y = 10 + 0.49 = 10.49\\) and \\(x + z = 10 + 0.51 =\n10.51\\).\nIn this number system rounding gives\n\\[ fl(x+y) = .10 \\times 10^2 = x, \\qquad fl(x+z) = .11 \\times 10^2 \\neq x.\n  \\]\n\n(Note that \\(\\frac{y}{x} &lt; eps\\) but \\(\\frac{z}{x} &gt; eps\\).)\nEvaluate the following expression in this number system.\n\\[ x+(y+y), \\quad (x+y)+y, \\quad x+(z+z), \\quad (x+z) +z. \\]\n(Also note the benefits of adding the smallest terms first!)\n\n\nExercise 2.2 (More examples)  \n\nVerify that a similar problem arises for the numbers\n\\[ x = .85 \\times 10^0, \\quad y = .3 \\times 10^{-2}, \\quad z = .6 \\times\n  10^{-2}, \\]\n\nin the system \\((\\beta, t, L, U) = (10, 2, -3, 3)\\).\n\nGiven the number system \\((\\beta, t, L, U) = (10, 3, -3, 3)\\) and \\(x =\n.100\\times 10^3\\), find nonzero numbers \\(y\\) and \\(z\\) from this system for which \\(fl(x+y) = x\\) and \\(fl(x+z) &gt; x\\).\n\n\nIt is sometimes helpful to think of another machine precision epsilon in other way: Machine precision epsilon is the smallest positive number \\(eps\\) such that \\(1 + eps &gt; 1\\), i.e. it is half the difference between \\(1\\) and the next largest representable number.\nExamples:\n\nFor the number system \\((\\beta, t, L, U) = (10, 2, -1, 2)\\),\n\\[ \\begin{array}{ccl} & .11 \\times 10^1 & \\qquad \\leftarrow {\\text{next\n  number}} \\\\\n   - & .10 \\times 10^1 & \\qquad \\leftarrow 1 \\\\ \\hline & .01 \\times 10^1 &\n     \\qquad \\leftarrow 0.1 \\end{array} \\]\n\nso \\(eps = \\frac{1}{2}(0.1) = 0.05\\).\n\nVerify that this approaches gives the previously calculated value for \\(eps\\) in the number system given by \\((\\beta, t, L, U) = (10, 3, -3, 3)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#other-features-of-finite-precision",
    "href": "src/lec02.html#other-features-of-finite-precision",
    "title": "2  Floating point number systems",
    "section": "2.4 Other “Features” of finite precision",
    "text": "2.4 Other “Features” of finite precision\nWhen working with floating-point numbers there are other things we need to worry about, too!\n\nOverflow\n\nThe number is too large to be represented, e.g. multiply the largest representable number by 10. This gives inf (infinity) with numpy.doubles and is usually “fatal”.\n\nUnderflow\n\nThe number is too small to be represented, e.g. divide the smallest representable number by 10. This gives \\(0\\) and may not be immediately obvious.\n\nDivide by zero\n\ngives a result of inf, but \\(\\frac{0}{0}\\) gives nan (not a number)\n\nDivide by inf\n\ngives \\(0.0\\) with no warning",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#is-this-all-academic",
    "href": "src/lec02.html#is-this-all-academic",
    "title": "2  Floating point number systems",
    "section": "2.5 Is this all academic?",
    "text": "2.5 Is this all academic?\nNo! There are many examples of major software errors that have occurred due to programmers not understanding the issues associated with computer arithmetic…\n\nIn February 1991, a basic rounding error within software for the US Patriot missile system caused it to fail, contributing to the loss of 28 lives.\nIn June 1996, the European Space Agency’s Ariane Rocket exploded shortly after take-off: the error was due to failing to handle overflow correctly.\nIn October 2020, a driverless car drove straight into a wall due to faulty handling of a floating-point error.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec02.html#summary",
    "href": "src/lec02.html#summary",
    "title": "2  Floating point number systems",
    "section": "2.6 Summary",
    "text": "2.6 Summary\n\nThere is inaccuracy in almost all computer arithmetic.\nCare must be taken to minimise its effects, for example:\n\nadd the smallest terms in an expression first;\navoid taking the difference of two very similar terms;\neven checking whether \\(a = b\\) is dangerous!\n\nThe usual mathematical rules no longer apply.\nThere is no point in trying to compute a solution to a problem to a greater accuracy than can be stored by the computer.\n\n\n2.6.1 Further reading\n\nWikipedia: Floating-point arithmetic\nDavid Goldberg, What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys, Volume 23, Issue 1, March 1991.\nJohn D Cook, Floating point error is the least of my worries, online, November 2011.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Floating point number systems</span>"
    ]
  },
  {
    "objectID": "src/lec03.html",
    "href": "src/lec03.html",
    "title": "3  Introduction to systems of linear equations",
    "section": "",
    "text": "3.1 Definition of systems of linear equations\nGiven an \\(n \\times n\\) matrix \\(A\\) and an \\(n\\)-vector \\(\\vec{b}\\), find the \\(n\\)-vector \\(\\vec{x}\\) which satisfies: \\[\\begin{equation}\n\\label{eq:sle}\nA \\vec{x} = \\vec{b}.\n\\end{equation}\\]\nWe can also write \\(\\eqref{eq:sle}\\) as a system of linear equations: \\[\\begin{align*}\n\\text{Equation 1:} &&\na_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation 2:} &&\na_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation i:} &&\na_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation n:} &&\na_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nNotes:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#definition-of-systems-of-linear-equations",
    "href": "src/lec03.html#definition-of-systems-of-linear-equations",
    "title": "3  Introduction to systems of linear equations",
    "section": "",
    "text": "The values \\(a_{ij}\\) are known as coefficients.\nThe right-hand side values \\(b_i\\) are known and are given to you as part of the problem.\n\\(x_1, x_2, x_3, \\ldots, x_n\\) are not known and are what you need to find to solve the problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#can-we-do-it",
    "href": "src/lec03.html#can-we-do-it",
    "title": "3  Introduction to systems of linear equations",
    "section": "3.2 Can we do it?",
    "text": "3.2 Can we do it?\nOur first question might be: Is it possible to solve \\(\\eqref{eq:sle}\\)?\nWe know a few simple cases where we can answer this question very quickly:\n\nIf \\(A = I_n\\), the \\(n \\times n\\) identity matrix, then we can solve this problem: \\[\\vec{x} = \\vec{b}.\\]\nIf \\(A = O\\), the \\(n \\times n\\) zero matrix, and \\(\\vec{b} \\neq \\vec{0}\\), the zero vector, then we cannot solve this problem: \\[O \\vec{x} = \\vec{0} \\neq\n\\vec{b} \\quad \\text{for any vector} \\quad \\vec{x}.\\]\nIf \\(A\\) is invertible, with inverse \\(A^{-1}\\), then we can solve this problem: \\[\n\\vec{x} = A^{-1} \\vec{b}.\n\\] However, in general, this is a terrible idea and we will see algorithms that are more efficient than finding the inverse of \\(A\\).\n\n\nRemark 3.1. One way to solve a system of linear equations is to compute the inverse of \\(A\\), \\(A^{-1}\\), directly, then the solution is found through matrix multiplication: \\(\\vec{x} = A^{-1} \\vec{b}\\). Here we compare two simple and one more specialised implementations:\n\ndef approach1(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b by applying the inverse of A\n    \"\"\"\n    A_inv = np.linalg.inv(A)\n    x = A_inv @ b\n    return x\n\n\ndef approach2(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b through numpy's linear solver\n    \"\"\"\n    x = np.linalg.solve(A, b)\n    return x\n\n\ndef approach3(A_sparse, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b through scipy's sparse linear\n    solver\n    \"\"\"\n    x = sp.sparse.linalg.spsolve(A_sparse, b)\n    return x\n\n\n\n\n\n\n\n\n\n\n\nThere are tools to help us determine when a matrix is invertible which arise naturally when considering about what \\(A \\vec{x} = \\vec{b}\\) means! We have to go back to the basic operations on vectors.\nThere are two fundamental operations you can do on vectors: addition and scalar multiplication. Consider the vectors: \\[\\begin{align*}\n\\vec{a} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}, \\quad\n\\vec{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}, \\quad\n\\vec{c} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 6 \\end{pmatrix}.\n\\end{align*}\\] Then, we can easily compute the following linear combinations: \\[\\begin{align}\n\\label{eq:vector-linear-comb-a}\n\\vec{a} + \\vec{b} & = \\begin{pmatrix} 3 \\\\ 3 \\\\ 6 \\end{pmatrix} \\\\\n\\label{eq:vector-linear-comb-b}\n\\vec{c} - 2 \\vec{a} & = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} \\\\\n\\label{eq:vector-linear-comb-c}\n\\vec{a} + 2 \\vec{b} + 2 \\vec{c} &= \\begin{pmatrix} 12 \\\\ 9 \\\\ 22 \\end{pmatrix}.\n\\end{align}\\] Now if we write \\(A\\) for the \\(3 \\times 3\\)-matrix whose columns are \\(\\vec{a},\n\\vec{b}, \\vec{c}\\): \\[\\begin{align*}\nA = \\begin{pmatrix}\n&& \\\\ \\vec{a} & \\vec{b} & \\vec{c} \\\\ &&\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix},\n\\end{align*}\\] then the three equations \\(\\eqref{eq:vector-linear-comb-a}\\), \\(\\eqref{eq:vector-linear-comb-b}\\), \\(\\eqref{eq:vector-linear-comb-c}\\), can be written as \\[\\begin{align*}\n\\vec{a} + \\vec{b} & = 1 \\vec{a} + 1 \\vec{b} + 0 \\vec{c}\n= A \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\\\\n\\vec{c} - 2\\vec{a} & = -2 \\vec{a} + 0 \\vec{b} - 2 \\vec{c}\n= A \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\\\\n\\vec{a} + 2 \\vec{b} + 2 \\vec{c} & = 1 \\vec{a} + 2 \\vec{b} + 2 \\vec{c}\n= A \\begin{pmatrix} 1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\end{align*}\\] In other words,\n\nWe can write any linear combination of vectors as a matrix-vector multiply,\n\nor if we reverse the process,\n\nWe can write matrix-vector multiplication as a linear combination of the columns of the matrix.\n\nThis rephrasing means that solving the system \\(A \\vec{x} = \\vec{b}\\) is equivalent to finding a linear combination of the columns of \\(A\\) which is equal to \\(b\\). So, our question about whether we can solve \\(\\eqref{eq:sle}\\), can also be rephrased as: does there exist a linear combination of the columns of \\(A\\) which is equal to \\(\\vec{b}\\)? We will next write this condition mathematically using the concept of span.\n\n3.2.1 The span of a set of vectors\n\nDefinition 3.1 Given a set of vectors of the same size, \\(S = \\{ \\vec{v}_1,\n\\ldots, \\vec{v}_k \\}\\), we say the span of \\(S\\) is the set of all vectors which are linear combinations of vectors in \\(S\\): \\[\\begin{equation}\n\\mathrm{span}(S) = \\left\\{ \\sum_{i=1}^k x_i \\vec{v}_i : x_i \\in \\mathbb{R}\n\\text{ for } i = 1, \\ldots, k \\right\\}.\n\\end{equation}\\]\n\n\nExample 3.1 Consider three new vectors \\[\\begin{align*}\n\\vec{a} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} \\quad\n\\vec{b} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\quad\n\\vec{c} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\end{align*}\\]\n\n\n\n\n\nThree example vectors in a plane.\n\n\n\n\n\nLet \\(S = \\{ \\vec{a} \\}\\), then \\(\\mathrm{span}(S) = \\{ x \\vec{a} : x \\in\n\\mathbb{R} \\}\\). Geometrically, we can think of the span of a single vector to be an infinite straight line which passes through the origin and \\(\\vec{a}\\).\nLet \\(S = \\{ \\vec{a}, \\vec{b} \\}\\), then \\(\\mathrm{span}(S) = \\mathbb{R}^2\\). To see this is true, we first see that \\(\\mathrm{span}(S)\\) is contained in \\(\\mathbb{R}^2\\) since any \\(2\\)-vectors added together and the scalar multiplication of a \\(2\\)-vector also form a \\(2\\)-vector. For the opposite inclusion, consider an arbitrary point \\(\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2\n\\end{pmatrix} \\in \\mathbb{R}^2\\) then \\[\\begin{align}\n\\nonumber\n\\frac{2 y_1 + y_2}{7} \\vec{a} + \\frac{-3 y_1 + 2 y_2}{7} \\vec{b}\n= \\frac{2 y_1 + y_2}{7} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n- \\frac{-3 y_1 + 2 y_2}{7} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\\\\n= \\begin{pmatrix}\n\\frac{4 y_1 + 2 y_2}{7} + \\frac{3 y_1 - 2 y_2}{7} \\\\\n\\label{eq:y-combo}\n\\frac{6 y_1 + 3 y_2}{7} + \\frac{-6 y_1 + 4 y_2}{7}\n\\end{pmatrix}\n= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\vec{y}.\n\\end{align}\\] This calculation shows, that we can always form a linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) which results in \\(\\vec{y}\\).\nLet \\(S = \\{ \\vec{a}, \\vec{b}, \\vec{c} \\}\\), then \\(\\mathrm{span}(S) =\n\\mathbb{R}^2\\). Since \\(\\vec{c} \\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\), any linear combination of \\(\\vec{a}, \\vec{b}, \\vec{c}\\) has an equivalent combination of just \\(\\vec{a}\\) and \\(\\vec{b}\\). In formulae, we can see that by applying the formula from \\(\\eqref{eq:y-combo}\\), we have \\[\\begin{align*}\n\\vec{c} = \\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b}.\n\\end{align*}\\] So we have, if \\(\\vec{y} \\in \\mathrm{span}(S)\\), then \\[\\begin{align*}\n\\vec{y} = x_1 \\vec{a} + x_2 \\vec{b} + x_3 \\vec{c} & \\Rightarrow \\vec{y}\n= (x_1 + \\frac{1}{7} x_3) \\vec{a} + (x_2 - \\frac{5}{7} x_3) \\vec{b},\n\\end{align*}\\] so \\(\\vec{y} \\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\). Conversely, if \\(\\vec{y}\n\\in \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\\), then \\[\\begin{align*}\n\\vec{y} = x_1 \\vec{a} + x_2 \\vec{b} & \\Rightarrow \\vec{y}\n= x_1 \\vec{a} + x_2 \\vec{b} + 0\n\\vec{c}.\n\\end{align*}\\] So the span of \\(S = \\mathrm{span}(\\{\\vec{a}, \\vec{b}\\})\n= \\mathbb{R}^2\\). Notice that we this final linear combination of \\(\\vec{a},\n\\vec{b}\\) and \\(\\vec{c}\\) to form \\(\\vec{y}\\) is not unique.\n\n\nOur first statement is that \\(\\eqref{eq:sle}\\) has a solution if \\(\\vec{b}\\) is in the span of the columns of \\(A\\). However, as we saw with Example 3.1, Part 3, we are not guaranteed that the linear combination is unique! For this we need a further condition.\n\n\n3.2.2 Linear independence\n\nDefinition 3.2 Given a set of vectors of the same size, \\(S = \\{\\vec{v}_1, \\ldots, \\vec{v}_k\n\\}\\), we say that \\(S\\) is linearly dependent, if there exist numbers \\(x_1, x_2,\n\\ldots x_k\\), not all zero, such that \\[\\begin{align*}\n\\sum_{i=1}^k x_i \\vec{v}_i = \\vec{0}.\n\\end{align*}\\] The set \\(S\\) is linearly independent if it is not linearly dependent.\n\nExercise 3.1 Can you write the definition of a linearly independent set of vectors explicitly?\n\n\n\nExample 3.2 Continuing from Example 3.1.\n\nLet \\(S = \\{ \\vec{a}, \\vec{b} \\}\\), then \\(S\\) is linearly independent. Indeed, let \\(x_1, x_2\\) be real numbers such that \\[\\begin{align*}\nx_1 \\vec{a} + x_2 \\vec{b} = \\vec{0},\n\\end{align*}\\] then, \\[\\begin{align*}\n2 x_1 - x_2 & = 0 && 3 x_1 + 2 x_2 & = 0\n\\end{align*}\\] The first equation says that \\(x_2 = 2 x_1\\), which when substituted into the second equation gives \\(3 x_1 + 4 x_1 = 7 x_1 = 0\\). Together this implies that \\(x_1 = x_2 = 0\\). Put simply this means that if we do have a linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) which is equal zero, then the corresponding scalar multiples are all zero.\nLet \\(S = \\{ \\vec{a}, \\vec{b}, \\vec{c} \\}\\), then \\(S\\) is linearly dependent. We have previously seen that: \\[\\begin{align*}\n\\vec{c} = \\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b},\n\\end{align*}\\] which we can rearrange to say that \\[\\begin{align*}\n\\frac{1}{7} \\vec{a} - \\frac{5}{7} \\vec{b} - \\vec{c} = \\vec{0}.\n\\end{align*}\\] We see that the definition of linear dependence is satisfied for \\(x_1 = \\frac{1}{7}, x_2 = -\\frac{5}{7}, x_3 = -1\\) which are all nonzero.\n\n\nSo linear independence removes the multiplicity (or non-uniqueness) in how we form linear combinations! The ideas of linear independence and spanning lead us to the final definition of this section.\n\n\n3.2.3 When vectors form a basis\n\nDefinition 3.3 We say that a set of \\(n\\)-vectors \\(S\\) is a basis of a set of \\(n\\)-vectors \\(V\\) if the span of \\(S\\) is \\(V\\) and \\(S\\) is linearly independent.\n\n\nExample 3.3  \n\nFrom Example 3.1, we have that \\(S = \\{ \\vec{a}, \\vec{b} \\}\\) is a basis of \\(\\mathbb{R}^2\\).\nAnother (perhaps simpler) basis of \\(\\mathbb{R}^2\\) are the coordinate axes: \\[\\begin{align*}\n\\vec{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad\n\\text{and} \\quad \\vec{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n\\end{align*}\\]\nLater, when we work with at eigenvectors and eigenvalues we will see that there are other convenient bases (plural of basis) to work with.\n\n\nWe phrase the idea that the existence and uniqueness of linear combinations together depend on the underlying set being a basis mathematically in the following Theorem:\n\nTheorem 3.1 Let \\(S\\) be a basis of \\(V\\). Then any vector in \\(V\\) can be written uniquely as a linear combination of entries in \\(S\\).\n\n\nExample 3.4  \n\nFrom the main examples (Example 3.1) in this section, we have that \\(S = \\{ \\vec{a}, \\vec{b}\n\\}\\) is a basis of \\(\\mathbb{R}^2\\) and we already know the formula for how to write \\(\\vec{y}\\) as a unique combination of \\(\\vec{a}\\) and \\(\\vec{b}\\): it is given in \\(\\eqref{eq:y-combo}\\).\nFor the simpler example of the coordinate axes: \\[\\begin{align*}\n\\vec{e}_1 =\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad\n\\vec{e}_2 =\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n\\end{align*}\\] we have that for any \\(\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\in \\mathbb{R}^2\\) \\[\\begin{align*}\n\\vec{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n= y_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} +\ny_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = y_1 \\vec{e}_1 + y_2 \\vec{e}_2.\n\\end{align*}\\]\n\n\n\nProof (Proof of Theorem 3.1). Let \\(\\vec{y}\\) be a vector in \\(V\\) and label \\(S = \\{ \\vec{v}_1, \\ldots \\vec{v}_k\n\\}\\). Since \\(S\\) forms a basis of \\(V\\), \\(\\vec{y} \\in \\mathrm{span}(S)\\) so there exists numbers \\(x_1, \\ldots, x_k\\) such that \\[\\begin{align}\n\\label{eq:basis_pf_x}\n\\vec{y} = \\sum_{i=1}^k x_i \\vec{v}_i.\n\\end{align}\\] Suppose that there exists another set of number \\(z_1, \\ldots z_k\\) such that \\[\\begin{align}\n\\label{eq:basis_pf_z}\n\\vec{y} = \\sum_{i=1}^k z_i \\vec{v}_i.\n\\end{align}\\] Taking the difference of \\(\\eqref{eq:basis_pf_x}\\) and \\(\\eqref{eq:basis_pf_z}\\), we see that \\[\\begin{align}\n\\vec{0} = \\sum_{i=1}^k (x_i - z_i) \\vec{v}_i.\n\\end{align}\\] Since \\(S\\) is linearly independent, this implies \\(x_i = z_i\\) for \\(i = 1, \\ldots,\nk\\), and we have shown that there is only one linear combination of the vectors \\(\\{ \\vec{v}_i \\}\\) to form \\(\\vec{y}\\).\n\nThere is a theorem that says that the number of vectors in any basis of a given ‘nice’ set of vectors \\(V\\) is the same, but is beyond the scope of this module!\n\nTheorem 3.2 Let \\(A\\) be a \\(n \\times n\\)-matrix. If the columns of \\(A\\) form a basis of \\(\\mathbb{R}^n\\), then there exists a unique \\(n\\)-vector \\(\\vec{x}\\) which satisfies \\(A \\vec{x} = \\vec{b}\\).\n\nWe do not give full details of the proof here since all the key ideas are already given above.\n\n\n3.2.4 Another characterisation: the determinant\nLet \\(\\vec{e}_j\\) be the (column) vector in \\(\\mathbb{R}^n\\) which has 0 for all coefficients apart from the \\(j\\)th place: \\[\n\\vec{e}_j = (0, \\ldots, 0, \\underbrace{1}_{\\text{$j$th place}}, 0, \\ldots, 0).\n\\] Then, we can compute that for any \\(n \\times n\\) matrix \\(A\\) that we have the \\(i\\)th component of \\(A \\vec{e}_j\\) is given by \\[\\begin{align*}\n(A \\vec{e}_j)_i = \\sum_{k=1}^n A_{ik} (\\vec{e}_j)_k = A_{ij}.\n\\end{align*}\\] That is that \\(A \\vec{e}_j\\) gives the \\(j\\)th column of the matrix \\(A\\).\nOne geometric way to see that the columns of \\(A\\) form a basis is to explore the shape of polytope with edges away from the origin given by the columns of \\(A\\) (hyper-parallelopiped).\n\nExample 3.5 For \\(A\\) given by \\[\nA = \\begin{pmatrix}\n3 & 1 \\\\ 0 & 3\n\\end{pmatrix}\n\\] the shape is a parallelogram that looks like this:\n\n\n\n\n\n\n\n\n\nFor \\(A\\) given by \\[\nA = \\begin{pmatrix}\n3 & 1 & 1\\\\ 0 & 3 & 1 \\\\ 0 & 0 & 3\n\\end{pmatrix},\n\\] the shape is a parallelopiped given that looks like this:\n\n\n\n\n\n\n\n\n\n\nIf we are given an \\(n \\times n\\) matrix \\(A\\), its \\(n\\) columns form a basis of \\(\\mathbb{R}^n\\) if, and only if, the area/volume/hyper-volume of the associated hyper-parallelepiped is nonzero. Roughly speaking this follows since if the area/volume/hyper-volume is zero then two of the column vectors must point in the same direction. This property is so important that the area/volume/hyper-volume of the hyper-parallelepiped associated with the matrix \\(A\\) has a special name: the determinant of \\(A\\) and we write \\(\\det A\\).\n\nDefinition 3.4 Let \\(A\\) be a square \\(n \\times n\\) matrix.\nIf \\(n = 2\\), \\[\\begin{equation}\n\\det A = \\det \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}\n= a_{11} a_{22} - a_{21} a_{12}.\n\\end{equation}\\]\nIf \\(n = 3\\), \\[\\begin{align*}\n\\det A & = \\det \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix} \\\\\n& = a_{11} (a_{22} a_{33} - a_{23} a_{32}) - a_{12} (a_{21} a_{33} - a_{23}\na_{31}) + a_{13} (a_{21} a_{32} - a_{22} - a_{31}).\n\\end{align*}\\]\nFor general \\(n\\), the determinant can be found by, for example, Laplace expansions \\[\n\\det A = \\sum_{j=1}^n (-1)^{j+1} a_{1,j} m_{1,j},\n\\] where \\(a_{1,j}\\) is the entry of the first row and \\(j\\)th column of \\(A\\) and \\(m_{1,j}\\) is the determinant of the submatrix obtained by removing the first row and the \\(j\\)th column from \\(A\\).\n\n\nExample 3.6  \n\nLet \\(A\\) be given by \\[\nA = \\begin{pmatrix}\n3 & 1 \\\\ 0 & 3\n\\end{pmatrix}.\n\\] Then we can compute that \\[\n\\det A = 3 \\times 3 - 1 \\times 0 = 9 - 0 = 9.\n\\]\nLet \\(A\\) be given by \\[\nA = \\begin{pmatrix}\n3 & 1 & 1\\\\ 0 & 3 & 1 \\\\ 0 & 0 & 3\n\\end{pmatrix}.\n\\] Then we can compute that \\[\\begin{align*}\n\\det A\n& = 3 \\times (1 \\times 3 - 0 \\times 3) - 1 \\times (0 \\times 3 - 3 \\times 0) +\n1 \\times (0 \\times 0 - 3 \\times 0) \\\\\n& = 3 \\times 3 - 1 \\times 0 + 1 \\times 0 = 9.\n\\end{align*}\\]\n\nExercise 3.2 Compute the determinant of \\[\n\\begin{pmatrix}\n2 & -1 & 3 \\\\\n3 & 2 & -4 \\\\\n5 & 1 & 1\n\\end{pmatrix}.\n\\]\n\n\n\nWe have been inaccurate in what we have said before. Strictly speaking the determinant we have defined here is the signed area/volume/hyper-volume of the associated hyper-parallelepiped, and we can recover the actual volume by taking \\(| \\det A |\\).\n\nTheorem 3.3 Let \\(A\\) be an \\(n \\times n\\) matrix and \\(\\vec{b}\\) be an \\(n\\)-column vector. The following are equivalent:\n\n\\(A \\vec{x} = \\vec{b}\\) has a unique solution.\nThe columns of \\(A\\) form a basis of \\(\\mathbb{R}^n\\).\n\\(\\det A \\neq 0\\).\n\n\nThis proof is beyond the scope of this course.\nWe do have the following rule for manipulating determinants too:\n\nLet \\(A, B\\) be square \\(n \\times n\\)-matrices then \\(\\det (AB) = \\det A \\det B\\).\nLet \\(A\\) be a square \\(n \\times n\\)-matrix and \\(c\\) a real number then \\(\\det (cA)\n= c^n \\det A\\).\nLet \\(A\\) be an invertible square matrix, then \\(\\det (A^{-1}) = \\frac{1}{\\det A}\\).\n\n\nExercise 3.3 Prove these three rules for manipulating determinants for \\(2 \\times 2\\) matrices by direct calculation.\n\n\nRemark 3.2. Computing determinants numerically is an important topic in it’s own right but we do not focus on this in this module.\nYou can use numpy to compute determinants\n\nA = np.array([[3.0, 1.0], [0.0, 3.0]])\nnp.linalg.det(A)\n\nnp.float64(9.000000000000002)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#special-types-of-matrices",
    "href": "src/lec03.html#special-types-of-matrices",
    "title": "3  Introduction to systems of linear equations",
    "section": "3.3 Special types of matrices",
    "text": "3.3 Special types of matrices\nThe general matrix \\(A\\) before the examples is known as a full matrix: any of its components \\(a_{ij}\\) might be nonzero.\nAlmost always, the problem being solved leads to a matrix with a particular structure of entries: Some entries may be known to be zero. If this is the case then it is often possible to use this knowledge to improve the efficiency of the algorithm (in terms of both speed and/or storage).\n\nExample 3.7 (Triangular matrix) One common (and important) structure takes the form\n\\[\nA = \\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\ a_{21} & a_{22} & 0 & \\cdots &\n0 \\\\ a_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots\n& \\vdots \\\\ a_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn} \\end{pmatrix}.\n\\]\n\nA is a lower triangular matrix. Every entry above the leading diagonal is zero:\n\\[ a_{ij} = 0 \\quad \\text{ for } \\quad j &gt; i. \\]\nThe transpose of this matrix is an upper triangular matrix and can be treated in a very similar manner.\nNote that the determinant of a triangular matrix is simply the product of diagonal coefficients: \\[\n\\det A = a_{11} a_{22} \\cdots a_{nn} = \\prod_{i=1}^n a_{ii}.\n\\]\n\n\n\nExample 3.8 (Sparse matrices) Sparse matrices are prevalent in any application which relies on some form of graph structure (see both the temperature, Example 1.4, and traffic network examples, Example 1.5).\n\nThe \\(a_{ij}\\) typically represents some form of “communication” between vertices \\(i\\) and \\(j\\) of the graph, so the element is only nonzero if the vertices are connected.\nThere is no generic pattern for these entries, though there is usually one that is specific to the problem solved.\nUsually, \\(a_{ii} \\neq 0\\) - the diagonal is nonzero.\nA “large” portion of the matrix is zero.\n\nA full \\(n \\times n\\) matrix has \\(n^2\\) nonzero entries.\nA sparse \\(n \\times n\\) has \\(\\alpha n\\) nonzero entries, where \\(\\alpha \\ll\n    n\\).\n\nMany special techniques exist for handling sparse matrices, some of which can be used automatically within Python (scipy.sparse documentation)\n\n\nWhat is the significance of these special examples?\n\nIn the next section we will discuss a general numerical algorithm for the solution of linear systems of equations.\nThis will involve reducing the problem to one involving a triangular matrix, which, as we show below, is relatively easy to solve.\nIn subsequent lectures, we will see that, for sparse matrix systems, alternative solution techniques are available.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec03.html#further-reading",
    "href": "src/lec03.html#further-reading",
    "title": "3  Introduction to systems of linear equations",
    "section": "3.4 Further reading",
    "text": "3.4 Further reading\n\nWikipedia: Systems of linear equations (includes a nice geometric picture of what a system of linear equations means).\nMaths is fun: Systems of linear equations (very basic!)\nGregory Gundersen Why shouldn’t I invert that matrix?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html",
    "href": "src/lec04.html",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "",
    "text": "4.1 Reminder of the problem\nThis section deals with first method we will use to solve systems of linear equations. We will see that by using the approach of Gaussian elimination (and variations of that method), we can solve any system of linear equations that has a solution.\nGaussian elimination should look very familiar to you – this looks a lot like solving simultaneous equations in high school. Here, we are trying to codify this approach into an algorithm that a computer can apply without any further human input in a way that always ‘works’ (or at least works when it should!).\nRecall the problem is to solve a set of \\(n\\) linear equations for \\(n\\) unknown values \\(x_j\\), for \\(j=1, 2, \\ldots, n\\).\nNotation:\n\\[\\begin{align*}\n\\text{Equation } 1:\n&& a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \\cdots + a_{1n} x_n & = b_1 \\\\\n\\text{Equation } 2:\n&& a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \\cdots + a_{2n} x_n & = b_2 \\\\\n\\vdots \\\\\n\\text{Equation } i:\n&& a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \\cdots + a_{in} x_n & = b_i \\\\\n\\vdots \\\\\n\\text{Equation } n:\n&& a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \\cdots + a_{nn} x_n & = b_n.\n\\end{align*}\\]\nWe can also write the system of linear equations in general matrix-vector form:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\na_{31} & a_{32} & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nRecall that the \\(n \\times n\\) matrix \\(A\\) represents the coefficients that multiply the unknowns in each equation (row), while the \\(n\\)-vector \\(\\vec{b}\\) represents the right-hand-side values.\nOur strategy will be to reduce the system to a triangular system of matrices, which is then easy to solve!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#elementary-row-operations",
    "href": "src/lec04.html#elementary-row-operations",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.2 Elementary row operations",
    "text": "4.2 Elementary row operations\nConsider equation \\(p\\) of the above system:\n\\[\na_{p1} x_1 + a_{p2} x_2 + a_{p3} x_3 + \\cdots + a_{pn} x_n = b_p,\n\\]\nand equation \\(q\\):\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q.\n\\]\nNote three things…\n\nThe order in which we choose to write the \\(n\\) equations is irrelevant\nWe can multiply any equation by an arbitrary real number (\\(k \\neq 0\\) say):\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p.\n\\]\nWe can add any two equations:\n\\[\nk a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \\cdots + k a_{pn} x_n = k b_p\n\\]\n\nadded to\n\\[\na_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \\cdots + a_{qn} x_n = b_q\n\\]\nyields\n\\[\n(k a_{p1} + a_{q1}) x_1 + (k a_{p2} + a_{q2}) x_2 + \\cdots + (k a_{pn} +\na_{qn}) x_n = k b_p + b_q.\n\\]\n\nExample 4.1 Consider the system \\[\\begin{align}\n2 x_1 + 3 x_2 & = 4  \\label{eq:eg1a} \\\\\n-3 x_1 + 2 x_2 & = 7 \\label{eq:eg1b}.\n\\end{align}\\] Then we have: \\[\\begin{align*}\n4 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow & 8 x_1 + 12 x_2 & = 16 \\\\\n-15. \\times \\text{\\eqref{eq:eg1b}} & \\rightarrow & 4.5 x_2 - 3 x_2 & = -10.5 \\\\\n\\text{\\eqref{eq:eg1a}} + \\text{\\eqref{eq:eg1b}} & \\rightarrow &\n-x_1 + 5 x_2 & = 11 \\\\\n\\text{\\eqref{eq:eg1b}} + 1.5 \\times \\text{\\eqref{eq:eg1a}} & \\rightarrow &\n0 + 6.5 x_2 & = 13.\n\\end{align*}\\]\n\n\nExercise 4.1 Consider the system \\[\\begin{align}\nx_1 + 2 x_2 & = 1 \\label{eq:eg2a} \\\\\n4 x_1 + x_2 & = -3 \\label{eq:eg2b}.\n\\end{align}\\] Work out the result of these elementary row operations: \\[\\begin{align*}\n2 \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n0.25 \\times \\text{\\eqref{eq:eg2b}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-1) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow \\\\\n\\text{\\eqref{eq:eg2b}} + (-4) \\times \\text{\\eqref{eq:eg2a}} & \\rightarrow\n\\end{align*}\\]\n\nFor a system written in matrix form our three observations mean the following:\n\nWe can swap any two rows of the matrix (and corresponding right-hand side entries). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n-3 & 2\\\\ 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n7 \\\\ 4\n\\end{pmatrix}\n\\]\nWe can multiply any row of the matrix (and corresponding right-hand side entry) by a scalar. For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & \\frac{3}{2} \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7\n\\end{pmatrix}\n\\]\nWe can replace row \\(q\\) by row \\(q + k \\times\\) row \\(p\\). For example:\n\\[\n\\begin{pmatrix}\n2 & 3 \\\\ -3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 7\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n2 & 3 \\\\ 0 & 6.5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n4 \\\\ 13\n\\end{pmatrix}\n\\]\n(here we replaced row \\(w\\) by row \\(2 + 1.5 \\times\\) row \\(1\\))\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\ 4 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}\n\\Rightarrow\n\\begin{pmatrix}\n1 & 2 \\\\ 0 & -7\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\ -7\n\\end{pmatrix}\n\\]\n(here we replaced row \\(2\\) by row \\(2 + (-4) \\times\\) row \\(1\\))\n\nOur strategy for solving systems of linear equations using Gaussian elimination is based on the following ideas:\n\nThe three types of operation described above are called elementary row operations (ERO).\nWe will apply a sequence of ERO to reduce an arbitrary system to a triangular form, which, we will see, can be easily solved.\nThe algorithm for reducing a general matrix to upper triangular form is known as forward elimination or (more commonly) as Gaussian elimination.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#gaussian-elimination",
    "href": "src/lec04.html#gaussian-elimination",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.3 Gaussian elimination",
    "text": "4.3 Gaussian elimination\nThe algorithm of Gaussian elimination is an ancient method that you may have already met at school – perhaps by a different name. The details of the method may seem confusing at first. We are following the ideas of elimination from systems of simultaneous equations, but in a way that a computer understands. Thinking like a computer is an important point in this section. You will have seen different ideas of how to solve systems of simultaneous equations where the first step is to “look at the equations to decide the easiest first step”. When there are \\(10^9\\) equations, it is not effective for a computer to try to find an easy way through the problem. The computer must be given a simple set of instructions to follow: this will be our algorithm.\nThe method is so old, in fact we have evidence of Chinese mathematicians using Gaussian elimination in 179CE (From Wikipedia):\n\nThe method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE. It was commented on by Liu Hui in the 3rd century.\nThe method in Europe stems from the notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems. The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.\n\n\n\n\n\n\nImage from Nine Chapter of the Mathematical art. By Yang Hui(1238-1298) - mybook, Public Domain, https://commons.wikimedia.org/w/index.php?curid=10317744\n\n\n\n\n\n4.3.1 The algorithm\nThe following algorithm systematically introduces zeros into the system of equations, below the diagonal.\n\nSubtract multiples of row 1 from the rows below it to eliminate (make zero) non-zero entries in column 1.\nSubtract multiples of the new row 2 from the rows below it to eliminate non-zero entries in column 2.\nRepeat for row \\(3, 4, \\ldots, n-1\\).\n\nAfter row \\(n-1\\) all entities below the diagonal have been eliminated, so \\(A\\) is now upper triangular and the resulting system can be solved by backward substitution.\n\nExample 4.2 Use Gaussian elimination to reduce the following system of equations to upper triangular form:\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\ 1 & 2 & 2 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nFirst, use the first row to eliminate the first column below the diagonal:\n\n(row 2) \\(- 0.5 \\times\\) (row 1) gives\n\\[\n\\begin{pmatrix}\n  2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ 2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 22\n\\end{pmatrix}\n\\]\n(row 3) \\(-\\) (row 1) then gives\n\\[\n\\begin{pmatrix}\n  2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & 3 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 10\n\\end{pmatrix}\n\\]\n\nNow use the second row to eliminate the second column below the diagonal.\n\n(row 3) \\(- 2 \\times\\) (row 2) gives\n\\[\n  \\begin{pmatrix}\n   2 & 1 & 4 \\\\ \\mathbf{0} & 1.5 & 0 \\\\ \\mathbf{0} & \\mathbf{0} & 2\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  x_1 \\\\ x_2 \\\\ x_3\n  \\end{pmatrix} =\n  \\begin{pmatrix}\n  12 \\\\ 3 \\\\ 4\n  \\end{pmatrix}\n  \\]\n\n\n\nExercise 4.2 Use Gaussian elimination to reduce the following system of linear equations to upper triangular form.\n\\[\n\\begin{pmatrix}\n4 & -1 & -1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n9 \\\\ -6 \\\\ 3\n\\end{pmatrix}.\n\\]\n\nThe key idea is that:\n\nEach row \\(i\\) is used to eliminate the entries in column \\(i\\) below \\(a_{ii}\\), i.e. it forces \\(a_{ji} = 0\\) for \\(j &gt; i\\).\nThe elimination is done by subtracting a multiple of row \\(i\\) from row \\(j\\):\n\\[\n  (\\text{row } j) \\leftarrow (\\text{row } j) - \\frac{a_{ji}}{a_{ii}}\n  (\\text{row } i).\n  \\]\nOur update formula guarantees that \\(a_{ji}\\) becomes zero because\n\\[\n  a_{ji} \\leftarrow a_{ji} - \\frac{a_{ji}}{a_{ii}} a_{ii} = a_{ji} - a_{ji} =\n  0.\n  \\]\n\n\n\n4.3.2 Python version\nWe start with some helper code which determines the size of the system we are working with:\n\ndef system_size(A, b):\n    \"\"\"\n    Validate the dimensions of a linear system and return its size.\n\n    This function checks whether the given coefficient matrix `A` is square\n    and whether its dimensions are compatible with the right-hand side vector\n    `b`. If the dimensions are valid, it returns the size of the system.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D array of shape ``(n, n)`` representing the coefficient matrix of\n        the linear system.\n    b : numpy.ndarray\n        A array of shape ``(n, o)`` representing the right-hand side vector.\n\n    Returns\n    -------\n    int\n        The size of the system, i.e., the number of variables `n`.\n\n    Raises\n    ------\n    ValueError\n        If `A` is not square or if the size of `b` does not match the number of\n        rows in `A`.\n    \"\"\"\n\n    # Validate that A is a 2D square matrix\n    if A.ndim != 2:\n        raise ValueError(f\"Matrix A must be 2D, but got {A.ndim}D array\")\n\n    n, m = A.shape\n    if n != m:\n        raise ValueError(f\"Matrix A must be square, but got A.shape={A.shape}\")\n\n    if b.shape[0] != n:\n        raise ValueError(\n            f\"System shapes are not compatible: A.shape={A.shape}, \"\n            f\"b.shape={b.shape}\"\n        )\n\n    return n\n\nThen we can implement the elementary row operations\n\ndef row_swap(A, b, p, q):\n    \"\"\"\n    Swap two rows in a linear system of equations in place.\n\n    This function swaps the rows `p` and `q` of the coefficient matrix `A`\n    and the right-hand side vector `b` for a linear system of equations\n    of the form ``Ax = b``. The operation is performed **in place**, modifying\n    the input arrays directly.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient matrix\n        of the linear system.\n    b : numpy.ndarray\n        A 2D NumPy array of shape ``(n, 1)`` representing the right-hand side\n        vector of the system.\n    p : int\n        The index of the first row to swap. Must satisfy ``0 &lt;= p &lt; n``.\n    q : int\n        The index of the second row to swap. Must satisfy ``0 &lt;= q &lt; n``.\n\n    Returns\n    -------\n    None\n        This function modifies `A` and `b` directly and does not return\n        anything.\n    \"\"\"\n    # get system size\n    n = system_size(A, b)\n    # swap rows of A\n    for j in range(n):\n        A[p, j], A[q, j] = A[q, j], A[p, j]\n    # swap rows of b\n    b[p, 0], b[q, 0] = b[q, 0], b[p, 0]\n\n\ndef row_scale(A, b, p, k):\n    \"\"\"\n    Scale a row of a linear system by a constant factor in place.\n\n    This function multiplies all entries in row `p` of the coefficient matrix\n    `A` and the corresponding entry in the right-hand side vector `b` by a\n    scalar `k`. The operation is performed **in place**, modifying the input\n    arrays directly.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient matrix\n        of the linear system.\n    b : numpy.ndarray\n        A 2D NumPy array of shape ``(n, 1)`` representing the right-hand side\n        vector of the system.\n    p : int\n        The index of the row to scale. Must satisfy ``0 &lt;= p &lt; n``.\n    k : float\n        The scalar multiplier applied to the entire row.\n\n    Returns\n    -------\n    None\n        This function modifies `A` and `b` directly and does not return\n        anything.\n    \"\"\"\n    n = system_size(A, b)\n\n    # scale row p of A\n    for j in range(n):\n        A[p, j] = k * A[p, j]\n    # scale row p of b\n    b[p, 0] = b[p, 0] * k\n\n\ndef row_add(A, b, p, k, q):\n    \"\"\"\n    Perform an in-place row addition operation on a linear system.\n\n    This function applies the elementary row operation:\n\n    ``row_p ← row_p + k * row_q``\n\n    where `row_p` and `row_q` are rows in the coefficient matrix `A` and the\n    right-hand side vector `b`. It updates the entries of `A` and `b`\n    **in place**, directly modifying the original data.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient matrix\n        of the linear system.\n    b : numpy.ndarray\n        A 2D NumPy array of shape ``(n, 1)`` representing the right-hand side\n        vector of the system.\n    p : int\n        The index of the row to be updated (destination row). Must satisfy\n        ``0 &lt;= p &lt; n``.\n    k : float\n        The scalar multiplier applied to `row_q` before adding it to `row_p`.\n    q : int\n        The index of the source row to be scaled and added. Must satisfy\n        ``0 &lt;= q &lt; n``.\n    \"\"\"\n    n = system_size(A, b)\n\n    # Perform the row operation\n    for j in range(n):\n        A[p, j] = A[p, j] + k * A[q, j]\n\n    # Update the corresponding value in b\n    b[p, 0] = b[p, 0] + k * b[q, 0]\n\nLet’s test we are ok so far:\nTest 1: swapping rows\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"swapping rows 0 and 1\")\nrow_swap(A, b, 0, 1)  # remember numpy arrays are indexed starting from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nswapping rows 0 and 1\n\nnew arrays:\nA = [ -3.0,  2.0 ]\n    [  2.0,  3.0 ]\nb = [  7.0 ]\n    [  4.0 ]\n\n\n\nTest 2: scaling one row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 0 |-&gt; 0.5 * row 0\")\nrow_scale(A, b, 0, 0.5)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 0 |-&gt; 0.5 * row 0\n\nnew arrays:\nA = [  1.0,  1.5 ]\n    [ -3.0,  2.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n\n\n\nTest 3: replacing a row by that adding a multiple of another row\n\nA = np.array([[2.0, 3.0], [-3.0, 2.0]])\nb = np.array([[4.0], [7.0]])\n\nprint(\"starting arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"row 1 |-&gt; row 1 + 1.5 * row 0\")\nrow_add(A, b, 1, 1.5, 0)  # remember numpy arrays are indexed started from zero!\nprint()\n\nprint(\"new arrays:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nstarting arrays:\nA = [  2.0,  3.0 ]\n    [ -3.0,  2.0 ]\nb = [  4.0 ]\n    [  7.0 ]\n\nrow 1 |-&gt; row 1 + 1.5 * row 0\n\nnew arrays:\nA = [  2.0,  3.0 ]\n    [  0.0,  6.5 ]\nb = [  4.0 ]\n    [ 13.0 ]\n\n\n\nNow we can define our Gaussian elimination function. We update the values in-place to avoid extra memory allocations.\n\ndef gaussian_elimination(A, b, verbose=False):\n    \"\"\"\n    Perform Gaussian elimination to reduce a linear system to upper triangular\n    form.\n\n    This function performs **forward elimination** to transform the coefficient\n    matrix `A` into an upper triangular matrix, while applying the same\n    operations to the right-hand side vector `b`. This is the first step in\n    solving a linear system of equations of the form ``Ax = b`` using Gaussian\n    elimination.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient matrix\n        of the system.\n    b : numpy.ndarray\n        A 2D NumPy array of shape ``(n, 1)`` representing the right-hand side\n        vector.\n    verbose : bool, optional\n        If ``True``, prints detailed information about each elimination step,\n        including the row operations performed and the intermediate forms of\n        `A` and `b`. Default is ``False``.\n\n    Returns\n    -------\n    None\n        This function modifies `A` and `b` **in place** and does not return\n        anything.\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"  row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print()\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nWe can try our code on Example 1:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"final system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\n# test that A is really upper triangular\nprint(\"Is A really upper triangular?\", np.allclose(A, np.triu(A)))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nfinal system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nIs A really upper triangular? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#solving-triangular-systems-of-equations",
    "href": "src/lec04.html#solving-triangular-systems-of-equations",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.4 Solving triangular systems of equations",
    "text": "4.4 Solving triangular systems of equations\nA general lower triangular system of equations has \\(a_{ij} = 0\\) for \\(j &gt; i\\) and takes the form:\n\\[\n\\begin{pmatrix}\na_{11} & 0 & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & 0 & \\cdots & 0 \\\\\na_{31} & a_{32} & a_{33} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n\n\\end{pmatrix}.\n\\]\nNote that first equation is\n\\[\na_{11} x_1 = b_1.\n\\]\nThen \\(x_i\\) can be found by calculating\n\\[\nx_i = \\frac{1}{a_{ii}} \\left(b_i - \\sum_{j=1}^{i-1} a_{ij} x_j \\right)\n\\]\nfor each row \\(i = 1, 2, \\ldots, n\\) in turn.\n\nEach calculation requires only previously computed values \\(x_j\\) (and the sum gives a loop for \\(j &lt; i\\).\nThe matrix \\(A\\) must have non-zero diagonal entries\ni.e. \\(a_{ii} \\neq 0\\) for \\(i = 1, 2, \\ldots, n\\).\nUpper triangular systems of equations can be solved in similarly.\n\n\nExample 4.3 Solve the lower triangular system of equations given by\n\\[\n\\begin{aligned}\n2 x_1 && && &= 2 \\\\\nx_1 &+& 2 x_2 && &= 7 \\\\\n2 x_1 &+& 4 x_2 &+& 6 x_3 &= 26\n\\end{aligned}\n\\]\nor, equivalently,\n\\[\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 7 \\\\ 26\n\\end{pmatrix}.\n\\]\nThe solution can be calculated systematically from\n\\[\n\\begin{aligned}\nx_1 &= \\frac{b_1}{a_{11}} = \\frac{2}{2} = 1 \\\\\nx_2 &= \\frac{b_2 - a_{21} x_1}{a_{22}}\n= \\frac{7 - 1 \\times 1}{2} = \\frac{6}{2} = 3 \\\\\nx_3 &= \\frac{b_3 - a_{31} x_1 - a_{32} x_2}{a_33}\n= \\frac{26 - 2 \\times 1 - 4 \\times 3}{6}  = \\frac{12}{6}\n= 2\n\\end{aligned}\n\\]\nwhich gives the solution \\(\\vec{x} = (1, 3, 2)^T\\).\n\n\nExercise 4.3 Solve the upper triangular linear system given by\n\\[\n\\begin{aligned}\n2 x_1 &+& x_2 &+& 4 x_3 &=& 12 \\\\\n&& 1.5 x_2 && &=& 3 \\\\\n&& && 2 x_3 &=& 4\n\\end{aligned}.\n\\]\n\n\nRemark 4.1. \n\nIt is simple to solve a lower (upper) triangular system of equations (provided the diagonal is non-zero).\nThis process is often referred to as forward (backwards) substitution.\nA general system of equations (i.e. a full matrix \\(A\\)) can be solved rapidly once it has been reduced to upper triangular form. The entire process is called Gaussian elimination with backward substitution.\n\n\nWe can define functions to solve both upper and lower triangular form systems of linear equations:\n\ndef forward_substitution(A, b):\n    \"\"\"\n    Solve a lower triangular system of linear equations using forward\n    substitution.\n\n    This function solves the system of equations:\n\n    .. math::\n        A x = b\n\n    where `A` is a **lower triangular matrix** (all elements above the main\n    diagonal are zero). The solution vector `x` is computed sequentially by\n    solving each equation starting from the first row.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the lower triangular\n        coefficient matrix of the system.\n    b : numpy.ndarray\n        A 1D NumPy array of shape ``(n,)`` or a 2D NumPy array of shape\n        ``(n, 1)`` representing the right-hand side vector.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        A NumPy array of shape ``(n,)`` containing the solution vector.\n    \"\"\"\n    \"\"\"\n    solves the system of linear equationa Ax = b assuming that A is lower\n    triangular. returns the solution x\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is lower triangular\n    if not np.allclose(A, np.tril(A)):\n        raise ValueError(\"Matrix A is not lower triangular\")\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform forwards solve\n    for i in range(n):\n        partial_sum = 0.0\n        for j in range(0, i):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\n\ndef backward_substitution(A, b):\n    \"\"\"\n    Solve an upper triangular system of linear equations using backward\n    substitution.\n\n    This function solves the system of equations:\n\n    .. math::\n        A x = b\n\n    where `A` is an **upper triangular matrix** (all elements below the main\n    diagonal are zero). The solution vector `x` is computed starting from the\n    last equation and proceeding backward.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the upper triangular\n        coefficient matrix of the system.\n    b : numpy.ndarray\n        A 1D NumPy array of shape ``(n,)`` or a 2D NumPy array of shape\n        ``(n, 1)`` representing the right-hand side vector.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        A NumPy array of shape ``(n,)`` containing the solution vector.\n    \"\"\"\n    # get size of system\n    n = system_size(A, b)\n\n    # check is upper triangular\n    assert np.allclose(A, np.triu(A))\n\n    # create solution variable\n    x = np.empty_like(b)\n\n    # perform backwards solve\n    for i in range(n - 1, -1, -1):  # iterate over rows backwards\n        partial_sum = 0.0\n        for j in range(i + 1, n):\n            partial_sum += A[i, j] * x[j]\n        x[i] = 1.0 / A[i, i] * (b[i] - partial_sum)\n\n    return x\n\nAnd we can then test it out!\n\nA = np.array([[2.0, 0.0, 0.0], [1.0, 2.0, 0.0], [2.0, 4.0, 6.0]])\nb = np.array([[2.0], [7.0], [26.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using forward substitution\")\nx = forward_substitution(A, b)\nprint()\n\nprint(\"The solution using forward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  0.0,  0.0 ]\n    [  1.0,  2.0,  0.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [  2.0 ]\n    [  7.0 ]\n    [ 26.0 ]\n\nSolving the system using forward substitution\n\nThe solution using forward substitution is:\nx = [  1.0 ]\n    [  3.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True\n\n\nWe can also do a backward substitution test:\n\nA = np.array([[2.0, 1.0, 4.0], [0.0, 1.5, 0.0], [0.0, 0.0, 2.0]])\nb = np.array([[12.0], [3.0], [4.0]])\n\nprint(\"The system is given by:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"The solution using backward substitution is:\")\nprint_array(x)\nprint()\n\nprint(\"Does x really solve the system?\", np.allclose(A @ x, b))\n\nThe system is given by:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nThe solution using backward substitution is:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the system? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "href": "src/lec04.html#combining-gaussian-elimination-and-backward-substitution",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.5 Combining Gaussian elimination and backward substitution",
    "text": "4.5 Combining Gaussian elimination and backward substitution\nOur grand strategy can now come together so we have a method to solve systems of linear equations:\nGiven a system of linear equations \\(A\\vec{x} = \\vec{b}\\);\n\nFirst perform Gaussian elimination to give an equivalent system of equations in upper triangular form;\nThen use backward substitution to produce a solution \\(\\vec{x}\\)\n\nWe can use our code to test this:\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian Elimination\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"Solving the system using backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[2.0, 1.0, 4.0], [1.0, 2.0, 2.0], [2.0, 4.0, 6.0]])\nb = np.array([[12.0], [9.0], [22.0]])\nprint(\"Does x really solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [  2.0,  1.0,  4.0 ]\n    [  1.0,  2.0,  2.0 ]\n    [  2.0,  4.0,  6.0 ]\nb = [ 12.0 ]\n    [  9.0 ]\n    [ 22.0 ]\n\nperforming Gaussian Elimination\neliminating column 0\n  row 1 |-&gt; row 1 - 0.5 * row 0\n  row 2 |-&gt; row 2 - 1.0 * row 0\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  3.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [ 10.0 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - 2.0 * row 1\n\nnew system\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\n\nupper triangular system:\nA = [  2.0,  1.0,  4.0 ]\n    [  0.0,  1.5,  0.0 ]\n    [  0.0,  0.0,  2.0 ]\nb = [ 12.0 ]\n    [  3.0 ]\n    [  4.0 ]\n\nSolving the system using backward substitution\n\nsolution using backward substitution:\nx = [  1.0 ]\n    [  2.0 ]\n    [  2.0 ]\n\nDoes x really solve the original system? True\n\n\n\nExercise 4.4 Use Gaussian elimination followed by backwards elimination to solve the system: \\[\\begin{equation*}\n\\begin{pmatrix}\n2 & 1 & -1 \\\\\n4 & 5 & 7 \\\\\n-8 & -3 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n6 \\\\ 30 \\\\ -22\n\\end{pmatrix}.\n\\end{equation*}\\]. $$",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#the-cost-of-gaussian-elimination",
    "href": "src/lec04.html#the-cost-of-gaussian-elimination",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.6 The cost of Gaussian Elimination",
    "text": "4.6 The cost of Gaussian Elimination\nGaussian elimination (GE) is unnecessarily expensive when it is applied to many systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).\n\nThe forward elimination process is the most computationally expensive part at \\(O(n^3)\\) but is exactly the same for any choice of \\(\\vec{b}\\).\nIn contrast, the solution of the resulting upper triangular system only requires \\(O(n^2)\\) operations.\n\n\n\n\n\n\nRuntime cost of applying Gaussian elimination (GE) and backwards solution (BS).\n\n\n\n\nWe can use this information to improve the way in which we solve multiple systems of equations with the same matrix \\(A\\) but different right-hand sides \\(\\vec{b}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#sec-LU-factorisation",
    "href": "src/lec04.html#sec-LU-factorisation",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.7 LU factorisation",
    "text": "4.7 LU factorisation\nOur next algorithm, called LU factorisation, is a way to try to speed up Gaussian elimination by reusing information. This can be used when we solve systems of equations with the same matrix \\(A\\) but different right hand sides \\(\\vec{b}\\) - this is more common than you would think!\nRecall the elementary row operations (EROs) from above. Note that the EROs can be produced by left multiplication with a suitable matrix:\n\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c \\\\ g & h & i \\\\ d & e & f\n\\end{pmatrix}\n\\]\nRow swap:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c & d \\\\ e & f & g & h \\\\ i & j & k & l \\\\ m & n & o & p\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c & d \\\\ i & j & k & l \\\\ e & f & g & h \\\\ m & n & o & p\n\\end{pmatrix}\n\\]\nMultiply row by \\(\\alpha\\):\n\\[\n\\begin{pmatrix}\n\\alpha & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\alpha a & \\alpha b & \\alpha c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n\\]\n\\(\\alpha \\times \\text{row } p + \\text{row } q\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\alpha & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ g & h & i\n\\end{pmatrix}\n= \\begin{pmatrix}\na & b & c \\\\ d & e & f \\\\ \\alpha a + g & \\alpha b + h & \\alpha c + i\n\\end{pmatrix}\n\\]\n\nSince Gaussian elimination (GE) is just a sequence of EROs and each ERO is just multiplication by a suitable matrix, say \\(E_k\\), forward elimination applied to the system \\(A \\vec{x} = \\vec{b}\\) can be expressed as \\[\n(E_m \\cdots E_1) A \\vec{x} = (E_m \\cdots E_1) \\vec{b},\n\\] here \\(m\\) is the number of EROs required to reduce the upper triangular form.\nLet \\(U = (E_m \\cdots E_1) A\\) and \\(L = (E_m \\cdots E_1)^{-1}\\). Now the original system \\(A \\vec{x} = \\vec{b}\\) is equivalent to\n\\[\\begin{equation}\n\\label{eq:LU}\nL U \\vec{x} = \\vec{b}\n\\end{equation}\\]\nwhere \\(U\\) is upper triangular (by construction) and \\(L\\) may be shown to be lower triangular (provided the EROs do not include any row swaps).\nOnce \\(L\\) and \\(U\\) are known it is easy to solve \\(\\eqref{eq:LU}\\)\n\nSolve \\(L \\vec{z} = \\vec{b}\\) in \\(O(n^2)\\) operations.\nSolve \\(U \\vec{x} = \\vec{z}\\) in \\(O(n^2)\\) operations.\n\n\\(L\\) and \\(U\\) may be found in \\(O(n^3)\\) operations by performing GE and saving the \\(E_i\\) matrices: however, it is more convenient to find them directly (also \\(O(n^3)\\) operations).\n\n4.7.1 Computing \\(L\\) and \\(U\\)\nConsider a general \\(4 \\times 4\\) matrix \\(A\\) and its factorisation \\(LU\\):\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\nl_{21} & 1 & 0 & 0 \\\\\nl_{31} & l_{32} & 1 & 0 \\\\\nl_{41} & l_{42} & l_{43} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & u_{14} \\\\\n0 & u_{22} & u_{23} & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}\n\\]\nFor the first column,\n\\[\n\\begin{aligned}\na_{11} & = (1, 0, 0, 0) (u_{11}, 0, 0, 0)^T && = u_{11}\n& \\rightarrow u_{11} & = a_{11} \\\\\na_{21} & = (l_{21}, 1, 0, 0)(u_{11}, 0, 0, 0)^T && = l_{21} u_{11}\n& \\rightarrow l_{21} & = a_{21} / u_{11} \\\\\na_{31} & = (l_{31}, l_{32}, 1, 0)(u_{11}, 0, 0, 0)^T && = l_{31} u_{11}\n& \\rightarrow l_{31} & = a_{31} / u_{11} \\\\\na_{41} & = (l_{41}, l_{42}, l_{43}, 1)(u_{11}, 0, 0, 0)^T && = l_{41} u_{11}\n& \\rightarrow l_{41} & = a_{41} / u_{11}\n\\end{aligned}\n\\]\nThe second, third and fourth columns follow in a similar manner, giving all the entries in \\(L\\) and \\(U\\).\n\nRemark 4.2. \n\n\\(L\\) is assumed to have 1’s on the diagonal, to ensure that the factorisation is unique.\nThe process involves division by the diagonal entries \\(u_{11}, u_{22}\\), etc., so they must be non-zero.\nIn general the factors \\(l_{ij}\\) and \\(u_{ij}\\) are calculated for each column \\(j\\) in turn, i.e.,\nfor j in range(n):\n  for i in range(j+1):\n      # Compute factors u_{ij}\n      ...\n  for i in range(j+1, n):\n      # Compute factors l_{ij}\n      ...\n\n\n\nExample 4.4 Use \\(LU\\) factorisation to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}.\n\\]\nThis can be rewritten in the form \\(A = LU\\) where\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{pmatrix}.\n\\]\nColumn 1 of \\(A\\) gives\n\\[\n\\begin{aligned}\n2 & = u_{11} && \\rightarrow & u_{11} & = 2 \\\\\n1 & = l_{21} u_{11} && \\rightarrow & l_{21} & = 0.5 \\\\\n2 & = l_{31} u_{11} && \\rightarrow & l_{31} & = 1.\n\\end{aligned}\n\\]\nColumn 2 of \\(A\\) gives\n\\[\n\\begin{aligned}\n1 & = u_{12} && \\rightarrow & u_{12} & = 1 \\\\\n2 & = l_{21} u_{12} + u_{22} && \\rightarrow & u_{22} & = 1.5 \\\\\n4 & = l_{31} u_{12} + l_{32} u_{22} && \\rightarrow & l_{32} & = 2.\n\\end{aligned}\n\\]\nColumn 3 of \\(A\\) gives\n\\[\n\\begin{aligned}\n4 & = u_{13} && \\rightarrow & u_{13} & = 4 \\\\\n2 & = l_{21} u_{13} + u_{23} && \\rightarrow & u_{23} & = 0 \\\\\n6 & = l_{31} u_{13} + l_{32} u_{23} + u_{33} && \\rightarrow & u_{33} & = 2.\n\\end{aligned}\n\\]\nSolve the lower triangular system \\(L \\vec{z} = \\vec{b}\\):\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 9 \\\\ 22\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nz_1 \\\\ z_2 \\\\ z_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\]\nSolve the upper triangular system \\(U \\vec{x} = \\vec{z}\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n12 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\\\ 2 \\\\ 2\n\\end{pmatrix}.\n\\]\n\n\nExercise 4.5 Rewrite the matrix \\(A\\) as the product of lower and upper triangular matrices where\n\\[\nA =\n\\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & 1 & 2.5\n\\end{pmatrix}.\n\\]\n\n\nRemark. The first example gives\n\\[\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n1 & 2 & 2 \\\\\n2 & 4 & 6\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0.5 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 1 & 4 \\\\\n0 & 1.5 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nNote that\n\nthe matrix \\(U\\) is the same as the fully eliminated upper triangular form produced by Gaussian elimination;\n\\(L\\) contains the multipliers that were used at each stage to eliminate the rows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "href": "src/lec04.html#effects-of-finite-precision-arithmetic",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.8 Effects of finite precision arithmetic",
    "text": "4.8 Effects of finite precision arithmetic\n\nExample 4.5 Consider the following linear system of equations\n\\[\n\\begin{pmatrix}\n0 & 2 & 1 \\\\\n2 & 1 & 0 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 4 \\\\ 5\n\\end{pmatrix}\n\\]\nProblem. We cannot eliminate the first column by the diagonal by adding multiples of row 1 to rows 2 and 3, respectively.\nSolution. Swap the order of the equations!\n\nSwap rows 1 and 2:\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 \\\\ 7 \\\\ 5\n\\end{pmatrix}\n\\]\nNow apply Gaussian elimination\n\\[\\begin{align*}\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 1.5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n&= \\begin{pmatrix}\n4 \\\\ 7 \\\\ 3\n\\end{pmatrix} \\\\\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & -0.75\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n&= \\begin{pmatrix}\n4 \\\\ 7 \\\\ -2.25\n\\end{pmatrix}.\n\\end{align*}\\]\n\n\n\nExample 4.6 Consider another system of equations\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n4 & 2 & 1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 5 \\\\ 2\n\\end{pmatrix}\n\\]\n\nApply Gaussian elimination as usual:\n\\[\\begin{align*}\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n2 & 2 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ 2\n\\end{pmatrix}\n\\\\\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\end{align*}\\]\nProblem. We cannot eliminate the second column below the diagonal by adding a multiple of row 2 to row 3.\nAgain, this problem may be overcome simply by swapping the order of the equations – this time, swapping rows 2 and 3:\n\\[\n\\begin{pmatrix}\n2 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ -1 \\\\ -1\n\\end{pmatrix}\n\\]\nWe can now continue the Gaussian elimination process as usual.\n\n\nIn general. Gaussian elimination requires row swaps to avoid breaking down when there is a zero in the “pivot” position. Avoiding zero-division might be a familiar aspect of Gaussian elimination, but there is an additional reason to apply pivoting when working with floating point numbers:\n\nExample 4.7 Consider using Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 + \\varepsilon \\\\ 3\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true, unique solution is \\((x_1, x_2)^T = (1, 2)^T\\).\nIf \\(\\varepsilon \\neq 0\\), Gaussian elimination gives\n\\[\n\\begin{pmatrix}\n\\varepsilon & 1 \\\\\n0 & 1 - \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 + \\varepsilon \\\\ 3 - \\frac{2 + \\varepsilon}{\\varepsilon}\n\\end{pmatrix}\n\\]\nProblems occur not only when \\(\\varepsilon = 0\\) but also when it is very small, i.e. when \\(\\frac{1}{\\varepsilon}\\) is very large, which will introduce very significant rounding errors into the computation.\n\nUse Gaussian elimination to solve the linear system of equations given by\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n\\varepsilon & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 2 + \\varepsilon\n\\end{pmatrix}\n\\]\nwhere \\(\\varepsilon \\neq 1\\).\n\nThe true solution is still \\((x_1, x_2)^T = (1, 2)^T\\).\nGaussian elimination now gives\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1 - \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 \\\\ 2 - 2\\varepsilon\n\\end{pmatrix}\n\\]\nThe problems due to small values of \\(\\varepsilon\\) have disappeared.\n\nThis is a genuine problem we see in the code versions too!\n\nprint(\"without row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[eps, 1.0], [1.0, 1.0]])\n    b = np.array([[2.0 + eps], [3.0]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nprint(\"with row swapping:\")\nfor eps in [1.0e-2, 1.0e-4, 1.0e-6, 1.0e-8, 1.0e-10, 1.0e-12, 1.0e-14]:\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n\n    gaussian_elimination(A, b)\n    x = backward_substitution(A, b)\n    print(f\"{eps=:.1e}\", end=\", \")\n    print_array(x.T, \"x.T\", end=\", \")\n\n    A = np.array([[1.0, 1.0], [eps, 1.0]])\n    b = np.array([[3.0], [2.0 + eps]])\n    print(\"Solution?\", np.allclose(A @ x, b))\nprint()\n\nwithout row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.00009,  2.00000 ], Solution? False\neps=1.0e-14, x.T = [  1.0214,  2.0000 ], Solution? False\n\nwith row swapping:\neps=1.0e-02, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-04, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-06, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-08, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-10, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-12, x.T = [  1.0,  2.0 ], Solution? True\neps=1.0e-14, x.T = [  1.0,  2.0 ], Solution? True\n\n\n\n\n\nRemark 4.3. \n\nWriting the equations in a different order has removed the previous problem.\nThe diagonal entries are now always relatively larger.\nThe interchange of the order of equations is a simple example of row pivoting. This strategy avoids excessive rounding errors in the computations.\n\n\n\n4.8.1 Gaussian elimination with pivoting\nKey idea:\n\nBefore eliminating entries in column \\(j\\):\n\nfind the entry in column \\(j\\), below the diagonal, of maximum magnitude;\nif that entry is larger in magnitude than the diagonal entry then swap its row with row \\(j\\).\n\nThen eliminate column \\(j\\) as before.\n\nThis algorithm will always work when the matrix \\(A\\) is non-singular. Conversely, if all of the possible pivot values are zero this implies that the matrix is singular and a unique solution does not exist. At each elimination step the row multiplies used are guaranteed to be at most one in magnitude so any errors in the representation of the system cannot be amplified by the elimination process. As always, solving \\(A \\vec{x} = \\vec{b}\\) requires that the entries in \\(\\vec{b}\\) are also swapped in the appropriate way.\nPivoting can be applied in an equivalent way to LU factorisation. The sequence of pivots is independent of the vector \\(\\vec{b}\\) and can be recorded and reused. The constraint imposed on the row multipliers means that for LU factorisation every entry in \\(L\\) satisfies \\(| l_{ij} | \\le 1\\).\n\nExample 4.8 Consider the linear system of equations given by\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n-3 & 2.1 - \\varepsilon & 6 \\\\\n5 & -1 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 9.9 + \\varepsilon \\\\ 11\n\\end{pmatrix}\n\\]\nwhere \\(0 \\le \\varepsilon \\ll 1\\), and solve it using\n\nGaussian elimination without pivoting\nGaussian elimination with pivoting.\n\nThe exact solution is \\(\\vec{x} = (0, -1, 2)^T\\) for any \\(\\varepsilon\\) in the given range.\n1. Solve the system using Gaussian elimination with no pivoting.\nEliminating the first column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nand then the second column gives\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 0 & 5 + 15/\\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5 + 2.5(12 + \\varepsilon)/\\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to\n\\[\nx_3 = \\frac{3 + \\frac{12 + \\varepsilon}{\\varepsilon}}{2 + \\frac{6}{\\varepsilon}}\n\\qquad\nx_2 = \\frac{(12 + \\varepsilon) - 6x_3}{-\\varepsilon} \\qquad\nx_1 = \\frac{7+ 7x_2}{10}.\n\\]\nThere are many divisions by \\(\\varepsilon\\), so we will have problems if \\(\\varepsilon\\) is (very) small.\n2. Solve the system using Gaussian elimination with pivoting.\nThe first stage is identical (because \\(a_{11} = 10\\) is largest).\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & -\\varepsilon & 6 \\\\\n0 & 2.5 & 5\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 12 + \\varepsilon \\\\ 7.5\n\\end{pmatrix}\n\\]\nbut now \\(|a_{22}| = \\varepsilon\\) and \\(|a_{32}| = 2.5\\) so we swap rows 2 and 3 to give\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & -\\varepsilon & 6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + \\varepsilon\n\\end{pmatrix}\n\\]\nNow we may eliminate column 2:\n\\[\n\\begin{pmatrix}\n10 & -7 & 0 \\\\\n0 & 2.5 & 5 \\\\\n0 & 0 & 6 + 2 \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n= \\begin{pmatrix}\n7 \\\\ 7.5 \\\\ 12 + 4 \\varepsilon\n\\end{pmatrix}\n\\]\nwhich leads to the exact answer:\n\\[\nx_3 = \\frac{12 + 4\\varepsilon}{6 + 2 \\varepsilon} = 2 \\qquad\nx_2 = \\frac{7.5 - 5x_3}{2.5} = -1 \\qquad\nx_1 = \\frac{7 + 7 x_2}{10} = 0.\n\\]\n\n\n\n4.8.2 Python code for Gaussian elimination with pivoting\n\ndef gaussian_elimination_with_pivoting(A, b, verbose=False):\n    \"\"\"\n    perform Gaussian elimnation with pivoting to reduce the system of linear\n    equations Ax=b to upper triangular form.\n    use verbose to print out intermediate representations\n    \"\"\"\n    # find shape of system\n    n = system_size(A, b)\n\n    # perform forwards elimination\n    for i in range(n - 1):\n        # eliminate column i\n        if verbose:\n            print(f\"eliminating column {i}\")\n\n        # find largest entry in column i\n        largest = abs(A[i, i])\n        j_max = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) &gt; largest:\n                largest, j_max = abs(A[j, i]), j\n\n        # swap rows j_max and i\n        row_swap(A, b, i, j_max)\n        if verbose:\n            print(f\"swapped system ({i} &lt;-&gt; {j_max})\")\n            print_array(A)\n            print_array(b)\n            print()\n\n        for j in range(i + 1, n):\n            # row j\n            factor = A[j, i] / A[i, i]\n            if verbose:\n                print(f\"row {j} |-&gt; row {j} - {factor} * row {i}\")\n            row_add(A, b, j, -factor, i)\n\n        if verbose:\n            print(\"new system\")\n            print_array(A)\n            print_array(b)\n            print()\n\nGaussian elimination without pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination without pivoting\")\ngaussian_elimination(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination without pivoting\neliminating column 0\n  row 1 |-&gt; row 1 - -0.3 * row 0\n  row 2 |-&gt; row 2 - 0.5 * row 0\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\n  row 2 |-&gt; row 2 - -244760849313613.9 * row 1\n\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  0.0, 1468565095881688.5 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [ 2937130191763377.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [ -0.030435 ]\n    [ -1.043478 ]\n    [  2.000000 ]\n\nDoes x solve the original system? False\n\n\nGaussian elimination with pivoting following by back subsitution:\n\neps = 1.0e-14\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\n\nprint(\"starting system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing Gaussian elimination with pivoting\")\ngaussian_elimination_with_pivoting(A, b, verbose=True)\nprint()\n\nprint(\"upper triangular system:\")\nprint_array(A)\nprint_array(b)\nprint()\n\nprint(\"performing backward substitution\")\nx = backward_substitution(A, b)\nprint()\n\nprint(\"solution using backward substitution:\")\nprint_array(x)\nprint()\n\nA = np.array([[10.0, -7.0, 0.0], [-3.0, 2.1 - eps, 6.0], [5.0, -1.0, 5.0]])\nb = np.array([[7.0], [9.9 + eps], [11.0]])\nprint(\"Does x solve the original system?\", np.allclose(A @ x, b))\n\nstarting system:\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nperforming Gaussian elimination with pivoting\neliminating column 0\nswapped system (0 &lt;-&gt; 0)\nA = [ 10.0, -7.0,  0.0 ]\n    [ -3.0,  2.1,  6.0 ]\n    [  5.0, -1.0,  5.0 ]\nb = [  7.0 ]\n    [  9.9 ]\n    [ 11.0 ]\n\nrow 1 |-&gt; row 1 - -0.3 * row 0\nrow 2 |-&gt; row 2 - 0.5 * row 0\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0, -0.0,  6.0 ]\n    [  0.0,  2.5,  5.0 ]\nb = [  7.0 ]\n    [ 12.0 ]\n    [  7.5 ]\n\neliminating column 1\nswapped system (1 &lt;-&gt; 2)\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0, -0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nrow 2 |-&gt; row 2 - -4.085620730620576e-15 * row 1\nnew system\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\n\nupper triangular system:\nA = [ 10.0, -7.0,  0.0 ]\n    [  0.0,  2.5,  5.0 ]\n    [  0.0,  0.0,  6.0 ]\nb = [  7.0 ]\n    [  7.5 ]\n    [ 12.0 ]\n\nperforming backward substitution\n\nsolution using backward substitution:\nx = [  0.0 ]\n    [ -1.0 ]\n    [  2.0 ]\n\nDoes x solve the original system? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec04.html#further-reading",
    "href": "src/lec04.html#further-reading",
    "title": "4  Direct solvers for systems of linear equations",
    "section": "4.9 Further reading",
    "text": "4.9 Further reading\nSome basic reading:\n\nWikipedia: Gaussian elimination\nJoseph F. Grcar. How ordinary elimination became Gaussian elimination. Historia Mathematica. Volume 38, Issue 2, May 2011. (More history)\n\nSome reading on LU factorisation:\n\n\\(A = LU\\) and solving systems [pdf]\nWikipedia: LU decomposition\nWikipedia: Matrix decomposition (Other examples of decompositions).\nNick Higham: What is an LU factorization? (a very mathematical treatment with additional references)\n\nSome reading on using Gaussian elimination with pivoting:\n\nGaussian elimination with Partial Pivoting [pdf]\nGaussian elimination with partial pivoting example [pdf]\n\nA good general reference for this area:\n\nTrefethen, Lloyd N.; Bau, David (1997), Numerical linear algebra, Philadelphia: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9.\n\nSome implementations:\n\nNumpy numpy.linalg.solve\nScipy scipy.linalg.lu\nLAPACK Gaussian elimination (uses LU factorisation): dgesv()\nLAPACK LU Factorisation: dgetrf().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Direct solvers for systems of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html",
    "href": "src/lec05.html",
    "title": "5  Iterative solutions of linear equations",
    "section": "",
    "text": "5.1 Iterative methods\nIn the previous section we looked at what are known as direct methods for solving systems of linear equations. They are guaranteed to solve with a fixed amount of work (we can even prove this in exact arithmetic!), but this fixed amount of work may be very large.\nFor a general \\(n \\times n\\) system of linear equations \\(A \\vec{x} = \\vec{b}\\), the computation expense of all direct methods if \\(O(n^3)\\). The amount of storage required for these approaches is \\(O(n^2)\\) which is dominated by the cost of storing the matrix \\(A\\). As \\(n\\) becomes larger the storage and computation work required limit the practicality of direct approaches.\nAs an alternative, we will propose some iterative methods. Iterative methods produce a sequence \\((\\vec{x}^{(k)})\\) of approximations to the solution of the linear system of equations \\(A \\vec{x} = \\vec{b}\\). The iteration is defined recursively and is typically of the form: \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)}),\n\\] where \\(\\vec{x}^{(k)}\\) is now a vector of values and \\(\\vec{F}\\) is some vector function (which needs to be defined to define the method). We will need to choose a starting value \\(\\vec{x}^{(k)}\\) but there is often a reasonable approximation which can be used. Once all this is defined, we still need to decide when we need to stop!\nThe key point here is that we want a method that is both computationally cheap and converges quickly to the solution. One way to do this is to construct an iteration given by\n\\[\\begin{equation}\n\\label{eq:general-iteration}\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + P (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{equation}\\]\nfor some matrix \\(P\\) such that\nWe call \\(\\vec{b} - A \\vec{x}^{(k)} = \\vec{r}\\) the residual. Note that the above bad examples could be written in the form of \\(\\eqref{eq:general-iteration}\\) with \\(P = O\\) (the zero matrix) or \\(P = A^{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#iterative-methods",
    "href": "src/lec05.html#iterative-methods",
    "title": "5  Iterative solutions of linear equations",
    "section": "",
    "text": "Remark 5.1. We use a value in brackets in the superscript to denote the iteration number to avoid confusion between the iteration number and the component of the vector: \\[\n\\vec{x}^{(k)} = (\\vec{x}^{(k)}_1, \\vec{x}^{(k)}_2, \\ldots, \\vec{x}^{(k)}_n).\n\\]\n\n\nExample 5.1 (Some terrible examples) These are examples of potential iterative methods which would not work very well!\n\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)}.\n\\]\nEach iteration is very cheap to compute but very inaccurate - it never converges!\nConsider\n\\[\n\\vec{F}(\\vec{x}^{(k)}) = \\vec{x}^{(k)} + A^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\]\nEach iteration is very expensive to compute - you have to invert \\(A\\)! - but it converges in just one step since\n\\[\n\\begin{aligned}\nA \\vec{x}^{(k+1)} & = A \\vec{x}^{(k)} + A A^{-1} (\\vec{b} - A \\vec{x}^{(k)})\n\\\\\n& = A \\vec{x}^{(k)} + \\vec{b} - A \\vec{x}^{(k)} \\\\\n& = \\vec{b}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\\(P\\) is easy to compute, or the matrix-vector product \\(\\vec{r} \\mapsto P\n\\vec{r}\\) is easy to compute,\n\\(P\\) approximates \\(A^{-1}\\) well enough that the algorithm converges in few iterations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#jacobi-iteration",
    "href": "src/lec05.html#jacobi-iteration",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.2 Jacobi iteration",
    "text": "5.2 Jacobi iteration\nOne straightforward choice for \\(P\\) in \\(\\eqref{eq:general-iteration}\\) is given by the Jacobi method where we take \\(P = D^{-1}\\) where \\(D\\) is the diagonal of \\(A\\): \\[\nD_{ii} = A_{ii} \\quad \\text{and} \\quad D_{ij} = 0 \\text{ for } i \\neq j.\n\\]\nThe Jacobi iteration is given by\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)})\n\\]\n\\(D\\) is a diagonal matrix, so \\(D^{-1}\\) is trivial to form (as long as the diagonal entries are all non-zero): \\[\n(D^{-1})_{ii} = \\frac{1}{D_{ii}}\n\\quad \\text{and} \\quad\n(D^{-1})_{ij} = 0 \\text{ for } i \\neq j.\n\\]\n\nRemark. \n\nThe cost of one iteration is \\(O(n^2)\\) for a full matrix, and this is dominated by the matrix-vector product \\(A \\vec{x}^{(k)}\\).\nThis cost can be reduced to \\(O(n)\\) if the matrix \\(A\\) is sparse - this is when iterative methods are especially attractive (Example 3.8).\nThe amount of work also depends on the number of iterations required to get a “satisfactory” solution.\n\nThe number of iterations depends on the matrix;\n\nFewer iterations are needed for a less accurate solution;\n\nA good initial estimate \\(\\vec{x}^{(0)}\\) reduces the required number of iterations.\n\nUnfortunately, the iteration might not converge!\n\n\nThe Jacobi iteration updates all elements of \\(\\vec{x}^{(k)}\\) simultaneously to get \\(\\vec{x}^{(k+1)}\\). Writing the method out component by component gives\n\\[\\begin{align*}\nx_1^{(k+1)} &= x_1^{(k)} + \\frac{1}{A_{11}} \\left( b_1 - \\sum_{j=1}^n A_{1j}\nx_j^{(k)} \\right) \\\\\nx_2^{(k+1)} &= x_2^{(k)} + \\frac{1}{A_{22}} \\left( b_2 - \\sum_{j=1}^n A_{2j}\nx_j^{(k)} \\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)} &= x_n^{(k)} + \\frac{1}{A_{nn}} \\left( b_n - \\sum_{j=1}^n A_{nj}\nx_j^{(k)} \\right).\n\\end{align*}\\]\nNote that once the first step has been taken, \\(x_1^{(k+1)}\\) is already known, but the Jacobi iteration does not make use of this information!\n\nExample 5.2 Take two iterations of Jacobi iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\): \\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\), the first iteration is \\[\n\\begin{aligned}\nx_1^{(1)} &= x_1^{(0)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(0)}\n- A_{12} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{2} (3.5 - 2 \\times 1 - 1 \\times 1) = 1.25 \\\\\nx_2^{(1)} &= x_2^{(0)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(0)}\n- A_{22} x_2^{(0)} \\right) \\\\\n&= 1 + \\frac{1}{4} (0.5 - (-1) \\times 1 - 4 \\times 1) = 0.375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(1)} = (1.25, 0.375)^T\\). Then the second iteration is \\[\n\\begin{aligned}\nx_1^{(2)} &= x_1^{(1)} + \\frac{1}{A_{11}} \\left( b_1 - A_{11} x_1^{(1)} -\nA_{12} x_2^{(1)} \\right) \\\\\n&= 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.375) = 1.5625 \\\\\nx_2^{(2)} &= x_2^{(1)} + \\frac{1}{A_{22}} \\left( b_2 - A_{21} x_1^{(1)} -\nA_{22} x_2^{(1)} \\right) \\\\\n&= 0.375 + \\frac{1}{4} (0.5 - (-1) \\times 1.25 - 4 \\times 0.375) = 0.4375. \\\\\n\\end{aligned}\n\\] So we have \\(\\vec{x}^{(2)} = (1.5625, 0.4375)\\).\nNote the only difference between the formulae for Iteration 1 and 2 is the iteration number, the superscript in brackets. The exact solution is given by \\(\\vec{x} = (1.5, 0.5)^T\\).\n\nWe note that we can also slightly simplify the way the Jacobi iteration is written. We can expand \\(A\\) into \\(A = L + D + U\\), where \\(L\\) and \\(U\\) are the parts of the matrix from below and above the diagonal respectively: \\[\nL_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &lt; j \\\\\n0 &\\quad \\text{if } i \\ge j,\n\\end{cases}\n\\qquad\nU_{ij} = \\begin{cases}\nA_{ij} &\\quad \\text{if } i &gt; j \\\\\n0 &\\quad \\text{if } i \\le j.\n\\end{cases}\n\\] Then we can calculate that: \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} D \\vec{x}^{(k)} + D^{-1}(\\vec{b}\n- (L + U) \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - \\vec{x}^{(k)} + D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}) \\\\\n& = D^{-1}(\\vec{b} - (L + U) \\vec{x}^{(k)}).\n\\end{aligned}\n\\] In this formulation, we do not explicitly form the residual as part of the computations. In practical situations, this may be a simpler formulation we can use if we have knowledge of the coefficients of \\(A\\), rather than just the matrix-vector product.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#gauss-seidel-iteration",
    "href": "src/lec05.html#gauss-seidel-iteration",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.3 Gauss-Seidel iteration",
    "text": "5.3 Gauss-Seidel iteration\nAs an alternative to Jacobi iteration, the iteration might use \\(x_i^{(k+1)}\\) as soon as it is calculated (rather than using the previous iteration), giving\n\\[\\begin{align*}\nx_1^{(k+1)}\n& = x_1^{(k)} + \\frac{1}{A_{11}} \\left(\nb_1 - \\sum_{j=1}^n A_{1j} x_j^{(k)}\n\\right) \\\\\nx_2^{(k+1)}\n& = x_2^{(k)} + \\frac{1}{A_{22}} \\left(\nb_2 - A_{21} x_1^{(k+1)} - \\sum_{j=2}^n A_{2j} x_j^{(k)}\n\\right) \\\\\nx_3^{(k+1)}\n& = x_3^{(k)} + \\frac{1}{A_{33}} \\left(\nb_3 - \\sum_{j=1}^2 A_{3j} x_j^{(k+1)} - \\sum_{j=3}^n A_{3j} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_i^{(k+1)}\n& = x_i^{(k)} + \\frac{1}{A_{ii}} \\left(\nb_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i}^n A_{ij} x_j^{(k)}\n\\right) \\\\\n\\vdots \\quad & \\hphantom{=} \\quad \\vdots \\\\\nx_n^{(k+1)}\n& = x_n^{(k)} + \\frac{1}{A_{nn}} \\left(\nb_n - \\sum_{j=1}^{n-1} A_{nj} x_j^{(k+1)} - A_{nn} x_n^{(k)}\n\\right).\n\\end{align*}\\]\nConsider the system \\(A \\vec{x}= b\\) with the matrix \\(A\\) split as \\(A = L + D + U\\), where \\(D\\) is the diagonal of \\(A\\), \\(L\\) contains the elements below the diagonal, and \\(U\\) contains the elements above the diagonal. The componentwise iteration above can be written in matrix form as \\[\n\\begin{aligned}\n\\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} (\\vec{b} - L \\vec{x}^{(k+1)} - (D + U)\n\\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} (\\vec{b} - (D + U)\n\\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} - D^{-1} L \\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k)} + D^{-1}\n(\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} + D^{-1} L \\vec{x}^{(k+1)} & = \\vec{x}^{(k)} + D^{-1} L\n\\vec{x}^{(k)} + D^{-1} (\\vec{b} - (L + D + U) \\vec{x}^{(k)}) \\\\\nD^{-1} (D + L) \\vec{x}^{(k+1)}  & = D^{-1} (D + L) \\vec{x}^{(k)} + D^{-1}\n(\\vec{b} - A \\vec{x}^{(k)}) \\\\\n(D + L) \\vec{x}^{(k+1)} &= D D^{-1} (D + L) \\vec{x}^{(k)} + D D^{-1} (\\vec{b}\n- A \\vec{x}^{(k)}) \\\\\n& = (D + L) \\vec{x}^{(k)} + (\\vec{b} - A \\vec{x}^{(k)}) \\\\\n\\vec{x}^{(k+1)} &= (D + L)^{-1} (D + L) \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} -\nA \\vec{x}^{(k)}) \\\\\n& = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\end{aligned}\n\\]\n…and hence the Gauss-Seidel iteration\n\\[\n\\vec{x}^{(k+1)} = \\vec{x}^{(k)} + (D + L)^{-1} (\\vec{b} - A \\vec{x}^{(k)}).\n\\] That is, we use \\(P = (D+L)^{-1}\\) in \\(\\eqref{eq:general-iteration}\\).\nIn general, we do not form the inverse of \\(D + L\\) explicitly here since it is more complicated to do so than simply computing the inverse of \\(D\\).\n\nExample 5.3 Take two iterations of Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 1)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\ -1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3.5 \\\\ 0.5\n\\end{pmatrix}\n\\]\nStarting from \\(\\vec{x}^{(0)} = (1, 1)^T\\) we have\nIteration 1:\n\\[\n\\begin{aligned}\nx^{(1)}_1 & = x^{(0)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(0)}_1 -\nA_{12} x^{(0)}_2) \\\\\n          & = 2 + \\frac{1}{2} (3.5 - 1 \\times 2 - 1 \\times 1) = 2.25 \\\\\nx^{(1)}_2 & = x^{(0)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(1)}_1 -\nA_{22} x^{(0)}_2) \\\\\n          & = 1 + \\frac{1}{4} (0.5 - (-1) \\times 2.25 - 4 \\times 1) = 0.6875.\n\\end{aligned}\n\\]\nIteration 2:\n\\[\n\\begin{aligned}\nx^{(2)}_1 & = x^{(1)}_1 + \\frac{1}{A_{11}} (b_1 - A_{11} x^{(1)}_1 a\n- A_{12} x^{(1)}_2) \\\\\n          & = 1.25 + \\frac{1}{2} (3.5 - 2 \\times 1.25 - 1 \\times 0.4375)\n          = 1.53125 \\\\\nx^{(2)}_2 & = x^{(1)}_2 + \\frac{1}{A_{22}} (b_2 - A_{21} x^{(2)}_1\n- A_{22} x^{(1)}_2) \\\\\n          & = 0.4375 + \\frac{1}{4} (0.5 - (-1) \\times 1.53125 - 4 \\times 0.4375)\n          = 0.5078125.\n\\end{aligned}\n\\]\nAgain, note the changes in the iteration number on the right-hand side of these equations, especially the differences against the Jacobi method.\n\nWhat happens if the initial estimate is altered to \\(\\vec{x}^{(0)} = (2, 1)^T\\).\n\n\n\nExercise 5.1 Take one iteration of (a) Jacobi iteration; (b) Gauss-Seidel iteration to approximate the solution of the following system using the initial guess \\(\\vec{x}^{(0)} = (1, 2, 3)^T\\):\n\\[\n\\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 10 \\\\ 6\n\\end{pmatrix}.\n\\]\nNote that the exact solution to this system is \\(x_1 = 2, x_2 = 2, x_3 = 2\\).\n\n\nRemark. \n\nHere, both methods converge, but relatively slowly. They might not converge at all!\nWe will discuss convergence and when to stop later.\nThe Gauss-Seidel iteration generally out-performs the Jacobi iteration.\nPerformance can depend on the order in which the equations are written.\nBoth iterative algorithms can be made faster and more efficient for sparse systems of equations (far more than direct methods).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#python-version-of-iterative-methods",
    "href": "src/lec05.html#python-version-of-iterative-methods",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.4 Python version of iterative methods",
    "text": "5.4 Python version of iterative methods\n\ndef jacobi_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    Solve a linear system Ax = b using the Jacobi iterative method.\n\n    The Jacobi method is an iterative algorithm for solving the linear system:\n\n    .. math::\n        Ax = b\n\n    starting from an initial guess ``x0``. At each iteration, the solution is\n    updated according to:\n\n    .. math::\n        x_i^{(k+1)} = x_i^{(k)} + \\\\frac{1}{A_{ii}} \\\\left( b_i -\n        \\\\sum_{j=0}^{n-1} A_{ij} x_j^{(k)} \\\\right)\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient\n        matrix.\n    b : numpy.ndarray\n        A 1D or 2D NumPy array of shape ``(n,)`` or ``(n, 1)`` representing the\n        right-hand side vector.\n    x0 : numpy.ndarray\n        Initial guess for the solution, same shape as ``b``.\n    max_iter : int\n        Maximum number of iterations to perform.\n    verbose : bool, optional\n        If ``True``, prints the value of the solution vector at each iteration.\n        Default is ``False``.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        Approximated solution vector after ``max_iter`` iterations.\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration(A, b, x0, max_iter, verbose=False):\n    \"\"\"\n    Solve a linear system Ax = b using the Gauss-Seidel iterative method.\n\n    The Gauss-Seidel method is an iterative algorithm for solving the linear\n    system:\n\n    .. math::\n        Ax = b\n\n    starting from an initial guess ``x0``. At each iteration, the solution is\n    updated sequentially using the most recently computed values:\n\n    .. math::\n        x_i^{(k+1)} = x_i^{(k)} + \\\\frac{1}{A_{ii}} \\\\left( b_i -\n        \\\\sum_{j=0}^{i-1} A_{ij} x_j^{(k+1)} -\n        \\\\sum_{j=i}^{n-1} A_{ij} x_j^{(k)} \\\\right)\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` representing the coefficient\n        matrix.\n    b : numpy.ndarray\n        A 1D or 2D NumPy array of shape ``(n,)`` or ``(n, 1)`` representing the\n        right-hand side vector.\n    x0 : numpy.ndarray\n        Initial guess for the solution, same shape as ``b``.\n    max_iter : int\n        Maximum number of iterations to perform.\n    verbose : bool, optional\n        If ``True``, prints the value of the solution vector at each iteration.\n        Default is ``False``.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        Approximated solution vector after ``max_iter`` iterations.\n\n    \"\"\"\n    n = system_size(A, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        for i in range(n):\n            Axi = 0.0\n            for j in range(i):\n                Axi += A[i, j] * xnew[j]\n            for j in range(i, n):\n                Axi += A[i, j] * x[j]\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Axi)\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\nA = np.array([[2.0, 1.0], [-1.0, 4.0]])\nb = np.array([[3.5], [0.5]])\nx0 = np.array([[1.0], [1.0]])\n\nprint(\"jacobi iteration\")\nx = jacobi_iteration(A, b, x0, 5, verbose=True)\nprint()\n\nprint(\"gauss seidel iteration\")\nx = gauss_seidel_iteration(A, b, x0, 5, verbose=True)\nprint()\n\njacobi iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.250,  0.375 ]\nafter iter=1: x.T = [  1.5625,  0.4375 ]\nafter iter=2: x.T = [  1.53125,  0.51562 ]\nafter iter=3: x.T = [  1.49219,  0.50781 ]\nafter iter=4: x.T = [  1.49609,  0.49805 ]\n\ngauss seidel iteration\nstarting value: x.T = [  1.0,  1.0 ]\nafter iter=0: x.T = [  1.2500,  0.4375 ]\nafter iter=1: x.T = [  1.53125,  0.50781 ]\nafter iter=2: x.T = [  1.49609,  0.49902 ]\nafter iter=3: x.T = [  1.50049,  0.50012 ]\nafter iter=4: x.T = [  1.49994,  0.49998 ]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#sparse-matrices",
    "href": "src/lec05.html#sparse-matrices",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.5 Sparse Matrices",
    "text": "5.5 Sparse Matrices\nWe met sparse matrices as an example of a special matrix format when we first thought about systems of linear equations (Example 3.8). Sparse matrices are very common in applications and have a structure which is very useful when used with iterative methods. There are two main ways in which sparse matrices can be exploited in order to obtain benefits within iterative methods.\n\nThe storage can be reduced from \\(O(n^2)\\).\nThe cost per iteration can be reduced from \\(O(n^2)\\).\n\nRecall that a sparse matrix is defined to be such that it has at most \\(\\alpha n\\) non-zero entries (where \\(\\alpha\\) is independent of \\(n\\)). Typically this happens when we know there are at most \\(\\alpha\\) non-zero entries in any row.\nThe simplest way in which a sparse matrix is stored is using three arrays:\n\nan array of floating point numbers (A_real say) that stores the non-zero entries;\nan array of integers (I_row say) that stores the row number of the corresponding entry in the real array;\nan array of integers (I_col say) that stores the column numbers of the corresponding entry in the real array.\n\nThis requires just \\(3 \\alpha n\\) units of storage - i.e. \\(O(n)\\). This is called the COO (coordinate) format.\nGiven the above storage pattern, the following algorithm will execute a sparse matrix-vector multiplication (\\(\\vec{z} = A \\vec{y}\\)) in \\(O(n)\\) operations:\nz = np.zeros((n, 1))\nfor k in range(nonzero):\n    z[I_row[k]] = z[I_row[k]] + A_real[k] * y[I_col[k]]\n\nHere nonzero is the number of non-zero entries in the matrix.\nNote that the cost of this operation is \\(O(n)\\) as required.\n\n\n5.5.1 Python experiments\nFirst let’s adapt our implementations to use this sparse matrix format:\n\ndef jacobi_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter, verbose=False\n):\n    \"\"\"\n    Solve a sparse linear system Ax = b using the Jacobi iterative method.\n\n    The system is represented in **sparse COO (coordinate) format** with:\n    - `A_real`: non-zero values\n    - `I_row`: row indices of non-zero entries\n    - `I_col`: column indices of non-zero entries\n\n    The Jacobi method updates the solution iteratively:\n\n    .. math::\n        x_i^{(k+1)} = x_i^{(k)} + \\\\frac{1}{A_{ii}} \\\\left( b_i -\n        \\\\sum_{j=0}^{n-1} A_{ij} x_j^{(k)} \\\\right)\n\n    Parameters\n    ----------\n    A_real : array-like\n        Array of non-zero entries of the sparse matrix A.\n    I_row : array-like\n        Row indices corresponding to each entry in `A_real`.\n    I_col : array-like\n        Column indices corresponding to each entry in `A_real`.\n    b : array-like\n        Right-hand side vector of length `n`.\n    x0 : numpy.ndarray\n        Initial guess for the solution vector, same length as `b`.\n    max_iter : int\n        Maximum number of iterations to perform.\n    verbose : bool, optional\n        If ``True``, prints the solution vector after each iteration. Default is\n        ``False``.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        Approximated solution vector after `max_iter` iterations.\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    # determine diagonal\n    # D[i] should be A_{ii}\n    D = np.zeros_like(x)\n    for k in range(nonzero):\n        if I_row[k] == I_col[k]:\n            D[I_row[k]] = A_real[k]\n\n    for iter in range(max_iter):\n        # precompute Ax\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / D[i] * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\n\ndef gauss_seidel_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter, verbose=False\n):\n    \"\"\"\n    Solve a sparse linear system Ax = b using the Gauss-Seidel iterative method.\n\n    The system is represented in **sparse COO (coordinate) format** with:\n    - `A_real`: non-zero values\n    - `I_row`: row indices of non-zero entries\n    - `I_col`: column indices of non-zero entries\n\n    The Gauss-Seidel method updates the solution sequentially using the most\n    recently computed values:\n\n    .. math::\n        x_i^{(k+1)} = x_i^{(k)} + \\\\frac{1}{A_{ii}} \\\\left( b_i -\n        \\\\sum_{j=0}^{i-1} A_{ij} x_j^{(k+1)} -\n        \\\\sum_{j=i}^{n-1} A_{ij} x_j^{(k)} \\\\right)\n\n    Parameters\n    ----------\n    A_real : array-like\n        Array of non-zero entries of the sparse matrix A.\n    I_row : array-like\n        Row indices corresponding to each entry in `A_real`.\n    I_col : array-like\n        Column indices corresponding to each entry in `A_real`.\n    b : array-like\n        Right-hand side vector of length `n`.\n    x0 : numpy.ndarray\n        Initial guess for the solution vector, same length as `b`.\n    max_iter : int\n        Maximum number of iterations to perform.\n    verbose : bool, optional\n        If ``True``, prints the solution vector after each iteration. Default is\n        ``False``.\n\n    Returns\n    -------\n    x : numpy.ndarray\n        Approximated solution vector after `max_iter` iterations.\n    \"\"\"\n    n, nonzero = system_size_sparse(A_real, I_row, I_col, b)\n\n    x = x0.copy()\n    xnew = np.empty_like(x)\n\n    if verbose:\n        print(\"starting value: \", end=\"\")\n        print_array(x.T, \"x.T\")\n\n    for iter in range(max_iter):\n        # precompute Ax using xnew if i &lt; j\n        Ax = np.zeros_like(x)\n        for k in range(nonzero):\n            if I_row[k] &lt; I_col[k]:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * xnew[I_col[k]]\n            else:\n                Ax[I_row[k]] = Ax[I_row[k]] + A_real[k] * x[I_col[k]]\n\n        for i in range(n):\n            xnew[i] = x[i] + 1.0 / (A[i, i]) * (b[i] - Ax[i])\n        x = xnew.copy()\n\n        if verbose:\n            print(f\"after {iter=}: \", end=\"\")\n            print_array(x.T, \"x.T\")\n\n    return x\n\nThen we can test the two different implementations of the methods:\n\n# random matrix\nn = 4\nnonzero = 10\nA_real, I_row, I_col, b = random_sparse_system(n, nonzero)\nprint(\"sparse matrix:\")\nprint(\"A_real =\", A_real)\nprint(\"I_row = \", I_row)\nprint(\"I_col = \", I_col)\nprint()\n\n# convert to dense for comparison\nA_dense = to_dense(A_real, I_row, I_col)\nprint(\"dense matrix:\")\nprint_array(A_dense)\nprint()\n\n# starting guess\nx0 = np.zeros((n, 1))\n\nprint(\"jacobi with sparse matrix\")\nx_sparse = jacobi_iteration_sparse(\n    A_real, I_row, I_col, b, x0, max_iter=5, verbose=True\n)\nprint()\n\nprint(\"jacobi with dense matrix\")\nx_dense = jacobi_iteration(A_dense, b, x0, max_iter=5, verbose=True)\nprint()\n\nsparse matrix:\nA_real = [-36.    11.25 101.25  10.25  11.25  16.   -36.    20.25]\nI_row =  [0 0 0 1 1 2 2 3]\nI_col =  [2 1 0 1 0 2 0 3]\n\ndense matrix:\nA_dense = [ 101.25, 11.25, -36.00,  0.00 ]\n          [ 11.25, 10.25,  0.00,  0.00 ]\n          [ -36.00,  0.00, 16.00,  0.00 ]\n          [  0.00,  0.00,  0.00, 20.25 ]\n\njacobi with sparse matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  0.75556,  2.09756, -1.25000,  1.00000 ]\nafter iter=1: x.T = [  0.078049,  1.268293,  0.450000,  1.000000 ]\nafter iter=2: x.T = [  0.77463,  2.01190, -1.07439,  1.00000 ]\nafter iter=3: x.T = [  0.150006,  1.247353,  0.492927,  1.000000 ]\nafter iter=4: x.T = [  0.79222,  1.93292, -0.91249,  1.00000 ]\n\njacobi with dense matrix\nstarting value: x.T = [  0.0,  0.0,  0.0,  0.0 ]\nafter iter=0: x.T = [  0.75556,  2.09756, -1.25000,  1.00000 ]\nafter iter=1: x.T = [  0.078049,  1.268293,  0.450000,  1.000000 ]\nafter iter=2: x.T = [  0.77463,  2.01190, -1.07439,  1.00000 ]\nafter iter=3: x.T = [  0.150006,  1.247353,  0.492927,  1.000000 ]\nafter iter=4: x.T = [  0.79222,  1.93292, -0.91249,  1.00000 ]\n\n\n\nWe see that we get the same results!\nNow let us see how long it takes to get a solution. The following plot shows the run times of using the two different implementations of the Jacobi method, each for 10 iterations. We see that, as expected, the run time of the dense formulation is \\(O(n^2)\\) and the run time of the sparse formulation is \\(O(n)\\).\n\n\n\n\n\n\n\n\n\nWe say “as expected” because we have already counted the number of operations per iteration and these implementations compute for a fixed number of iterations. In the next section, we look at alternative stopping criteria.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#convergence-of-an-iterative-method",
    "href": "src/lec05.html#convergence-of-an-iterative-method",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.6 Convergence of an iterative method",
    "text": "5.6 Convergence of an iterative method\nWe have discussed the construction of iterations which aim to find the solution of the equations \\(A \\vec{x} = \\vec{b}\\) through a sequence of better and better approximations \\(\\vec{x}^{(k)}\\).\nIn general the iteration takes the form \\[\n\\vec{x}^{(k+1)} = \\vec{F}(\\vec{x}^{(k)})\n\\] here \\(\\vec{x}^{(k)}\\) is a vector of values and \\(\\vec{F}\\) is some vector-valued function which we have defined.\nHow can we decide if this iteration has converged? We need \\(\\vec{x} -\n\\vec{x}^{(k)}\\) to be small, but we do not have access to the exact solution \\(\\vec{x}\\) so we have to do something else!\nHow do we decide that a vector/array is small? The most common measure is to use the Euclidean norm of an array (which you met last year!). The Euclidean norm, or norm for short, is defined to be the square root of the sum of squares of the entries of the array: \\[\n\\| \\vec{r} \\| = \\sqrt{ \\sum_{i=1}^n r_i^2 }.\n\\] where \\(\\vec{r}\\) is a vector with \\(n\\) entries.\n\nExample 5.4 Consider the following sequence \\(\\vec{x}^{(k)}\\):\n\\[\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix},\n\\begin{pmatrix}\n1.5 \\\\ 0.5\n\\end{pmatrix},\n\\begin{pmatrix}\n1.75 \\\\ 0.25\n\\end{pmatrix},\n\\begin{pmatrix}\n1.875 \\\\ 0.125\n\\end{pmatrix},\n\\begin{pmatrix}\n1.9375 \\\\ -0.0625\n\\end{pmatrix},\n\\begin{pmatrix}\n1.96875 \\\\ -0.03125\n\\end{pmatrix},\n\\ldots\n\\]\n\nWhat is \\(\\|\\vec{x}^{(1)} - \\vec{x}^{(0)}\\|\\)?\nWhat is \\(\\|\\vec{x}^{(5)} - \\vec{x}^{(4)}\\|\\)?\n\nLet \\(\\vec{x} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\\).\n\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(3)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(4)}\\|\\)?\nWhat is \\(\\|\\vec{x} - \\vec{x}^{(5)}\\|\\)?\n\n\nRather than decide in advance how many iterations (of the Jacobi or Gauss-Seidel methods) to use, we can use the following stopping criteria:\n\nA maximum number of iterations.\nThe change in values is small enough:\n\\[\n  \\|x^{(k+1)} - \\vec{x}^{(k)}\\| &lt; tol,\n  \\]\nThe norm of the residual is small enough:\n\n\\[\n\\| \\vec{r} \\| = \\| \\vec{b} - A \\vec{x}^{(k)} \\| &lt; tol\n\\]\nIn the second and third cases, we call \\(tol\\) the convergence tolerance and the choice of \\(tol\\) controls the accuracy of the solution.\n\nExercise 5.2 (Discussion) What is a good convergence tolerance?\n\nIn general there are two possible reasons that an iteration may fail to converge.\n\nIt may diverge - this means that \\(\\|\\vec{x}^{(k)}\\| \\to \\infty\\) as \\(k\\) (the number of iterations) increases, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n4 \\\\ 2\n\\end{pmatrix},\n\\begin{pmatrix}\n16 \\\\ 4\n\\end{pmatrix},\n\\begin{pmatrix}\n64 \\\\ 8\n\\end{pmatrix},\n\\begin{pmatrix}\n256 \\\\ 16\n\\end{pmatrix},\n\\begin{pmatrix}\n1024 \\\\ 32\n\\end{pmatrix},\n\\ldots\n\\]\nIt may neither converge nor diverge, e.g.:\n\\[\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n2 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n3 \\\\ 0\n\\end{pmatrix},\n\\ldots\n\\]\n\nIn addition to testing for convergence it is also necessary to include tests for failure to converge.\n\nDivergence may be detected by monitoring \\(\\|\\vec{x}^{(k)}\\|\\).\nImpose a maximum number of iterations to ensure that the loop is not repeated forever!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#summary",
    "href": "src/lec05.html#summary",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.7 Summary",
    "text": "5.7 Summary\nMany complex computational problems simply cannot be solved with today’s computers using direct methods. Iterative methods are used instead since they can massively reduce the computational cost and storage required to get a “good enough” solution.\nThese basic iterative methods are simple to describe and program but generally slow to converge to an accurate answer - typically \\(O(n)\\) iterations are required! Their usefulness for general matrix systems is very limited therefore - but we have shown their value in the solution of sparse systems however.\nMore advanced iterative methods do exist but are beyond the scope of this module - see Final year projects, MSc projects, PhD, and beyond!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec05.html#further-reading",
    "href": "src/lec05.html#further-reading",
    "title": "5  Iterative solutions of linear equations",
    "section": "5.8 Further reading",
    "text": "5.8 Further reading\nMore details on these basic (and related) methods:\n\nWikipedia: Jacobi method\nWikipedia: Gauss-Seidel method\nWikipedia: Iterative methods\n\nsee also Richardson method, Damped Jacobi method, Successive over-relaxation method (SOR), Symmetric successive over-relaxation method (SSOR) and Krylov subspace methods\n\n\nMore details on sparse matrices:\n\nWikipedia Sparse matrix - including a long detailed list of software libraries support sparse matrices.\nStackoverflow: Using a sparse matrix vs numpy array\nJason Brownlee: A gentle introduction to sparse matrices for machine learning, Machine learning mastery\n\nSome related textbooks:\n\nJack Dongarra Templates for the solution of linear systems: Stopping criteria\nJack Dongarra Templates for the solution of linear systems: Stationary iterative methods\nGolub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins, ISBN 978-0-8018-5414-9.\nSaad, Yousef (2003). Iterative Methods for Sparse Linear Systems (2nd ed.). SIAM. p. 414. ISBN 0898715342.\n\nSome software implementations:\n\nscipy.sparse custom routines specialised to sparse matrices\nSuiteSparse, a suite of sparse matrix algorithms geared toward the direct solution of sparse linear systems\nscipy.sparse iterative solvers: Solving linear problems\nPETSc: Linear system solvers - a high-performance linear algebra toolkit",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative solutions of linear equations</span>"
    ]
  },
  {
    "objectID": "src/lec06.html",
    "href": "src/lec06.html",
    "title": "6  Complex numbers",
    "section": "",
    "text": "6.1 Basic definitions\nThis section of the notes supports the final learning outcome around eigenvalues and eigenvectors.\nA complex number is an element of a number system which extends our familiar real number system. Our motivation will be to find eigenvalues and eigenvectors which is the next topic in this module. To find eigenvalues, we need to solve of polynomial equations and it will turn out to be useful to always be able to get a solution to any polynomial equation.\nThe key idea of complex numbers is to create a new symbol, which we will call \\(i\\), or the imaginary unit which satisfies: \\[\\begin{equation*}\ni^2 = -1 \\qquad \\sqrt{-1} = i.\n\\end{equation*}\\] By taking multiples of this imaginary unit, we can create many more new numbers, like \\(3i, \\sqrt{5} i\\) or \\(-12 i\\). These are examples of imaginary numbers.\nWe form complex numbers by adding real and imaginary numbers whilst keeping each part separate – for example, \\(2 + 3i\\), \\(\\frac{1}{2} + \\sqrt{5} i\\) or \\(12 -\n12i\\).\nWe notice that all real numbers \\(x\\) must also be complex numbers since \\(x = x +\n0 i\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#basic-definitions",
    "href": "src/lec06.html#basic-definitions",
    "title": "6  Complex numbers",
    "section": "",
    "text": "Example 6.1 Consider the polynomial equation \\[\\begin{equation}\nx^2 + 1 = 0.\n\\end{equation}\\]\n\n\n\n\n\nPlot of \\(y = x^2 + 1\\).\n\n\n\n\nWe can see that this equation has no solution over the real numbers: There are no real values \\(x\\) such that \\(x^2 + 1 = 0\\).\n\n\n\n\nDefinition 6.1 Any number that can be written as \\(z = a + bi\\) with \\(a, b\\) real numbers and \\(i\\) the imaginary unit is called complex number. In this format, we call \\(a\\) the real part of \\(z\\) and \\(b\\) the imaginary part of \\(z\\).\n\n\n\nRemark 6.1. Among the first recorded uses of complex numbers in European mathematics was by an Italian mathematician, Gerolamo Cardano, in around 1545. He later described complex numbers as being “as subtle as they are useless” and “mental torment”.\nThe term imaginary was coined by René Descartes in 1637:\n\n… sometimes only imaginary, that is one can imagine as many as I said in each equation, but sometimes there exist no quantity that matches that which we imagine.\n\n\n\nExercise 6.1 What are the real and imaginary parts of these numbers?\n\n\\(3 + 6i\\)\n\\(-3.5 + 2i\\)\n\\(5\\)\n\\(7i\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#calculations-with-complex-numbers",
    "href": "src/lec06.html#calculations-with-complex-numbers",
    "title": "6  Complex numbers",
    "section": "6.2 Calculations with complex numbers",
    "text": "6.2 Calculations with complex numbers\nWe can perform the basic operations, such as addition, subtraction, multiplication and division, on complex numbers.\nAddition and subtraction are relatively straightforward: we treat the real and imaginary parts separately.\n\nExample 6.2 We can compute that \\[\\begin{align*}\n& (2 + 3i) + (12 - 12i) = (2 + 12) + (3 - 12)i = 14 - 9i \\\\\n& (2 + 3i) - (12 - 12i) = (2 - 12) + (3 - (-12))i = -10 + 15i.\n\\end{align*}\\]\n\n\nExercise 6.2 Compute \\((3+6i) + (-3.5 + 2i)\\) and \\((3 + 6i) - (-3.5 + 2i)\\).\n\nFor multiplying and dividing, we first say that applying these operations between complex and real numbers again follows the usual rules. Let \\(x\\) be a real number and \\(z = (a + bi)\\) be a complex number, then \\[\\begin{align*}\n& x \\times (a + bi) = (a + bi) \\times x = (x \\times a) + (x \\times b) i \\\\\n& \\frac{a + bi}{x} = \\frac{a}{x} + \\frac{b}{x} i.\n\\end{align*}\\]\nFor multiplication between complex numbers, things are harder. We expand brackets and apply the rule that \\(i^2 = -1\\):\n\nExample 6.3 \\[\\begin{align*}\n& (2 + 3i) \\times (12 - 12i)  \\\\\n& = 2 \\times 12 + 3i \\times 12 + 2 \\times - 12 i + 3i \\times - 12 i\n&& \\text{(expand brackets)} \\\\\n& = 2 \\times 12 + (12 \\times 3) i + (2 \\times - 12) i + (3 \\times - 12) \\times\ni^2 && \\text{(rearrange)} \\\\\n& = 24 + 36 i - 24 i - 36 \\times i^2 && \\text{(compute products)} \\\\\n& = 24 + 36 i - 24 i + 36 && \\text{(use $i^2 = -1$)} \\\\\n& = 60 + 12 i && \\text{(collect terms)}.\n\\end{align*}\\]\n\nThis leads us to a general formula: \\[\\begin{equation*}\n(a + bi) \\times (c + di) = (ac - bd) + (ad + bc) i.\n\\end{equation*}\\]\n\nExercise 6.3 Compute \\((3 + 6i) \\times (-3.5 + 2i)\\).\n\nDivision is harder - you may want to skip this on first reading since it is not so important for what follows in these notes. When we divide complex numbers, we try to rewrite the fraction to have a real denominator by “rationalising the denominator”.\n\nExample 6.4 Suppose we want to find \\((2 + 3i) / (12 - 12i)\\). Our idea is to find a number so that we can write \\[\\begin{equation*}\n\\frac{2 + 3i}{12 - 12i} = \\frac{2 + 3i}{12 - 12i} \\times \\frac{z}{z}\n= \\frac{(2 + 3i)z}{(12 - 12i)z}\n= \\frac{\\text{something}}{\\text{something real}}.\n\\end{equation*}\\] The answer is to use \\(z = 12 + 12 i\\) - that is the denominator with the sign of the imaginary part flipped (we will give this a name later on).\nWe can compute that \\[\\begin{align*}\n& (12 - 12i) \\times (12 + 12i) \\\\\n& = (12 \\times 12) + (12 \\times 12i) + (-12i \\times 12) + (-12i \\times 12i)\n&& \\text{(expand bracket)}\\\\\n& = 12 \\times 12 + (12 \\times 12) i + (-12 \\times 12) i + (-12 \\times 12) i^2\n&& \\text{(rearrange)} \\\\\n& = 144 + 144i - 144i -144 i^2 && \\text{(compute products)} \\\\\n& = 144 + 144i - 144i + 144 && \\text{(use $i^2 = -1$)} \\\\\n& = 288 + 0 i && \\text{(collect terms)}.\n\\end{align*}\\] So we have that \\((12 - 12i) \\times (12 + 12i) = 288\\) is a real number.\nWe continue by computing that \\[\\begin{align*}\n& (2 + 3i) \\times (12 + 12i) \\\\\n& = (2 \\times 12) + (2 \\times 12i) + (3i \\times 12) + (3i \\times 12i) \\\\\n& = 2 \\times 12 + (2 \\times 12) i  + (3 \\times 12) i + (3 \\times 12) i^2 \\\\\n& = 24 + 24 i + 36 i + 36 i^2 \\\\\n& = 24 + 24 i + 36 i - 36 \\\\\n& = -12 + 60i.\n\\end{align*}\\]\nThus we infer that \\[\\begin{align*}\n\\frac{2 + 3i}{12 - 12i} &= \\frac{2 + 3i}{12 - 12i} \\times\n\\frac{12 + 12i}{12 + 12i} \\\\\n& = \\frac{(2 + 3i)(12 + 12i)}{(12 - 12i)(12 + 12i)} \\\\\n& = \\frac{-12 + 60i}{288} \\\\\n& = \\frac{-12}{288} + \\frac{60}{288} i \\\\\n& = -\\frac{1}{24} + \\frac{5}{24} i.\n\\end{align*}\\]\nTo check we have not done anything silly, we should also check that \\((12 + 12i)\n/ (12 + 12i) = 1\\). This is left as an exercise.\n\n\nExercise 6.4 Find \\((3 + 6i) / (-3.5 + 2i)\\) and \\((12 + 12i) / (12 + 12i)\\).\n\n\nRemark 6.2. One thing to be careful of when considering products is that the identity \\(i^2\n= -1\\) appears to break one rule of arithmetic of square roots: \\[\\begin{equation*}\ni^2 = (\\sqrt{-1})^2 = \\sqrt{-1} \\sqrt{-1} \\neq \\sqrt{(-1) \\times (-1)} = \\sqrt{1}\n= 1.\n\\end{equation*}\\] In fact, we have that \\(\\sqrt{x} \\sqrt{y} = \\sqrt{xy}\\) only if \\(x, y &gt; 0\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#a-geometric-picture",
    "href": "src/lec06.html#a-geometric-picture",
    "title": "6  Complex numbers",
    "section": "6.3 A geometric picture",
    "text": "6.3 A geometric picture\nThe idea of adding complex numbers by considering real and imaginary parts separately is reminiscent of adding two-dimensional vectors. For this reason, it is often helpful to think of complex numbers as points in the complex plane.\nThe complex plane is the two-dimensional space formed by considering the real and imaginary parts of a complex number as two different coordinate axes.\n\nExample 6.5  \n\n\n\n\n\n\n\n\n\n\nAdding complex numbers looks just like adding two dimensional vectors! We can also use this geometric picture to help with some further operations.\nThe complex conjugate of a complex number \\(z = a + bi\\) is given by \\(\\bar{z} =\na - bi\\). The complex conjugate is that number we used before when working out how to divide complex numbers.\n\nExample 6.6 The complex conjugate of \\(2 + 3i\\) is \\(2 - 3i\\). The complex conjugate of \\(12 - 12i\\) is \\(12 + 12i\\).\n\nExercise 6.5 Find the complex conjugates of \\(3 + 6i\\) and \\(-3.5 + 2i\\).\n\n\nWe have already seen that the complex conjugate of a complex number is helpful when performing division of complex numbers. The reason is that computing the product of a number and its conjugate always gives a real, positive number: \\[\\begin{align*}\n& (a + bi) \\times (a - bi) \\\\\n& = (a \\times a) + (a \\times - b i) + (b i \\times a) + (bi \\times - bi) \\\\\n& = (a \\times a) + (a \\times -b) i + (b \\times a) i + (b \\times - b) i^2 \\\\\n& = (a \\times a) + (a \\times -b + b \\times a) i - (b \\times - b) \\\\\n& = a^2 + b^2 + 0 i.\n\\end{align*}\\]\nIn fact, we use this same calculation to define the modulus (sometimes called the absolute value) of a complex number \\(z = a + bi\\) \\[\\begin{equation}\n\\label{eq:modulus}\n|z| = |a + bi| = \\sqrt{a^2 + b^2} = \\sqrt{z \\bar{z}}.\n\\end{equation}\\]\n\nExample 6.7  \n\n\n\n\n\n\n\n\n\n\nExercise 6.6 Find the value of \\[\\begin{equation}\n|3 + 6i| \\quad \\text{and} \\quad |-3.5 + 2i|.\n\\end{equation}\\]\n\n\nConsider two complex numbers \\(z = a + bi\\) and \\(y = c + di\\). Then, we have already seen that \\[\\begin{align*}\nz y\n& = (a + bi) \\times (c + di) = (ac - bd) + (ad + bc)i.\n\\end{align*}\\] We can compute the square of the modulus of the product \\(zy\\) as \\[\\begin{align*}\n|zy|^2\n& = (ac - bd)^2 + (ad + bc)^2 \\\\\n& = a^2 c^2 - 2 abcd + b^2 d^2 + a^2 d^2 + 2 abcd + b^2 c^2 \\\\\n& = a^2 c^2 + b^2 d^2 + a^2 d^2 + b^2 c^2 \\\\\n& = (a^2 + b^2) (c^2 + d^2),\n\\end{align*}\\] and we have computed that \\(|zy|= |z| |y|\\).\nIn particular, if \\(y\\) has modulus 1, then \\(|zy| = |z|\\). This means that \\(zy\\) and \\(z\\) are the same distance from the origin but ‘point’ in different directions. We can write the real and imaginary parts as \\(y = c + di = \\cos(\\theta) + i\n\\sin(\\theta)\\), where \\(\\theta\\) is the angle between the positive real axis and the line between \\(0\\) and \\(y\\). Then \\[\\begin{align*}\nz y\n& = (ac - bd) + (ad + bc)i \\\\\n& = (a \\cos(\\theta) - b \\sin(\\theta)) + (a \\sin(\\theta) + b\\cos(\\theta)) i.\n\\end{align*}\\] Recalling the example of a rotation matrix from Example 1.3, we see that multiplying by \\(y\\) is the same as rotating the complex point (\\(z\\)) by an angle of \\(\\theta\\) radians in the anti-clockwise direction.\nThis leads us to thinking polar coordinates for the complex plane. Polar coordinates are a different form of coordinates that replace the usual \\(x\\) and \\(y\\)-directions (up and across) by two values which represent the distance to the origin (that we call radius) and angle to the positive \\(x\\)-axis (that we call the angle). When talking about a complex number \\(z\\) represented in the complex plane, we know that the modulus \\(|z|\\) represents the radius. The idea of \\(\\theta\\) above represents the angle of a complex number that we know call the argument.\n\nDefinition 6.2 Let \\(z\\) be a complex number. The polar form of \\(z\\) is \\(R (\\cos \\theta + i\n\\sin \\theta)\\). We call \\(R\\) the modulus of \\(z\\) and \\(\\theta\\) is the argument of \\(z\\).\n\nThe representation of the angle unique up to adding integer multiples of \\(2\n\\pi\\), since rotating a point by \\(2 \\pi\\) about the origin leaves it unchanged.\n\nExample 6.8 Let \\(z = 12 -12i\\). Then \\[\\begin{align*}\n|z| = |12 - 12i| = \\sqrt{12^2 + 12^2} = \\sqrt{2 \\times 144} = 12 \\sqrt{2}.\n\\end{align*}\\] We have \\(\\arg{z} = -\\pi / 4\\) since \\[\\begin{equation*}\n\\cos(-\\pi/4) = \\frac{1}{\\sqrt{2}}, \\quad \\text{and} \\quad \\sin(-\\pi/4)\n= \\frac{-1}{\\sqrt{2}},\n\\end{equation*}\\] so\n\\[\\begin{equation*}\n12 \\sqrt{2} (\\cos(-\\pi/4) + i \\sin(-\\pi/4))\n= 12 \\sqrt{2} \\left( \\frac{1}{\\sqrt{2}} + i \\frac{-1}{\\sqrt{2}} \\right)\n= 12 - 12i.\n\\end{equation*}\\]\n\nExercise 6.7 Compute the modulus and argument of \\(2\\), \\(3i\\) and \\(4 + 4i\\).\n\n\n\nExample 6.9  \n\n\n\n\n\n\n\n\n\n\nThe polar representation of complex numbers then gives us a nice way to understand multiplication of complex numbers. If \\(y \\neq 0\\), then we can check that \\(\\left| \\frac{y}{|y|} \\right| = 1\\) and \\(\\arg{y} = \\arg{\\frac{y}{|y|}}\\). Then writing \\(zy = z \\frac{y}{|y|} |y|\\), we can use our calculations above to infer that multiplying by \\(y\\) corresponds to an anticlockwise rotation by \\(\\arg{y}\\) then scaling by \\(|y|\\).\n\nExercise 6.8 Check that for any non-zero complex number \\(y\\), that \\(\\left| \\frac{y}{|y|}\n\\right| = 1\\) and \\(\\arg{y} = \\arg{\\frac{y}{|y|}}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#eulers-formula",
    "href": "src/lec06.html#eulers-formula",
    "title": "6  Complex numbers",
    "section": "6.4 Euler’s formula",
    "text": "6.4 Euler’s formula\nThe polar form of complex numbers can be further developed thanks to two helpful identities. We only state the identities here. There are some examples on the worksheet to help your understanding here.\nEuler’s formula is relates the polar form of a complex number to complex exponentials: Let \\(x\\) be a real number, then \\[\n\\exp (i x) = \\cos x + i \\sin x.\n\\] The result can be shown by showing that the function \\(f(x) = \\exp(-ix)(\\cos x +\ni \\sin x)\\) is a constant function and that the constant value is \\(1\\). For the purposes of this part of the module, you might want to think of \\(\\exp(ix)\\) simply as a shorthand for \\(\\cos x + i \\sin x\\) when used in calculations.\nFor example, as a consequence of Euler’s formula, we can compute powers of complex numbers very simply. Let \\(z\\) be given in polar form by \\(z = R \\exp(i\n\\theta)\\) (that is \\(R\\) is the magnitude of \\(z\\) and \\(\\theta\\) is the argument of \\(z\\)). Then \\(z^p\\) is, by the rule of exponentiation, \\(z^p = R^p \\exp(i p\n\\theta)\\).\nEuler’s formula is the key important step for showing Euler’s identity: \\[\n\\exp(i \\pi) = -1.\n\\] This is shown by applying Euler’s formula at \\(x= \\pi\\). This is a formula which many mathematicians a surprising and beautiful result!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#solving-polynomial-equations",
    "href": "src/lec06.html#solving-polynomial-equations",
    "title": "6  Complex numbers",
    "section": "6.5 Solving polynomial equations",
    "text": "6.5 Solving polynomial equations\nAs we have mentioned above, we will be using complex numbers when solving polynomial equations to work out eigenvalues and eigenvectors of matrices later in the section. The reason complex numbers are useful here is this very important Theorem:\n\nTheorem 6.1 (The Fundamental Theorem of Algebra) For any complex numbers \\(a_0, \\ldots, a_n\\) not all zero, there is at least one complex number \\(z\\) which satisfies: \\[\\begin{equation*}\na_n z^n + \\cdots + a_1 z + a_0 = 0\n\\end{equation*}\\]\n\nIt is really important to note here that this is not true if we want \\(z\\) to be a real number. Let’s revisit Example 6.1.\n\nExample 6.10 Consider the polynomial equation \\[\\begin{equation}\nx^2 + 1 = 0.\n\\end{equation}\\]\nWe saw before that this equation has no solution over the real numbers, but the Fundamental Theorem of Algebra tells us there must be at least one solution which is a complex number. In fact it has two solutions - \\(i\\) and \\(-i\\): \\[\\begin{align*}\n& i^2 + 1 = 0 \\\\\n& (-i)^2 + 1 = (-1)^2 i^2 + 1 = i^2 + 1 = 0.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nNotice that along the real line (Imaginary part \\(=0\\)), the value of the function is always above 1.\n\nIn general, to find complex roots of other quadratic equations, we can apply the quadratic formula:\n\nExample 6.11 To find the values of \\(z\\) which satisfy \\(z^2 - 2z + 2 = 0\\), we see: \\[\\begin{align*}\nz = \\frac{+2 \\pm \\sqrt{(-2)^2 - 4 \\times 1 \\times 2}}{2} =\n\\frac{2 \\pm \\sqrt{-4}}{2} = \\frac{2 \\pm 2 \\sqrt{-1}}{2} = 1 \\pm i.\n\\end{align*}\\]\n\nExercise 6.9 Find the value of \\(z\\) which satisfy \\(z^2 - 4z + 20 = 0\\).\n\n\nWe will see in later sections that although this is one possible solution to compute the eigenvalues for \\(2 \\times 2\\) matrices this approach becomes infeasible for larger size matrices and we need another approach!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#complex-vectors-and-matrices",
    "href": "src/lec06.html#complex-vectors-and-matrices",
    "title": "6  Complex numbers",
    "section": "6.6 Complex vectors and matrices",
    "text": "6.6 Complex vectors and matrices\nLast year you were introduced to vectors of real numbers. In the remaining part of Linear Algebra, we will need to work with complex matrices and vectors. The definitions are mostly what you would expect but with some slight subtleties.\n\nDefinition 6.3 A complex matrix is a matrix whose entries are complex numbers. A complex vector is a vector whose entries are complex numbers.\n\nOne thing we like to do with vectors is to take scalar products and to find their length. When working with complex vectors, we replace the scalar product with a Hermitian product. We write \\(\\langle \\vec{a}, \\vec{b} \\rangle\\) for the Hermitian product of two complex \\(n\\)-vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) and the value is given by \\[\\begin{equation}\n\\label{eq:hermitian-product}\n\\langle \\vec{a}, \\vec{b} \\rangle = \\sum_{i=1}^n a_i \\bar{b}_i.\n\\end{equation}\\] We see that the Hermitian product is the same as taking the scalar product of \\(\\vec{a}\\) and the complex conjugate of \\(\\vec{b}\\). If the entries in \\(\\vec{b}\\) have no imaginary part (i.e. they are all real numbers), then the Hermitian product of \\(\\vec{a}\\) and \\(\\vec{b}\\) is equal to the scalar product of \\(\\vec{a}\\) and \\(\\vec{b}\\).\n\nExample 6.12 Let \\(\\vec{a} = (1 + 2i, 2 + i, 3 - 4i)^T\\) and \\(\\vec{b} =\n(6 + 2i, 3 + 4i, 1 + i)^T\\) then: \\[\\begin{align*}\n\\langle \\vec{a}, \\vec{b} \\rangle\n& = (1 + 2i) \\times (6 - 2i) + (2 + i) \\times (3 - 4i) + (3 - 4i) \\times (1 - i)\n\\\\\n& = (10 + 10i) + (10 - 5i) + (-1 - 7i) = 19 - 2i.\n\\end{align*}\\]\n\nExercise 6.10 Find the Hermitian product of \\(\\vec{c} = (3 + i, -2 + 2i, 4 - i)^T\\) and \\(\\vec{d}\n= (-1 + 3i, 2 + 6i, 2i)^T\\).\n\n\nRecalling the definition of modulus of a complex number \\(\\eqref{eq:modulus}\\), we can also compute that for a complex \\(n\\)-vector \\(\\vec{z}\\) that \\[\\begin{align*}\n\\langle \\vec{z}, \\vec{z} \\rangle\n= \\sum_{i=1}^n z_i \\bar{z}_i = \\sum_{i=1}^n |z_i|^2.\n\\end{align*}\\] So for a complex vector, we define the Euclidean norm as \\[\\begin{align*}\n\\| \\vec{z} \\| = \\sqrt{ \\sum_{i=1}^n |z_i|^2 } = \\sqrt{ \\langle z, z \\rangle }.\n\\end{align*}\\]\nRecall that for a real symmetric \\(n \\times n\\) matrix \\(A\\) and any \\(n\\)-vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) that \\[\\begin{align*}\nA \\vec{x} \\cdot \\vec{y}\n& = \\sum_{i=1}^n (A \\vec{x})_i y_i && \\text{(definition of scalar product)} \\\\\n& = \\sum_{i=1}^n \\left( \\sum_{j=1}^n A_{ij} x_j \\right) y_i\n&& \\text{(definition of matrix-vector product)} \\\\\n& = \\sum_{i=1}^n \\sum_{j=1}^n A_{ji} x_j y_i\n&& \\text{(symmetry of $A$)} \\\\\n& = \\sum_{j=1}^n x_j \\sum_{i=1}^n \\left(A_{ji} y_i \\right)\n&& \\text{(rearranging)} \\\\\n& = \\sum_{j=1}^n x_j (A y)_j\n&& \\text{(definition of matrix-vector product)} \\\\\n& = \\vec{x} \\cdot A \\vec{y}\n&& \\text{(definition of scalar product)}.\n\\end{align*}\\]\nReplacing all terms with complex value and the scalar product with the Hermitian product, we get a similar calculation: \\[\\begin{align*}\n\\langle A \\vec{x}, \\vec{y} \\rangle\n& = \\sum_{i=1}^n (A \\vec{x})_i \\bar{y}_i &&\n\\text{(definition of Hermitian product)} \\\\\n& = \\sum_{i=1}^n \\left( \\sum_{j=1}^n A_{ij} x_j \\right) \\bar{y}_i\n&& \\text{(definition of matrix-vector product)} \\\\\n& = \\sum_{i=1}^n \\sum_{j=1}^n A_{ji} x_j \\bar{y}_i\n&& \\text{(symmetry of $A$)} \\\\\n& = \\sum_{j=1}^n x_j \\sum_{i=1}^n \\left(A_{ji} \\bar{y}_i \\right)\n&& \\text{(rearranging)} \\\\\n& = \\sum_{j=1}^n x_j \\bar{(\\bar{A} y)}_j\n&& \\text{(definition of matrix-vector product)} \\\\\n& = \\langle \\vec{x}, \\bar{A} \\vec{y} \\rangle\n&& \\text{(definition of Hermitian)}.\n\\end{align*}\\] This is quite unsatisfactory (note the extra bar on \\(A\\) in the final equation), and we would like something that generalises the idea of a symmetric matrix to the Hermitian product case too.\nThe key idea is to generalise the definition of matrix transpose to the complex setting:\n\nDefinition 6.4 For a complex \\(n\\)-matrix \\(A\\), we write \\(A^H\\) for the conjugate transpose (also called the Hermitian transpose) given by \\[\\begin{equation*}\n(A^H)_{ij} = \\bar{A}_{ji} = \\bar{A}^T.\n\\end{equation*}\\] If \\(A^H = A\\), then we say that \\(A\\) is Hermitian.\n\nRepeating the above calculations, we can see that if \\(A\\) is Hermitian then \\[\\begin{equation*}\n\\langle A \\vec{x}, \\vec{y} \\rangle = \\langle \\vec{x}, A \\vec{y} \\rangle.\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec06.html#summary",
    "href": "src/lec06.html#summary",
    "title": "6  Complex numbers",
    "section": "6.7 Summary",
    "text": "6.7 Summary\nIn this section, we introduced complex numbers and their algebraic and geometric representations. Key concepts include:\n\nDefinition of complex numbers as combinations of real and imaginary parts.\nArithmetic operations: addition, subtraction, multiplication, and division.\nGeometric interpretation in the complex plane.\nComplex conjugates and their role in division and modulus.\nPolar representation and argument of complex numbers.\nApplication of complex numbers in solving polynomial equations.\nExtension to complex vectors and matrices, including Hermitian products and Hermitian matrices.\n\nThese concepts form the foundation for understanding eigenvalues and eigenvectors in the context of complex matrices.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Complex numbers</span>"
    ]
  },
  {
    "objectID": "src/lec07.html",
    "href": "src/lec07.html",
    "title": "7  Eigenvectors and eigenvalues",
    "section": "",
    "text": "7.1 Key definitions\nThis section of the notes will introduce our second big linear algebra problem. Throughout, we will be considering a square (\\(n\\times n\\)) matrix.\nFor this problem, we will think of a matrix \\(A\\) acting on functions \\(\\vec{x}\\): \\[\\begin{equation*}\n\\vec{x} \\mapsto A \\vec{x}.\n\\end{equation*}\\] We are interested in when is the output vector \\(A \\vec{x}\\) is parallel to \\(\\vec{x}\\).\nIf \\(A \\vec{x} = \\vec{0}\\), then \\(\\vec{x}\\) is an eigenvector associated with the eigenvalue \\(0\\). In fact, we know that \\(0\\) is an eigenvalue of \\(A\\) if, and only if, \\(A\\) is singular.\n{.remark} If we take the product of all eigenvalues of a \\(n \\times n\\) matrix \\(A\\), we get the determinant of the matrix \\(A\\): \\[\\begin{equation*}\n\\det A = \\lambda_1 \\cdot \\lambda_2 \\cdots \\lambda_n.\n\\end{equation*}\\]\nIf we add up all the eigenvalues of a \\(n \\times n\\) matrix, we get the trace of the matrix \\(A\\). We can also find the trace by adding up the diagonal components of the matrix: \\[\\begin{equation*}\n\\lambda_1 + \\cdots + \\lambda_n = a_{11} + a_{22} + \\cdots + a_{nn} =\n\\mathrm{trace}(A).\n\\end{equation*}\\]\nLet \\(A\\) and \\(B\\) be two \\(n \\times n\\) matrices. Then, we cannot use the eigenvalues of \\(A\\) and \\(B\\) to work out the eigenvalues of \\(A + B\\) or the eigenvalues of \\(A B\\), in general. :::",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#key-definitions",
    "href": "src/lec07.html#key-definitions",
    "title": "7  Eigenvectors and eigenvalues",
    "section": "",
    "text": "Definition 7.1 We say that any vector \\(\\vec{x}\\) where \\(A \\vec{x}\\) is parallel is \\(\\vec{x}\\) is called an eigenvector of \\(A\\). Here by parallel, we mean that there exists a number \\(\\lambda\\) (which can be positive, negative or zero, real or complex) such that \\[\\begin{equation}\n\\label{eq:evalues}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We call the associated number \\(\\lambda\\) an eigenvalue of \\(A\\).\nWe will later see that an \\(n \\times n\\) square matrix always has \\(n\\) eigenvalues (which may not always be distinct).\n\n\n\nExample 7.1 Let \\(P\\) be the 3x3 matrix that represents projection onto a plane \\(\\Pi =\n\\{\\vec{x} \\in \\mathbb{R}^3 : \\vec{a} \\cdot \\vec{x} = 0\\}\\) (i.e. \\(P\\) maps any point in \\(\\mathbb{R}^3\\) to its nearest point in \\(\\Pi\\)). What are the eigenvalues and eigenvectors of \\(P\\)?\n\n\n\n\n\n\n\n\n\n\nif \\(\\vec{x}\\) is in the plane \\(\\Pi\\), then \\(P \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the associated eigenvalue is \\(1\\).\nif \\(\\vec{y}\\) is perpendicular to the plane \\(\\Pi\\), then \\(P \\vec{y} = \\vec{0}\\). this means that \\(\\vec{y}\\) is an eigenvector and the associated eigenvalue is \\(0\\).\n\nLet \\(\\vec{y}\\) be perpendicular to \\(\\Pi\\) (so that \\(P \\vec{y} = \\vec{0}\\) and \\(\\vec{y}\\) is an eigenvector of \\(P\\)), then for any number \\(s\\), we can compute \\[\\begin{equation*}\nP (s \\vec{y}) = s P \\vec{y} = s \\vec{0} = \\vec{0}.\n\\end{equation*}\\] This means that \\(s \\vec{y}\\) is also an eigenvector of \\(P\\) associated to the eigenvalue \\(0\\). As a consequence when we compute eigenvectors, we need to take care to normalise the vector to ensure we get a unique answer.\nWe end up with a two-dimensional space of eigenvectors (i.e., the plane \\(\\Pi\\)) associated with eigenvalue \\(1\\) and a one-dimensional space of eigenvectors (i.e., the line perpendicular to \\(\\Pi\\)) associated with the eigenvalue \\(0\\).\n\n\nExample 7.2 Let \\(A\\) be the permutation matrix which takes an input two-vector and outputs a two-vector with the components swapped. The matrix is given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n0 & 1 \\\\ 1 & 0 \\\\\n\\end{pmatrix}.\n\\end{equation*}\\] What are the eigenvectors and eigenvalues of \\(A\\)?\n\nLet \\(\\vec{x} = (1, 1)^T\\), then swapping the components of \\(\\vec{x}\\) gives back the same vector \\(\\vec{x}\\). In equations, we can write \\(A \\vec{x} = \\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector and the eigenvalue is \\(1\\).\nLet \\(\\vec{x} = (-1, 1)^T\\), then swapping the components of \\(\\vec{x}\\) gives back \\((1, -1)^T\\) which we can see is \\(-\\vec{x}\\). In equations, we can write \\(A\n\\vec{x} = -\\vec{x}\\). This means that \\(\\vec{x}\\) is an eigenvector of \\(A\\) and the associated eigenvalue is \\(-1\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#how-to-find-eigenvalues-and-eigenvectors",
    "href": "src/lec07.html#how-to-find-eigenvalues-and-eigenvectors",
    "title": "7  Eigenvectors and eigenvalues",
    "section": "7.2 How to find eigenvalues and eigenvectors",
    "text": "7.2 How to find eigenvalues and eigenvectors\nTo compute eigenvalues and eigenvectors, we start from \\(\\eqref{eq:evalues}\\) and move everything to the left-hand side and use the identity matrix \\((I_n)\\): \\[\\begin{equation*}\n(A - \\lambda I_n) \\vec{x} = \\vec{0}.\n\\end{equation*}\\] This tells us that if we want to find an eigenvalue of \\(A\\), then we need to find a number \\(\\lambda\\) such that \\((A - \\lambda I_n)\\) can multiply a non-zero vector and give us back the zero-vector. This happens when \\((A - \\lambda I)\\) is singular (Theorem 3.3).\nAs we saw earlier one way to test if a matrix is singular, is if the determinant is 0. This gives us a test we can use to determine eigenvalues: \\[\\begin{equation}\n\\label{eq:char}\n\\det( A - \\lambda I_n ) = 0.\n\\end{equation}\\] In fact, this equation no longer depends on the eigenvector \\(\\vec{x}\\), and if we can find solutions \\(\\lambda\\) to this equation then \\(\\lambda\\) is an eigenvalue of \\(A\\).\nWe call \\(\\eqref{eq:char}\\) the characteristic equation or eigenvalue equation. We will see that \\(\\eqref{eq:char}\\) gives us a degree \\(n\\) polynomial equation in \\(\\lambda\\).\nOnce we have found an eigenvalue by solving the characteristic equation for a value \\(\\lambda^*\\), we need to find a vector \\(\\vec{x}\\) such that \\[\\begin{equation*}\n(A - \\lambda^*) \\vec{x} = \\vec{0}.\n\\end{equation*}\\] In general, this is possible using a variation of Gaussian elimination with pivoting, but we do not explore this method in this module.\n\nExample 7.3 Let \\(A\\) be the matrix given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n3 & 1 \\\\ 1 & 3 \\\\\n\\end{pmatrix}.\n\\end{equation*}\\]\nThen, we can compute that \\[\\begin{align*}\n\\det(A - \\lambda I_n)\n& = \\det \\begin{pmatrix}\n3 - \\lambda & 1 \\\\ 1 & 3 - \\lambda\n\\end{pmatrix} \\\\\n& = (3 - \\lambda)(3 - \\lambda) - 1 \\times 1 \\\\\n& = \\lambda^2 + 6 \\lambda + 8.\n\\end{align*}\\] So we want to find values \\(\\lambda\\) such that \\[\\begin{equation*}\n\\det(A - \\lambda I_n) =\\lambda^2 + 6 \\lambda + 8 = 0.\n\\end{equation*}\\] We can read off, by factorisation, that the values of \\(\\lambda\\) are \\(4\\) and \\(2\\).\nWe can now start computing the associated eigenvectors.\nTo find the eigenvector associated with the eigenvalue \\(4\\). We see that \\[\\begin{equation*}\nA - 4 I_n = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}.\n\\end{equation*}\\] We can identify that \\((A - 4I_n) (1, 1)^T = \\vec{0}\\). So \\((1, 1)\\) is an eigenvector associated with \\(4\\).\nTo find the eigenvector associated with the eigenvalue \\(2\\). We see that \\[\\begin{equation*}\nA - 2I_n = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n\\end{equation*}\\] We can identify that \\((A - 2 I_n) (-1, 1)^T = \\vec{0}\\). So \\((-1, 1)\\) is an eigenvector associated with \\(2\\).\n\nWe note that this example is actually surprisingly similar to Example 7.2. We see that the eigenvectors are actually the same! We can see that the matrices are related too: \\[\\begin{equation*}\n\\begin{pmatrix}\n0 & 1 \\\\ 1 & 0 \\\\\n\\end{pmatrix}\n-3 I\n= \\begin{pmatrix}\n3 & 1 \\\\ 1 & 3\n\\end{pmatrix}.\n\\end{equation*}\\] So we can compute that if \\(A \\vec{x} = \\lambda \\vec{x}\\) then \\[\\begin{equation*}\n(A + 3 I) \\vec{x} = \\lambda \\vec{x} + 3 \\vec{x} = (\\lambda + 3) \\vec{x}.\n\\end{equation*}\\] So we see that \\(\\vec{x}\\) is also an eigenvector of \\(A\\) and the associated eigenvector is \\(\\lambda + 3\\).\nAlthough this procedure gives us back eigenvalues there are cases where we have to be careful. We have seen one example above with a two-dimensional eigenspace associated with one eigenvector (Example 7.1). Here are two other cases we must be careful:\n\nExample 7.4 Let \\(Q\\) denote the \\(2 \\times 2\\) matrix that rotates any vector by \\(\\pi/2\\) (\\(=90^\\circ\\)): \\[\\begin{equation*}\nQ = \\begin{pmatrix}\n0 & -1 \\\\ 1 & 0\n\\end{pmatrix}.\n\\end{equation*}\\] Our intuition says that there can be no vectors that when rotated by \\(\\pi/2\\) give something parallel to the input vector, but we can still compute: \\[\\begin{equation*}\n\\det Q = \\det \\begin{pmatrix}\n-\\lambda & -1 \\\\ 1 & -\\lambda\n\\end{pmatrix}\n= \\lambda^2 + 1.\n\\end{equation*}\\] So we can find eigenvalues by finding the values \\(\\lambda\\) such that \\[\\begin{equation*}\n\\lambda^2 + 1 = 0.\n\\end{equation*}\\] We saw in the section on Complex Numbers (Chapter 6) that the solutions to this equation are \\(\\pm i\\). This means that a general algorithm for finding eigenvalues and eigenvectors need to handle complex numbers too.\n\n\nExample 7.5 Let \\(A\\) be the \\(2 \\times 2\\) matrix given by \\[\\begin{equation*}\nA = \\begin{pmatrix}\n3 & 1 \\\\ 0 & 3\n\\end{pmatrix}.\n\\end{equation*}\\] If we follow our procedure above we get a single repeated eigenvalue \\(3\\).\nLooking at the shifted matrix, \\(A - 3 I_n\\): \\[\\begin{equation*}\nA - 3 I_n = \\begin{pmatrix}\n0 & 1 \\\\ 0 & 0\n\\end{pmatrix}.\n\\end{equation*}\\] we can identify one eigenvector \\((1, 0)^T\\), but there is no other eigenvector (in a different direction)! Indeed, we can compute that: \\[\\begin{align*}\n(A - 3 I_n) \\begin{pmatrix}\nx \\\\ y\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & 1 \\\\ 0 & 0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y\n\\end{pmatrix}\n= \\begin{pmatrix}\ny \\\\ 0\n\\end{pmatrix}.\n\\end{align*}\\] This tells us that if \\((A - 3 I_n) (x, y)^T = \\vec{0}\\) if, and only if, \\(y = 0\\). Thus all eigenvectors have the form \\((x, 0)^T\\) and point in the same direction as \\((1, 0)^T\\).\n\n\nExercise 7.1 Find the eigenvalues and eigenvectors for the matrix: \\[\\begin{equation*}\nA = \\begin{pmatrix} 9 & -2 \\\\ -2 & 6 \\end{pmatrix}.\n\\end{equation*}\\]\n\n\nExample 7.6 (Eigenvalues and eigenvectors of a triangular matrix) Consider an upper triangular matrix (Example 3.7). When forming the characteristic equation, we see that most terms are simply zero: \\[\\begin{align*}\n& \\det \\begin{pmatrix}\n3 - \\lambda & 2 \\\\\n0 & 1 - \\lambda\n\\end{pmatrix} \\\\\n& = (3 - \\lambda)(1 - \\lambda) - 2 \\times 0 =\n(3 - \\lambda)(1 - \\lambda) \\\\\n%\n& \\det \\begin{pmatrix}\n3 - \\lambda & 2 & 4 \\\\\n0 & 1 - \\lambda & -1 \\\\\n0 & 0 & 2 - \\lambda\n\\end{pmatrix} \\\\ & =\n(3 - \\lambda) ((1-\\lambda)(2 - \\lambda) - (-1) \\times 0)\n- 2 (0 (2 - \\lambda) - (-1 \\times 0)) + 4 (0 \\times 0 - 1 \\times 0) \\\\\n& = (3 - \\lambda)(1 - \\lambda)( 2 - \\lambda).\n\\end{align*}\\] Indeed this is true in general, \\[\\begin{align*}\n\\det (A - \\lambda I_n) & = \\det \\begin{pmatrix}\na_{11} - \\lambda & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\n0 & a_{22} - \\lambda & a_{23} & \\cdots & a_{2n} \\\\\n0 & 0 & a_{33} - \\lambda & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & a_{nn} - \\lambda \\end{pmatrix} \\\\\n& = \\prod_{i=1}^n (a_{ii} - \\lambda).\n\\end{align*}\\] So we can read off that the eigenvalues of an upper triangular matrix are simply the diagonal coefficients.\nWe can see the eigenvectors are the coordinate directions: \\[\\begin{align*}\n\\begin{pmatrix}\n3 & 2 \\\\ 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}\n& = \\begin{pmatrix}\n3 \\\\ 0\n\\end{pmatrix} \\\\\n\\begin{pmatrix}\n3 & 2 \\\\ 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n& = \\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#important-theory",
    "href": "src/lec07.html#important-theory",
    "title": "7  Eigenvectors and eigenvalues",
    "section": "7.3 Important theory",
    "text": "7.3 Important theory\nWe have established a way to identify eigenvalues and eigenvectors for an arbitrary square matrix. This method can be used to prove the existence of eigenvalues.\n\nTheorem 7.1 Any square \\(n \\times n\\) matrix has \\(n\\) complex eigenvalues (possibly not distinct).\n\n\nProof. For any matrix the characteristic equation \\(\\eqref{eq:char}\\) is a degree \\(n\\) polynomial. The Fundamental Theorem of Algebra (Theorem 6.1) tells us that any degree \\(n\\) polynomial has \\(n\\) roots over the complex numbers. The \\(n\\) roots of the characteristic equation are the \\(n\\) eigenvalues.\n\nThe Abel-Ruffini theorem states that there is no solution in the radicals for a general polynomial of degree 5 or higher with arbitrary coefficients. This implies that there is no ‘nice’ closed form for roots of polynomials of degree 5 or higher. So, if we want an algorithm to find eigenvalues and eigenvectors of larger matrices then we need to do something else!\nLet’s suppose that we have an \\(n \\times n\\) matrix \\(A\\) and we have found \\(n\\) eigenvectors and \\(n\\) eigenvalues (all distinct). Let’s call the eigenvectors by \\(\\vec{x}_{1}, \\ldots \\vec{x}_{n}\\) and the eigenvalues \\(\\lambda_1, \\ldots\n\\lambda_n\\) then we have the equation: \\[\\begin{equation*}\nA \\vec{x}_j = \\lambda_j \\vec{x}_j.\n\\end{equation*}\\] So if we form the matrices \\(S\\) to have columns equal to each eigenvector in turn and \\(\\Lambda\\) (pronounced lambda) to be the diagonal matrix with the eigenvalues listed along the diagonal we see that we have: \\[\\begin{equation*}\nA S = S \\Lambda.\n\\end{equation*}\\] If \\(S\\) is invertible, we can multiply on the right by \\(S^{-1}\\) to see that we have \\[\\begin{equation}\n\\label{eq:SLamSinv}\nA = S \\Lambda S^{-1}.\n\\end{equation}\\] This formula shows another factorisation of the matrix \\(A\\) into simpler matrices, very much like we had when we computed the LU-factorisation matrix (Section 4.7).\nThe equation \\(\\eqref{eq:SLamSinv}\\) is an example of a more general idea of similar matrices. We say that two matrices \\(A\\) and \\(B\\) are similar if there exists an invertible \\(n \\times n\\) matrix \\(P\\) such that \\[\\begin{equation*}\nB = P^{-1} A P.\n\\end{equation*}\\] Since \\(P\\) is invertible, we can pre-multiply this equation by \\(P\\) and post-multiply by \\(P^{-1}\\) and see that being similar is a symmetric property.\n\nLemma 7.1 The matrix \\(A\\) is similar to the diagonal matrix \\(\\Lambda\\) formed by the eigenvalues of \\(A\\).\n\n\nProof. From \\(\\eqref{eq:SLamSinv}\\), we have that \\[\\begin{equation*}\n\\Lambda = S^{-1} A S.\n\\end{equation*}\\]\n\nThis leads to a nice theorem which we will use to help compute eigenvectors and eigenvalues of larger matrices:\n\nTheorem 7.2 If \\(A\\) and \\(B\\) are similar matrices then \\(A\\) and \\(B\\) have the same eigenvalues.\n\n\nProof. We start by writing \\(B = P^{-1} A P\\). Then we can compute that \\[\\begin{equation}\n\\label{eq:Bsim-alt}\nB P^{-1} = P^{-1} A.\n\\end{equation}\\] Let \\(\\lambda\\) be an eigenvalue of \\(A\\) with eigenvector \\(\\vec{x}\\) and write \\(\\vec{y} = P^{-1} \\vec{x}\\). Then we have that \\[\\begin{align*}\nB \\vec{y} & = B P^{-1} \\vec{x} && \\text{(definition of $\\vec{y}$)} \\\\\n& = P^{-1} A \\vec{x}  && \\text{(from \\eqref{eq:Bsim-alt})} \\\\\n& = P^{-1} (\\lambda \\vec{x}) && \\text{(since $\\vec{x}$ is an eigenvector)} \\\\\n& = \\lambda P^{-1} \\vec{x} && \\text{(rearranging)} \\\\\n& = \\lambda \\vec{y} && \\text{(definition of $\\vec{y}$)}.\n\\end{align*}\\] This shows that any eigenvalue of \\(A\\) is an eigenvalue of \\(B\\). It also gives a formula for how eigenvectors change between \\(A\\) and \\(B\\).\nTo show any eigenvalue of \\(B\\) is an eigenvalue of \\(A\\), we repeat the calculation with \\(A\\) and \\(B\\) swapped.\n\nThe key idea of the methods we will use to compute eigenvalues to apply a sequence of matrices to convert a matrix \\(A\\) into a form similar to \\(A\\) for which reading off the eigenvalues is easier. However, the quality of the algorithms we apply depend heavily on properties of the matrix \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec07.html#sec-symmetric-nice",
    "href": "src/lec07.html#sec-symmetric-nice",
    "title": "7  Eigenvectors and eigenvalues",
    "section": "7.4 Why symmetric matrices are nice",
    "text": "7.4 Why symmetric matrices are nice\nWhilst all our methods are developed for complex matrices, when we restrict to real-valued symmetric matrices, we have this nice result.\n\nTheorem 7.3 Let \\(A\\) be a symmetric matrix (\\(A^T = A\\)) with real entries. Then \\(A\\) has \\(n\\) real eigenvalues (zero imaginary part) and any distinct eigenvectors are orthogonal.\n\n\nProof. Let \\(\\lambda\\) be an eigenvalue of \\(A\\) with eigenvector \\(\\vec{x}\\). Recall that \\(\\vec{x} \\neq 0\\). Then, since \\(A\\) has real values, we can compute that: \\[\\begin{equation*}\n\\bar{(A \\vec{x})}_i = \\bar{\\left(\\sum_{j=1}^n A_{ji} x_i\\right)}\n= \\sum_{j=1}^n A_{ji} \\bar{x_i} = (A \\bar{\\vec{x}})_i.\n\\end{equation*}\\] We also note that any real, symmetric matrix is automatically Hermitian.\nThen we see that \\[\\begin{align*}\n\\lambda \\langle \\vec{x}, \\vec{x} \\rangle\n& = \\langle (\\lambda \\vec{x}), \\vec{x} \\rangle\n&& \\text{(from definition of Hermitian product)}\\\\\n& = \\langle (A \\vec{x}), \\vec{x} \\rangle\n&& \\text{(definition of eigenvalue and eigenvector)} \\\\\n& = \\langle \\vec{x}, A \\vec{x} \\rangle\n&& \\text{(symmetry of $A$)} \\\\\n& = \\langle \\vec{x}, \\lambda \\vec{x} \\rangle\n&& \\text{(definition of eigenvalue and eigenvector)} \\\\\n& = \\bar{\\lambda} \\langle \\vec{x}, \\vec{x} \\rangle\n&& \\text{(from definition of Hermitian product)}.\n\\end{align*}\\] Since, \\(\\langle \\vec{x}, \\vec{x} \\rangle &gt; 0\\) (recall \\(\\vec{x} \\neq 0\\)), we can divide by \\(\\langle \\vec{x}, \\vec{x} \\rangle\\) so infer that \\[\\begin{equation*}\n\\lambda = \\bar{\\lambda}.\n\\end{equation*}\\]\nNext, let \\(\\vec{x}\\) and \\(\\vec{y}\\) be eigenvectors of \\(A\\) with distinct, eigenvalues \\(\\lambda\\) and \\(\\mu\\), respectively. From the first part of the proof, we know that \\(\\lambda\\) and \\(\\mu\\) are real. We compute that \\[\\begin{align*}\n\\lambda \\langle \\vec{x}, \\vec{y} \\rangle\n& = \\langle \\lambda \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\langle A \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, A^H \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, A \\vec{y} \\rangle \\\\\n& = \\langle \\vec{x}, \\mu \\vec{y} \\rangle \\\\\n& = \\bar{\\mu} \\langle \\vec{x}, \\vec{y} \\rangle \\\\\n& = \\mu \\langle \\vec{x}, \\vec{y} \\rangle.\n\\end{align*}\\] Subtracting the right-hand side from the left hand side we see that \\[\\begin{equation*}\n(\\lambda - \\mu) \\langle \\vec{x}, \\vec{y} \\rangle = 0.\n\\end{equation*}\\] This implies that if \\(\\lambda\\) and \\(\\mu\\) are distinct, then \\(\\langle \\vec{x},\n\\vec{y} \\rangle = 0\\).\n\n\nCorollary 7.1 Let \\(A\\) be a symmetric \\(n \\times n\\) matrix with real entries. Then the eigenvectors of \\(A\\) form a basis of \\(\\mathbb{R}^n\\).\n\n\nProof. We can only give an incomplete proof of this result. We will show that the eigenvectors are linearly independent. The proof is completed by showing that if you have any \\(n\\) linearly independent vectors in \\(\\mathbb{R}^n\\) then you must have a basis.\nDenote by \\(\\vec{x}^{(1)}, \\ldots \\vec{x}^{(n)}\\) the \\(n\\) eigenvectors of \\(A\\). We want to show that the eigenvectors are linearly independent. Suppose that we have real numbers \\(\\alpha_1, \\alpha_2, \\cdots, \\alpha_n\\) such that: \\[\\begin{equation}\n\\label{eq:evalue-lin}\n\\sum_{i=1}^n \\alpha_i \\vec{x}^{(i)} = \\vec{0}.\n\\end{equation}\\] To show the eigenvectors are linearly independent, we need to show that all \\(\\alpha_i = 0\\). We can do this by taking the inner product of \\(\\eqref{eq:evalue-lin}\\) with \\(\\vec{x}^{(j)}\\) for any \\(j\\): \\[\\begin{equation*}\n0 = \\vec{0} \\cdot \\vec{x}^{(j)}\n= \\left(\\sum_{i=1}^n \\alpha_i \\vec{x}^{(i)}\\right) \\cdot \\vec{x}^{(j)}\n= \\sum_{i=1}^n \\alpha_i \\left( \\vec{x^{(i)}} \\cdot \\vec{x}^{(j)} \\right)\n= \\alpha_j \\vec{x}^{(j)} \\cdot \\vec{x}^{(j)}.\n\\end{equation*}\\] Since we have that \\(|\\vec{x}^{(j)}| &gt; 0\\), we have \\(\\alpha_j = 0\\) and we have shown that the eigenvectors are linearly independent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Eigenvectors and eigenvalues</span>"
    ]
  },
  {
    "objectID": "src/lec08.html",
    "href": "src/lec08.html",
    "title": "8  Eigenvectors and eigenvalues: practical solutions",
    "section": "",
    "text": "8.1 QR algorithm\nIn the previous lecture, we defined the eigenvalue problem for a matrix \\(A\\): Finding numbers \\(\\lambda\\) (eigenvalues) and vectors \\(\\vec{x}\\) (eigenvectors) which satisfy the equation: \\[\\begin{equation}\nA \\vec{x} = \\lambda \\vec{x}.\n\\end{equation}\\] We saw one starting point for finding eigenvalues is to find the roots of the characteristic equation: a polynomial of degree \\(n\\) for an \\(n \\times n\\) matrix \\(A\\). However, we have already seen that this approach will be infeasible for large matrices. Instead, we will find a sequence of similar matrices to \\(A\\) such that we can read off the eigenvalues from the final matrix.\nIn equations, we can say our “grand strategy” is to find a sequence of matrices \\(P_1, P_2, \\ldots\\) to form a sequence of matrices: \\[\\begin{equation}\n\\label{eq:similarity_transform}\nA, P_1^{-1} A P, P_2^{-1} P_1^{-1} A P_1 P_2, P_3^{-1} P_2^{-1} P_1^{-1} A P_1\nP_2 P_3, \\ldots\n\\end{equation}\\] We aim to get all the way to a simple matrix where we read off the eigenvalues and eigenvectors.\nFor example, if at level \\(m\\), say, we have transformed \\(A\\) into a diagonal matrix the eigenvalues are the diagonal of the matrix \\[\\begin{equation*}\nP_m^{-1} P_{m-1}^{-1} \\cdots P_2^{-1} P_1^{-1} A P_1 P_2 \\cdots P_{m-1} P_m,\n\\end{equation*}\\] and the eigenvectors are the columns of the matrix \\[\\begin{equation*}\nS_m = P_1 P_2 \\cdots P_{m-1} P_m.\n\\end{equation*}\\] We have seen a similar example for upper triangular matrices (Example 7.6).\nThe QR algorithm is an iterative method for computing eigenvalues and eigenvectors. At each step a matrix is factored into a product in a similar fashion to LU factorisation (Section 4.7). In this case, we factor a matrix, \\(A\\) into a product of an orthonormal matrix, \\(Q\\), and an upper triangular matrix, \\(R\\): \\[\nA = Q R.\n\\] This is QR factorisation.\nGiven a matrix \\(A\\), the algorithm repeatedly applies QR factorisation. First, we set \\(A^{(0)} = A\\), then we successively perform for \\(k=0, 1, 2, \\ldots\\):\nAs we take more and more steps, we hope that \\(A^{(k)}\\) converges to an upper triangular matrix whose diagonal entries are the eigenvalues of the original matrix.\nRearranging the first step within each iteration, we see that \\[\nR^{(k)} = (Q^{(k)})^{-1} A^{(k)} = (Q^{(k)})^T A^{(k)}.\n\\] Substituting this value of \\(R^{(k)}\\) into the second step gives \\[\nA^{(k+1)} = (Q^{(k)})^{-1} A^{(k)} Q^{(k)},\n\\] and we see that at each step we are finding a sequence of similar matrices, all with the same eigenvalues ({Theorem 7.2}). We can additionally find the eigenvectors of \\(A\\) by forming the product \\[\nQ = Q^{(1)} Q^{(2)} \\cdots Q^{(m)}.\n\\]\nThe hard part of the method is computing the QR factorisation. One classical way to get a QR factorisation is to use the Gram-Schmidt process. In general, the Gram-Schmidt process is used to take a sequence of vectors and form a new sequence which is orthonormal. We can apply this to the columns of \\(A\\) to form an orthonormal matrix. If we track this process as a matrix-matrix product, we find that the other factor is upper triangular.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#qr-algorithm",
    "href": "src/lec08.html#qr-algorithm",
    "title": "8  Eigenvectors and eigenvalues: practical solutions",
    "section": "",
    "text": "Compute the QR factorisation of \\(A^{(k)}\\) into an orthonormal part and upper triangular part \\[\nA^{(k)} = Q^{(k)} R^{(k)};\n\\]\nUpdate the matrix \\(A^{(k+1)}\\) recombining \\(Q\\) and \\(R\\) in the reverse order: \\[\nA^{(k+1)} = R^{(k)} Q^{(k)}.\n\\]\n\n\n\n\n\n8.1.1 The Gram-Schmidt process\nThe key idea is shown in Figure 8.1. Given a vector \\(\\vec{a}\\) (blue) and a vector \\(\\vec{q}\\) (green) with length 1. We can compute the projection of \\(\\vec{a}\\) onto the direction \\(\\vec{q}\\) (orange) by \\[\\begin{align*}\n(\\vec{a} \\cdot \\vec{q}) \\vec{q}.\n\\end{align*}\\] If we subtract this term from \\(\\vec{a}\\). We end up with a vector \\(\\vec{u}\\) with \\(\\vec{u} \\cdot \\vec{q} = 0\\). The difference \\(\\vec{u}\\) is given by \\[\\begin{align*}\n\\vec{u} = \\vec{a} - (\\vec{a} \\cdot \\vec{q}) \\vec{q},\n\\end{align*}\\] and we can compute that \\[\\begin{align*}\n\\vec{u} \\cdot \\vec{q}\n& = (\\vec{a} - (\\vec{a} \\cdot \\vec{q}) \\vec{q}) \\cdot \\vec{q} \\\\\n& = (\\vec{a} \\cdot \\vec{q}) - (\\vec{a} \\cdot \\vec{q}) (\\vec{q} \\cdot \\vec{q})\n&& \\text{(properties of scalar product)}\\\\\n& = (\\vec{a} \\cdot \\vec{q}) - (\\vec{a} \\cdot \\vec{q})\n&& \\text{(since $\\| \\vec{q} \\| = 1$)} \\\\\n& = 0.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nFigure 8.1: Projection of \\(\\vec{a}\\) away from \\(\\vec{q}\\) onto \\(\\vec{u}\\).\n\n\n\n\n\n\nExample 8.1 Consider the sequence of vectors \\[\\begin{equation*}\n\\vec{a}^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\vec{a}^{(2)} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix},\n\\vec{a}^{(3)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n\\end{equation*}\\] We will manipulate these vectors to form three orthonormal vectors.\n\n\n\n\n\n\n\n\n\nFirst, we set \\(\\vec{q}^{(1)} = \\vec{a}^{(1)} / \\| \\vec{a}^{(1)} \\|\\): \\[\\begin{align*}\n\\| \\vec{a}^{(1)} \\| = \\sqrt{1^2 + 0^2 + 1^2} = \\sqrt{2},\n\\end{align*}\\] so \\[\\begin{equation*}\n\\vec{q}^{(1)} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\\n\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n\\end{equation*}\\]\nSecond, we want to find \\(\\vec{q}^{(2)}\\) which must satisfy that \\(\\vec{q}^{(2)} \\cdot \\vec{q}^{(1)} = 0\\). We can do this by subtracting from \\(\\vec{a}^{(2)}\\) the portion of \\(\\vec{a}^{(2)}\\) which points in the direction \\(\\vec{q}^{(1)}\\). We call this \\(\\vec{u}^{(2)}\\): \\[\\begin{align*}\n\\vec{u}^{(2)}\n& = \\vec{a}^{(2)} - \\left(\\vec{a}^{(2)} \\cdot \\vec{q}^{(1)} \\right)\n\\vec{q}^{(1)} \\\\\n& = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} -\n\\left(\\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} \\cdot\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\\n\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\right)\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\\n\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} -\n\\frac{2}{\\sqrt{2}}\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\\n\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\\\\n& = \\begin{pmatrix} 2 - \\frac{2}{\\sqrt{2}} \\frac{1}{\\sqrt{2}} \\\\\n-1 - \\frac{2}{\\sqrt{2}} 0 \\\\\n0 - \\frac{2}{\\sqrt{2}} \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n= \\begin{pmatrix}\n1 \\\\ -1 \\\\ -1\n\\end{pmatrix}.\n\\end{align*}\\] We then normalise \\(\\vec{u}^{(2)}\\) to get a unit-length vector \\(q^{(2)}\\): \\[\\begin{align*}\n\\vec{q}^{(2)} & = \\vec{u}^{(2)} / \\| \\vec{u}^{(2)} \\|\n= \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}} \\\\\n\\frac{-1}{\\sqrt{3}}\n\\end{pmatrix}.\n\\end{align*}\\]\nThird, we will find \\(q^{(3)}\\) which we need to check satisfies \\(\\vec{q}^{(3)}\n\\cdot \\vec{q}^{(2)} = 0\\) and \\(\\vec{q}^{(3)} \\cdot \\vec{q}^{(1)} = 0\\). We can do this by subtracting from \\(\\vec{a}^{(3)}\\) the portion of \\(\\vec{a}^{(3)}\\) which points in the direction \\(\\vec{q}^{(1)}\\) and the portion of \\(\\vec{a}^{(3)}\\) which points in the direction \\(\\vec{q}^{(2)}\\). We call this term \\(\\vec{u}^{(3)}\\) \\[\\begin{align*}\n\\vec{u}^{(3)}\n& = \\vec{a}^{(3)} - \\left(\\vec{a}^{(3)} \\cdot \\vec{q}^{(1)} \\right)\n\\vec{q}^{(1)} - \\left( \\vec{a}^{(3)} \\cdot \\vec{q}^{(2)} \\right) \\vec{q}^{(2)}\n\\\\\n& = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\left(\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\cdot\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix} \\right)\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix} - \\left(\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\cdot\n\\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}}\n\\end{pmatrix} \\right)\n\\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\left(\n\\frac{2}{\\sqrt{2}} \\right)\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix} - \\left(\n\\frac{-2}{\\sqrt{3}}\n\\right)\n\\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}} \\\\ \\frac{-1}{\\sqrt{3}}\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} -\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} -\n\\begin{pmatrix} -2/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}\n= \\begin{pmatrix}\n2/3 \\\\ 4/3 \\\\ -2/3.\n\\end{pmatrix}.\n\\end{align*}\\] Again, we normalise \\(\\vec{u}^{(3)}\\) to get \\(\\vec{q}^{(3)}\\): \\[\\begin{align*}\n\\| \\vec{u}^{(3)} \\| & = \\sqrt{(2/3)^2 + (4/3)^2 + (-2/3)^2}\n= \\sqrt{4/9 + 16/9 + 4/9} = \\sqrt{24/9} \\\\\n& = \\frac{2}{3} \\sqrt{6},\n\\end{align*}\\] so \\[\\begin{align*}\n\\vec{q}^{(3)} & = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\\n\\frac{2}{\\sqrt{6}} \\\\ \\frac{-1}{\\sqrt{6}} \\end{pmatrix}.\n\\end{align*}\\]\n\nExercise 8.2 Verify the orthonormality conditions for \\(\\vec{q}^{(1)}, \\vec{q}^{(2)}\\) and \\(\\vec{q}^{(3)}\\).\n\n\n\n\n\n\n\n\n\n\nNow we have applied the Gram-Schmidt process to convert from the vectors \\(\\vec{a}^{(j)}\\) to the vectors \\(\\vec{q}^{(j)}\\). We can consider the matrix \\(Q\\) whose columns are the vectors \\(\\vec{q}^{(j)}\\) and the matrix \\(A\\) whose columns are the vectors \\(\\vec{a}^{(j)}\\): \\[\\begin{equation*}\nQ = \\begin{pmatrix}\n&& \\\\\n\\vec{q}^{(1)} & \\vec{q}^{(2)} & \\vec{q}^{(3)} \\\\\n&&\n\\end{pmatrix}\n\\quad \\text{and} \\quad\n  A = \\begin{pmatrix}\n&& \\\\\n\\vec{a}^{(1)} & \\vec{a}^{(2)} & \\vec{a}^{(3)} \\\\\n&&\n\\end{pmatrix}\n\\end{equation*}\\] Then we can compute that \\[\\begin{align*}\n(Q^T A)_{ij} = \\vec{q}^{(j)} \\cdot \\vec{a}^{(j)}.\n\\end{align*}\\] So \\[\\begin{align*}\n(Q^T A)\n& = \\begin{pmatrix}\n(1, 0, 1)^T \\cdot (\\tfrac{1}{\\sqrt{2}}, 0, \\tfrac{1}{\\sqrt{2}})^T\n& (2, -1, 0)^T \\cdot (\\tfrac{1}{\\sqrt{2}}, 0, \\tfrac{1}{\\sqrt{2}})^T\n& (1, 2, 1)^T \\cdot (\\tfrac{1}{\\sqrt{2}}, 0, \\tfrac{1}{\\sqrt{2}})^T\n\\\\\n(1, 0, 1)^T \\cdot (\\tfrac{1}{\\sqrt{3}}, \\tfrac{-1}{\\sqrt{3}},\n\\tfrac{-1}{\\sqrt{3}})^T\n& (2, -1, 0)^T \\cdot (\\tfrac{1}{\\sqrt{3}}, \\tfrac{-1}{\\sqrt{3}},\n\\tfrac{-1}{\\sqrt{3}})^T\n& (1, 2, 1)^T \\cdot (\\tfrac{1}{\\sqrt{3}}, \\tfrac{-1}{\\sqrt{3}},\n\\tfrac{-1}{\\sqrt{3}})^T\n\\\\\n(1, 0, 1)^T \\cdot (\\tfrac{1}{\\sqrt{6}}, \\tfrac{2}{\\sqrt{6}},\n\\tfrac{-1}{\\sqrt{6}})^T\n& (2, -1, 0)^T \\cdot (\\tfrac{1}{\\sqrt{6}}, \\tfrac{2}{\\sqrt{6}},\n\\tfrac{-1}{\\sqrt{6}})^T\n& (1, 2, 1)^T \\cdot (\\tfrac{1}{\\sqrt{6}}, \\tfrac{2}{\\sqrt{6}},\n\\tfrac{-1}{\\sqrt{6}})^T\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix}\n\\tfrac{2}{\\sqrt{2}} & \\tfrac{2}{\\sqrt{2}} & \\tfrac{2}{\\sqrt{2}} \\\\\n0 & \\tfrac{3}{\\sqrt{3}} & \\tfrac{-2}{\\sqrt{3}} \\\\\n0 & 0 & \\tfrac{4}{\\sqrt{6}}\n\\end{pmatrix}.\n\\end{align*}\\] Hence we have found an upper triangular matrix \\(R = Q^T A\\). Since \\(Q\\) is orthonormal, we know \\(Q^{-1} = Q^T\\) and we have a factorisation: \\[\\begin{equation*}\nA = Q R.\n\\end{equation*}\\]\n\nExercise 8.3 Continue the QR-factorisation process by computing \\(B = R Q\\) and apply the Gram-Schmidt process to the columns of \\(B\\).\n\n\n\nRemark. The Gram-Schmidt algorithm relies on the fact that after each projection there should be something left - i.e. \\(\\vec{u}^{(j)}\\) should be non-zero. If \\(\\vec{a}^{(j)}\\) is in the span of \\(\\{ \\vec{q}^{(1)}, \\ldots, \\vec{q}^{(j-1)}\n\\}\\), then the projection onto \\(\\vec{u}^{(j)}\\) will give \\(\\vec{0}\\). There are a few ways to test this, but the key idea is that if \\(A\\) is non-singular then we will always have \\(\\vec{u}^{(j)} \\neq \\vec{0}\\) – at least in exact-precision calculations…\n\n\n\n8.1.2 Python QR factorisation using Gram-Schmidt\n\ndef gram_schmidt_qr(A):\n    \"\"\"\n    Compute the QR factorisation of a square matrix using the classical\n    Gram-Schmidt process.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A square 2D NumPy array of shape ``(n, n)`` representing the input\n        matrix.\n\n    Returns\n    -------\n    Q : numpy.ndarray\n        Orthonormal matrix of shape ``(n, n)`` where the columns form an\n        orthonormal basis for the column space of A.\n    R : numpy.ndarray\n        Upper triangular matrix of shape ``(n, n)``.\n    \"\"\"\n    n, m = A.shape\n    if n != m:\n        raise ValueError(f\"the matrix A is not square, {A.shape=}\")\n\n    Q = np.empty_like(A)\n    R = np.zeros_like(A)\n\n    for j in range(n):\n        # Start with the j-th column of A\n        u = A[:, j].copy()\n\n        # Orthogonalize against previous q vectors\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i], A[:, j])  # projection coefficient\n            u -= R[i, j] * Q[:, i]  # subtract the projection\n\n        # Normalize u to get q_j\n        R[j, j] = np.linalg.norm(u)\n        Q[:, j] = u / R[j, j]\n\n    return Q, R\n\nLet’s test it without our example above:\n\n\nA = [  1.0,  2.0,  1.0 ]\n    [  0.0, -1.0,  2.0 ]\n    [  1.0,  0.0,  1.0 ]\nQR factorisation:\nQ = [  0.70711,  0.57735,  0.40825 ]\n    [  0.00000, -0.57735,  0.81650 ]\n    [  0.70711, -0.57735, -0.40825 ]\nR = [  1.41421,  1.41421,  1.41421 ]\n    [  0.00000,  1.73205, -1.15470 ]\n    [  0.00000,  0.00000,  1.63299 ]\nHave we computed a factorisation? (A == Q @ R?) True\nIs Q orthonormal? (Q.T @ Q == np.eye(3)) True",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#finding-eigenvalues-and-eigenvectors",
    "href": "src/lec08.html#finding-eigenvalues-and-eigenvectors",
    "title": "8  Eigenvectors and eigenvalues: practical solutions",
    "section": "8.2 Finding eigenvalues and eigenvectors",
    "text": "8.2 Finding eigenvalues and eigenvectors\nThe algorithm given above says that we use the QR factorisation to iteratively find a sequence of matrices \\(A^{(j)}\\) which should converge to an upper-triangular matrix.\n\ndef gram_schmidt_eigen(A, maxiter=100, verbose=False):\n    \"\"\"\n    Compute the eigenvalues and eigenvectors of a square matrix using the QR\n    algorithm with classical Gram-Schmidt QR factorisation.\n\n    This function implements the basic QR algorithm:\n\n    1. Factorise the matrix `A` into `Q` and `R` using Gram-Schmidt QR\n       factorisation.\n    2. Update the matrix as:\n\n       .. math::\n           A_{k+1} = R_k Q_k\n\n    3. Accumulate the orthonormal transformations in `V` to compute the\n       eigenvectors.\n    4. Iterate until `A` becomes approximately upper triangular or until the\n       maximum number of iterations is reached.\n\n    Once the iteration converges, the diagonal of `A` contains the eigenvalues,\n    and the columns of `V` contain the corresponding eigenvectors.\n\n    Parameters\n    ----------\n    A : numpy.ndarray\n        A square 2D NumPy array of shape ``(n, n)`` representing the input\n        matrix. This matrix will be **modified in place** during the\n        computation.\n    maxiter : int, optional\n        Maximum number of QR iterations to perform. Default is 100.\n    verbose : bool, optional\n        If ``True``, prints intermediate matrices (`A`, `Q`, `R`, and `V`) at\n        each iteration. Useful for debugging and understanding convergence.\n        Default is ``False``.\n\n    Returns\n    -------\n    eigenvalues : numpy.ndarray\n        A 1D NumPy array of length ``n`` containing the eigenvalues of `A`.\n        These are the diagonal elements of the final upper triangular matrix.\n    V : numpy.ndarray\n        A 2D NumPy array of shape ``(n, n)`` whose columns are the normalized\n        eigenvectors corresponding to the eigenvalues.\n    it : int\n        The number of iterations taken by the algorithm.\n    \"\"\"\n    # identity matrix to store eigenvectors\n    V = np.eye(A.shape[0])\n\n    if verbose:\n        print_array(A)\n\n    it = -1\n    for it in range(maxiter):\n        if verbose:\n            print(f\"\\n\\n{it=}\")\n\n        # perform factorisation\n        Q, R = gram_schmidt_qr(A)\n        if verbose:\n            print_array(Q)\n            print_array(R)\n\n        # update A and V in place\n        A = R @ Q\n        V = V @ Q\n\n        if verbose:\n            print_array(A)\n            print_array(V)\n\n        # test for convergence: is A upper triangular up to tolerance 1.0e-8?\n        if np.allclose(A, np.triu(A), atol=1.0e-8):\n            break\n\n    eigenvalues = np.diag(A)\n    return eigenvalues, V, it\n\nWe test this out in code first for the matrix from Example 7.3:\n\nA = np.array([[3.0, 1.0], [1.0, 3.0]])\nprint_array(A)\n\neigenvalues, eigenvectors, it = gram_schmidt_eigen(A)\nprint_array(eigenvalues)\nprint_array(eigenvectors)\nprint(\"iterations required:\", it)\n\nA = [  3.0,  1.0 ]\n    [  1.0,  3.0 ]\neigenvalues = [  4.0 ]\n              [  2.0 ]\neigenvectors = [  0.7071, -0.7071 ]\n               [  0.7071,  0.7071 ]\niterations required: 27\n\n\nThese values agree with those from Example 7.3. Note that this code normalises the eigenvectors to have length one, so we have slightly different values for the eigenvectors but still in the same directions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  },
  {
    "objectID": "src/lec08.html#correctness-and-convergence",
    "href": "src/lec08.html#correctness-and-convergence",
    "title": "8  Eigenvectors and eigenvalues: practical solutions",
    "section": "8.3 Correctness and convergence",
    "text": "8.3 Correctness and convergence\nLet’s see what happens when we try this same approach for a bigger symmetric matrices. Here we have a test that samples ten different random matrices and computes the average number of iterations, average run time and maximum error in the eigenvalue equation.\n\n# replicable random seed\nnp.random.seed(42)\n\n\ndef test_accuracy_of_eigensolve(A, eigenvalues, eigenvectors):\n    \"\"\"\n    test accuracy of solution of eigenvalue problem\n    \"\"\"\n    residuals = []\n    for i in range(len(eigenvalues)):\n        residual = np.linalg.norm(\n            A @ eigenvectors[:, i] - eigenvalues[i] * eigenvectors[:, i]\n        )\n        residuals.append(residual)\n    return max(residuals)\n\n\n# replicable seed\nrepeats = 10\n\n# restart the seed\nnp.random.seed(42)\n\n\ndef random_symmetric_matrix(n):\n    # generate a random matrix\n    S = special_ortho_group.rvs(n)\n    D = np.diag(np.random.randint(1, 10, (n,)) / 2)\n    A = S.T @ D @ S\n    return A\n\n\nprint(\"size  time (s)       iter  error\")\n\nn_values = [2**j for j in range(1, 8)]\nit_values = []\nruntime_values = []\n\nfor n in n_values:\n    runtime = 0.0\n    total_its = 0\n    max_error = 0.0\n\n    for _ in range(repeats):\n        # generate new random matrix\n        A = random_symmetric_matrix(n)\n\n        # do and time the solve, ensure converged\n        maxiter = 1000_000\n        start = time.perf_counter()\n        eigenvalues, eigenvectors, it = gram_schmidt_eigen(A, maxiter=maxiter)\n        end = time.perf_counter()\n\n        if it == maxiter - 1:\n            # skip\n            continue\n\n        # store the data\n        runtime += end - start\n        total_its += it\n        max_error = max(\n            max_error,\n            test_accuracy_of_eigensolve(A, eigenvalues, eigenvectors),\n        )\n\n    runtime /= repeats\n    average_it = total_its / repeats\n\n    it_values.append(average_it)\n    runtime_values.append(runtime)\n\n    print(f\"{n:4d}   {runtime:.6e}  {average_it:5.1f}  {max_error:.6e}\")\n\nsize  time (s)       iter  error\n   2   2.585514e-03   44.6  9.319111e-09\n   4   5.806107e-03   78.1  9.895223e-09\n   8   2.177212e-02  137.7  1.212740e-08\n  16   7.238927e-02  152.7  1.346641e-08\n  32   2.804394e-01  165.6  1.246268e-08\n  64   1.083003e+00  162.3  1.646202e-08\n 128   7.231050e+00  173.8  1.478504e-08\n\n\n\n\n\n\n\n\n\n\n\nWe see that the method converges very well with a good low accuracy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Eigenvectors and eigenvalues: practical solutions</span>"
    ]
  }
]